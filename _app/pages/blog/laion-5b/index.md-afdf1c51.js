import{S as hg,i as ug,s as mg,C as mp,w as pg,x as cg,y as gg,z as wg,A as ng,q as vg,o as bg,B as Lg,L as fg,e as l,k as d,t as i,c as r,a as s,d as t,m as h,h as o,M as S,b as u,g as f,G as a,E as _g}from"../../../chunks/index-3013e6a1.js";import{M as yg}from"../../../chunks/MarkdownPost-0cffa593.js";var Eg="/_app/assets/acquisition-pipeline-dceeb516.jpeg",Ig="/_app/assets/clip-table-243f242b.jpeg",kg="/_app/assets/cloob-table-d32e02b1.png",Ag="/_app/assets/eval_best_auto0185-a3a0b0e8.jpeg",Bg="/_app/assets/eval_best_auto0190-b33827c6.jpeg",xg="/_app/assets/french-cat-a38c687b.jpeg",Tg="/_app/assets/laion-1b-nolang-piechart-be84bead.png",Pg="/_app/assets/laion-2b-en-piechart-6cdd8a6d.png",Cg="/_app/assets/laion-2b-multi-piechart-f72282ce.png",Mg="/_app/assets/ski-vis-22b91006.png",Ng="/_app/assets/watermarks-562edda5.png";function Og(We){let w,A,te,ae,c,M,X,_s,ys,Se,Es,Is,De,ks,As,Jo,B,Bs,Ge,xs,Ts,qe,Ps,Cs,Ue,Ms,Ns,Ko,ze,Os,$o,Re,Ws,Qo,He,Ss,Yo,je,Ds,Xo,Ve,Gs,Zo,Fe,qs,el,D,Je,Ke,Us,zs,Rs,$e,Qe,Hs,js,Vs,Ye,Xe,Fs,Js,tl,ie,Ks,Ze,$s,Qs,al,oe,Ys,et,Xs,Zs,il,tt,en,ol,L,N,tn,at,an,on,it,ln,rn,ot,sn,nn,fn,Ee,dn,lt,hn,un,mn,rt,st,pn,cn,gn,Ie,wn,nt,vn,bn,Ln,ft,_n,dt,yn,En,O,In,ht,kn,An,ut,Bn,xn,mt,Tn,Pn,Cn,W,Mn,pt,Nn,On,ct,Wn,Sn,gt,Dn,Gn,ll,le,qn,wt,Un,zn,rl,x,Rn,vt,Hn,jn,bt,Vn,Fn,Lt,Jn,Kn,sl,_t,$n,nl,re,Qn,yt,Yn,Xn,fl,Et,Zn,dl,It,ef,hl,b,qi,tf,af,Ui,of,lf,zi,rf,sf,Ri,nf,ff,Hi,df,hf,ji,uf,mf,Vi,pf,cf,Fi,gf,ul,se,wf,kt,vf,bf,ml,At,Lf,pl,ne,_f,Bt,yf,Ef,cl,xt,If,gl,G,kf,Tt,Af,Bf,Pt,xf,Tf,wl,Ct,Pf,vl,Mt,Cf,bl,Nt,pp,Ll,Ot,Mf,_l,q,Ji,Nf,Of,Ki,Wf,Sf,$i,Df,yl,Wt,Gf,El,St,qf,Il,Dt,Uf,kl,Gt,zf,Al,qt,Rf,Bl,Ut,Hf,xl,zt,cp,Tl,Rt,jf,Pl,U,Qi,Vf,Ff,Yi,Jf,Kf,Xi,$f,Cl,Ht,Qf,Ml,g,Zi,Yf,Xf,eo,Zf,ed,to,td,ad,ao,id,od,io,ld,rd,oo,sd,nd,lo,fd,dd,ro,hd,ud,so,md,pd,no,cd,gd,fo,wd,Nl,jt,vd,Ol,Vt,bd,Wl,Ft,Ld,Sl,Jt,gp,Dl,Kt,_d,Gl,z,ho,yd,Ed,uo,Id,kd,mo,Ad,ql,$t,Bd,Ul,Qt,xd,zl,Yt,Td,Rl,Xt,Pd,Hl,Zt,Cd,jl,ea,wp,Vl,ta,Md,Fl,aa,Nd,Jl,ia,Od,Kl,oa,Wd,$l,la,Sd,Ql,ra,Dd,Yl,sa,Gd,Xl,E,po,qd,Ud,co,zd,Rd,go,Hd,jd,wo,Vd,Fd,vo,Jd,Zl,na,Kd,er,T,$d,fa,Qd,Yd,da,Xd,Zd,ha,eh,th,tr,P,bo,ah,ih,Lo,oh,lh,_o,rh,sh,yo,nh,ar,ua,fh,ir,R,dh,ma,hh,uh,pa,mh,ph,or,ca,ch,lr,H,gh,ga,wh,vh,wa,bh,Lh,rr,va,_h,sr,j,yh,ba,Eh,Ih,La,kh,Ah,nr,_a,Bh,fr,V,xh,ya,Th,Ph,Ea,Ch,Mh,dr,Ia,Nh,hr,F,Oh,ka,Wh,Sh,Aa,Dh,Gh,ur,Ba,qh,mr,xa,vp,pr,Ta,Uh,cr,Pa,zh,gr,Ca,Rh,wr,Ma,Hh,vr,Z,jh,ke,Vh,Fh,Ae,Jh,br,Na,Kh,Lr,Oa,$h,_r,fe,Eo,Qh,Yh,Io,Xh,yr,_,Zh,Wa,eu,tu,Sa,au,iu,Da,ou,lu,Ga,ru,su,qa,nu,fu,Er,Ua,du,Ir,za,hu,kr,J,ko,uu,mu,Ao,pu,cu,Bo,gu,Ar,Ra,wu,Br,Ha,vu,xr,ja,bu,Tr,Va,bp,Pr,Fa,Lu,Cr,Be,_u,xe,yu,Mr,de,Eu,Te,Iu,ku,Nr,Ja,Au,Or,ee,Ka,Bu,xu,$a,Tu,Pu,Wr,Qa,Lp,Sr,Ya,_p,Dr,he,xo,Cu,Mu,To,Nu,Gr,ue,Ou,Po,Wu,Su,qr,Xa,Du,Ur,K,Gu,Za,qu,Uu,ei,zu,Ru,zr,$,Pe,Hu,ju,Ce,Vu,Fu,Me,Ju,Rr,ti,yp,Hr,ai,Ku,jr,me,$u,ii,Qu,Yu,Vr,oi,Xu,Fr,Ne,Zu,Oe,em,Jr,li,Ep,Kr,ri,tm,$r,si,am,Qr,ni,im,Yr,I,fi,di,om,lm,rm,hi,ui,sm,nm,fm,mi,pi,dm,hm,um,ci,gi,mm,pm,cm,wi,vi,gm,wm,Xr,bi,vm,Zr,Li,bm,es,_i,Lm,ts,yi,_m,as,pe,ym,Ei,Em,Im,is,Ii,km,os,ki,Am,ls,m,rs,Bm,Co,xm,Tm,Mo,Pm,Cm,No,Mm,Nm,Oo,Om,Wm,Wo,Sm,Dm,So,Gm,qm,Do,Um,zm,Go,Rm,Hm,qo,jm,Vm,Uo,Fm,Jm,zo,Km,$m,Ro,Qm,Ym,Ho,Xm,Zm,jo,ep;return{c(){w=l("div"),A=l("img"),ae=d(),c=l("p"),M=i("Large image-text models like ALIGN, BASIC, Turing Bletchly, FLORENCE & GLIDE have shown better and better performance compared to previous flagship models like CLIP and DALL-E. Most of them had been trained on billions of image-text pairs and unfortunately, no datasets of this size had been openly available until now. To address this problem we present LAION 5B, a large-scale dataset for research purposes consisting of 5,85B CLIP-filtered image-text pairs. 2,3B contain English language, 2,2B samples from 100+ other languages and 1B samples have texts that do not allow a certain language assignment (e.g. names ). Additionally, we provide several nearest neighbor indices, an improved web interface for exploration & subset creation as well as detection scores for watermark and NSFW. We also announce a full reproduction of a clip training trained on LAION-400M at "),X=l("a"),_s=i("open_clip"),ys=i(". Explore the dataset at the "),Se=l("a"),Es=i("search demo"),Is=i(". See also the "),De=l("a"),ks=i("same post on laion website"),As=i("."),Jo=d(),B=l("p"),Bs=i("We thank our sponsors "),Ge=l("a"),xs=i("hugging face"),Ts=i(", "),qe=l("a"),Ps=i("doodlebot"),Cs=i(" and "),Ue=l("a"),Ms=i("stability"),Ns=i(" for providing us with computing resources to produce this dataset! We also thank the-eye.eu for hosting the image embeddings and a copy of the whole dataset."),Ko=d(),ze=l("h3"),Os=i("Disclaimer on dataset purpose and content warning"),$o=d(),Re=l("p"),Ws=i("The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a \u201Csafe\u201D subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress."),Qo=d(),He=l("h2"),Ss=i("Introduction"),Yo=d(),je=l("p"),Ds=i("Since the release of CLIP & DALL-E in January 2021, several similar large multi-modal language-vision models have been trained by large groups. Models like FLORENCE, Turing Bletchley, ALIGN & BASIC demonstrated very strong transfer capabilities on novel datasets in absence of per-sample labels, which also steadily improved when growing training data amount, following scaling laws observed in previous research work. These models require billions of image-text pairs to achieve competitive performances and unfortunately, no billion-scale image-text pair dataset had been openly available up until now. To address this problem we release LAION 5B, a CLIP-filtered dataset of 5,85 billion high-quality image-text pairs, their CLIP ViT-L/14 embeddings, kNN-indices, a web interface for exploration & subset-creation and NSFW- and watermark-detection scores and tools. We describe the procedure to create the dataset and demonstrate successful training of DALL-E architecture. Having sufficiently large scales, the dataset opens venues for research on multi-modal language-vision models to a broad community."),Xo=d(),Ve=l("h2"),Gs=i("Download the data"),Zo=d(),Fe=l("p"),qs=i("We release the following packages under the LAION-5B project:"),el=d(),D=l("ul"),Je=l("li"),Ke=l("a"),Us=i("laion2B-en"),zs=i(" 2.32 billion of these contain texts in the English language"),Rs=d(),$e=l("li"),Qe=l("a"),Hs=i("laion2B-multi"),js=i(" 2.26 billion contain texts from 100+ other languages"),Vs=d(),Ye=l("li"),Xe=l("a"),Fs=i("laion1B-nolang"),Js=i(" 1.27 billion have texts where a particular language couldn\u2019t be clearly detected."),tl=d(),ie=l("p"),Ks=i("The data can comfortably be downloaded with "),Ze=l("a"),$s=i("img2dataset"),Qs=i(" (240TB in 384, 80TB in 224)"),al=d(),oe=l("p"),Ys=i("For training usage, we recommend reading the "),et=l("a"),Xs=i("usage guide for training"),Zs=i("."),il=d(),tt=l("p"),en=i("In particular, we release this data:"),ol=d(),L=l("ul"),N=l("li"),tn=i("5.85 billion pairs of image URLs and the corresponding metadata at "),at=l("a"),an=i("laion2B-en"),on=d(),it=l("a"),ln=i("laion2B-multi"),rn=d(),ot=l("a"),sn=i("laion1B-nolang"),nn=i(" (800GB)"),fn=d(),Ee=l("li"),dn=i("A "),lt=l("a"),hn=i("knn index"),un=i(" that enables quick search in the laion5B dataset (1.6TB)"),mn=d(),rt=l("li"),st=l("a"),pn=i("Indices"),cn=i(" for laion2B-en, laion2B-multi, laion1B-nolang (2TB)"),gn=d(),Ie=l("li"),wn=i("Clip ViT-L/14 "),nt=l("a"),vn=i("image embeddings"),bn=i(" (9TB)"),Ln=d(),ft=l("li"),_n=i("Web demo of image-text search on LAION-5B "),dt=l("a"),yn=i("clip-retrieval"),En=d(),O=l("li"),In=i("Safety tags at "),ht=l("a"),kn=i("laion2B-en-safety"),An=d(),ut=l("a"),Bn=i("laion2B-multi-safety"),xn=d(),mt=l("a"),Tn=i("laion1B-nolang-safety"),Pn=i(" (50GB)"),Cn=d(),W=l("li"),Mn=i("Watermark tags at "),pt=l("a"),Nn=i("laion2B-en-watermark"),On=d(),ct=l("a"),Wn=i("laion2B-multi-watermark"),Sn=d(),gt=l("a"),Dn=i("laion1B-nolang-watermark"),Gn=i(" (50GB)"),ll=d(),le=l("p"),qn=i("The metadata files are parquet files that contain the following attributes: URL, TEXT, the cosine similarity score between the text and image embedding and height and width of the image. Watermark and safety tags can be joined with the metadata prior to downloading by using "),wt=l("a"),Un=i("this script"),zn=i(". Once that is done, they can easily be filtered upon with a probability threshold at your choice (we recommend 0.5 for safety and 0.8 for watermark)."),rl=d(),x=l("p"),Rn=i("You can also find the prejoined files at "),vt=l("a"),Hn=i("laion2B-en-joined"),jn=d(),bt=l("a"),Vn=i("laion2B-multi-joined"),Fn=d(),Lt=l("a"),Jn=i("laion1B-nolang-joined"),Kn=i(" (800GB)."),sl=d(),_t=l("h2"),$n=i("License"),nl=d(),re=l("p"),Qn=i("We distribute the metadata dataset (the parquet files) under the "),yt=l("a"),Yn=i("Creative Common CC-BY 4.0"),Xn=i(" license, which poses no particular restriction. The images are under their copyright."),fl=d(),Et=l("h2"),Zn=i("Dataset columns"),dl=d(),It=l("p"),ef=i("We provide these columns :"),hl=d(),b=l("ul"),qi=l("li"),tf=i("URL: the image url, millions of domains are covered"),af=d(),Ui=l("li"),of=i("TEXT: captions, in english for en, other languages for multi and nolang"),lf=d(),zi=l("li"),rf=i("WIDTH: picture width"),sf=d(),Ri=l("li"),nf=i("HEIGHT: picture height"),ff=d(),Hi=l("li"),df=i("LANGUAGE: the language of the sample, only for laion2B-multi, computed using cld3"),hf=d(),ji=l("li"),uf=i("similarity: cosine between text and image ViT-B/32 embeddings, clip for en, mclip for multi and nolang"),mf=d(),Vi=l("li"),pf=i("pwatermark: probability of being a watermarked image, computed using our watermark detector"),cf=d(),Fi=l("li"),gf=i("punsafe: probability of being an unsafe image, computed using our clip based detector"),ul=d(),se=l("p"),wf=i("pwatermark and punsafe are available either as individual collections that must be "),kt=l("a"),vf=i("joined"),bf=i(" with the hash of url+text, or as prejoined collections."),ml=d(),At=l("h2"),Lf=i("Dataset Statistics"),pl=d(),ne=l("p"),_f=i("We "),Bt=l("a"),yf=i("computed"),Ef=i(" some statistics on the datasets to let people understand better: Samples are considered unsafe if the model predicts it as unsafe with a probability of more than 0.5. More than 0.8 for watermark. These values are pretty conservative, so the estimated safeness and watermark proportion may be higher than the truth. Other thresholds may be chosen to get a different precision/recall tradeoff."),cl=d(),xt=l("p"),If=i("Computed quantiles are quantiles from 0.05 to 0.95."),gl=d(),G=l("p"),kf=i("Also see the whole "),Tt=l("a"),Af=i("sheet"),Bf=i(" and the whole "),Pt=l("a"),xf=i("dashboard"),Tf=i("."),wl=d(),Ct=l("h3"),Pf=i("Laion2B-en"),vl=d(),Mt=l("p"),Cf=i("Total: 2.3B samples"),bl=d(),Nt=l("img"),Ll=d(),Ot=l("p"),Mf=i("Number with height and width bigger than"),_l=d(),q=l("ul"),Ji=l("li"),Nf=i("256 -> 1324M"),Of=d(),Ki=l("li"),Wf=i("512 -> 488M"),Sf=d(),$i=l("li"),Df=i(`1024 -> 76M
Width quantiles: 132.0, 160.0, 180.0, 210.0, 225.0, 240.0, 262.0, 300.0, 309.0, 340.0, 400.0, 450.0, 480.0, 512.0, 600.0, 656.0, 760.0, 960.0, 1050.0
Height quantiles: 125.0, 150.0, 166.0, 188.0, 208.0, 225.0, 250.0, 270.0, 300.0, 320.0, 350.0, 380.0, 418.0, 470.0, 500.0, 600.0, 672.0, 800.0, 1014.0`),yl=d(),Wt=l("p"),Gf=i("Unsafe proportion: 2.9%"),El=d(),St=l("p"),qf=i("Watermark proportion: 6.1%"),Il=d(),Dt=l("p"),Uf=i("Average text length: 67"),kl=d(),Gt=l("p"),zf=i("Text length quantiles: 21.0, 25.0, 30.0, 33.0, 37.0, 40.0, 43.0, 47.0, 50.0, 54.0, 58.0, 62.0, 67.0, 72.0, 78.0, 85.0, 96.0, 114.0, 152.0"),Al=d(),qt=l("h3"),Rf=i("Laion2B-multi"),Bl=d(),Ut=l("p"),Hf=i("Total: 2.2B samples"),xl=d(),zt=l("img"),Tl=d(),Rt=l("p"),jf=i("Number with height and width bigger than"),Pl=d(),U=l("ul"),Qi=l("li"),Vf=i("256 -> 1299M"),Ff=d(),Yi=l("li"),Jf=i("512 -> 480M"),Kf=d(),Xi=l("li"),$f=i(`1024 -> 57M
Width quantiles: 140.0, 160.0, 188.0, 205.0, 235.0, 250.0, 284.0, 300.0, 324.0, 366.0, 420.0, 480.0, 520.0, 600.0, 640.0, 720.0, 800.0, 960.0, 1080.0
Height quantiles: 120.0, 144.0, 160.0, 180.0, 200.0, 217.0, 240.0, 262.0, 300.0, 320.0, 350.0, 394.0, 416.0, 458.0, 500.0, 564.0, 636.0, 725.0, 1000.0`),Cl=d(),Ht=l("p"),Qf=i("Top 10 languages: LANGUAGE count proportion:"),Ml=d(),g=l("ul"),Zi=l("li"),Yf=i("ru 241M 0.106"),Xf=d(),eo=l("li"),Zf=i("fr 168M 0.074"),ed=d(),to=l("li"),td=i("de 150M 0.066"),ad=d(),ao=l("li"),id=i("es 149M 0.066"),od=d(),io=l("li"),ld=i("zh 143M 0.063"),rd=d(),oo=l("li"),sd=i("ja 131M 0.057"),nd=d(),lo=l("li"),fd=i("it 95M 0.042"),dd=d(),ro=l("li"),hd=i("pt 88M 0.038"),ud=d(),so=l("li"),md=i("nl 66M 0.029"),pd=d(),no=l("li"),cd=i("pl 62M 0.027"),gd=d(),fo=l("li"),wd=i("no 49M 0.021"),Nl=d(),jt=l("p"),vd=i(`Unsafe proportion: 3.3%
Watermark proportion: 5.6%
Average text length: 52
Text length quantiles: 12.0, 16.0, 20.0, 23.0, 27.0, 30.0, 33.0, 37.0, 40.0, 44.0, 48.0, 52.0, 57.0, 61.0, 67.0, 74.0, 81.0, 93.0, 120.0`),Ol=d(),Vt=l("h3"),bd=i("Laion1B-nolang"),Wl=d(),Ft=l("p"),Ld=i("Total: 1.2B samples"),Sl=d(),Jt=l("img"),Dl=d(),Kt=l("p"),_d=i("Number with height and width bigger than"),Gl=d(),z=l("ul"),ho=l("li"),yd=i("256 -> 1324M"),Ed=d(),uo=l("li"),Id=i("512 -> 488M"),kd=d(),mo=l("li"),Ad=i(`1024 -> 76M
Width quantiles: 135.0, 160.0, 181.0, 207.0, 225.0, 241.0, 264.0, 300.0, 306.0, 338.0, 398.0, 426.0, 499.0, 520.0, 600.0, 655.0, 768.0, 940.0, 1080.0
Height quantiles: 118.0, 144.0, 160.0, 186.0, 200.0, 220.0, 240.0, 260.0, 292.0, 305.0, 338.0, 368.0, 405.0, 456.0, 500.0, 562.0, 637.0, 768.0, 1000.0`),ql=d(),$t=l("p"),Bd=i("Unsafe proportion: 3%"),Ul=d(),Qt=l("p"),xd=i("Watermark proportion: 4%"),zl=d(),Yt=l("p"),Td=i("Average text length: 46"),Rl=d(),Xt=l("p"),Pd=i("Text length quantiles: 13.0, 17.0, 20.0, 23.0, 26.0, 29.0, 32.0, 35.0, 38.0, 41.0, 44.0, 48.0, 51.0, 56.0, 60.0, 67.0, 73.0, 82.0, 99.0"),Hl=d(),Zt=l("h2"),Cd=i("Acquisition pipeline"),jl=d(),ea=l("img"),Vl=i(`
 
The acquisition pipeline follows the flowchart above and can be split into three major components:
* Distributed processing of petabyte-scale Common Crawl dataset, which produces a collection of matching URLs and captions (preprocessing phase)
* The distributed download of images based on shuffled data to pick a correct distribution of URLs, to avoid too heavy request loads on single websites
* Few GPU node post-processing of the data, which is much lighter and can be run in a few days, producing the final dataset.
`),ta=l("p"),Md=i(`###Distributed processing of Common Crawl
To create image-text pairs, we parse through WAT files from Common Crawl and parse out all HTML IMG tags containing an alt-text attribute. At the same time, we perform a language detection on text with three possible outputs: English language with confidence, another language with confidence, no language which contains \u201Cno detection\u201D and \u201Cdetection under the confidence threshold\u201D. The \u201Cno language\u201D set often contains short texts, mostly with names of people and places. All extracted information by the preprocessing workers were packed and sent to the Postgresql node for storage using the COPY command. The Postgresql server was maintained to keep about 500M records at all times by means of balancing the ingress and egress of data from the database.`),Fl=d(),aa=l("h3"),Nd=i("Distributed downloading of the images"),Jl=d(),ia=l("p"),Od=i("We download the raw images from the parsed URLs with asynchronous requests using Trio and Asks libraries in order to maximize all resources usage: vCPUs, RAM and bandwidth. We found that a single node in the cloud with 1-2 vCPUs, 0.5-1GB RAM and 5-10Mbps download bandwidth is inexpensive enough to allow downloading on a limited budget. Such a unit can process 10000 links in about 10-15 minutes. Each batch consisted of 10000 links taken from the Postgresql server by using the TABLESAMPLE technique, ensuring that the distribution among the 10000 links was following the distribution of the existing 500M records available on the database. We found that the distribution is still good when in the database are still above 20M records to be processed given that we had some 300 downloading workers at any time. The above techniques allowed both maximizing downloading speed and minimizing IP reputation damages."),Kl=d(),oa=l("h3"),Wd=i("CLIP inference at the post-processing stage"),$l=d(),la=l("p"),Sd=i("The data pipeline continued with GPU nodes doing inference on the collected image-text pairs, and calculating the similarity of the embeddings for the image and the text. After the similarity score was established we removed the pairs under the threshold we decided to use, i.e 0.28 for the English dataset ( with CLIP ViT B/32 ) and 0.26 for the rest (with mCLIP). As an estimation, we removed about 90% of the samples, trimming the 50+ billion of candidates to just below 6 billion."),Ql=d(),ra=l("h3"),Dd=i("Filtering out unsuitable image-text pairs"),Yl=d(),sa=l("p"),Gd=i("After downloading the WAT files from Common Crawl, we apply the following filtering conditions:"),Xl=d(),E=l("ul"),po=l("li"),qd=i("All samples with less than 5 characters alt-text length or less than 5 KB image size are dropped."),Ud=d(),co=l("li"),zd=i("All images with the too big resolution, potentially DOS bombs, were dropped before attempting to process them."),Rd=d(),go=l("li"),Hd=i("Duplicate removal is performed with a bloom filter based on URL. Future runs would include more variate deduplication rules, such as URL + language for the multilanguage dataset."),jd=d(),wo=l("li"),Vd=i("We use CLIP respectively MCLIP to compute embeddings of the image and alt-text. Then we compute the cosine similarity of both embeddings and drop all samples with cosine similarity below 0.28 for the English language ( with CLIP B/32) and 0.26 for the multilingual dataset (MCLIP). These thresholds were selected based on human inspection of the test results."),Fd=d(),vo=l("li"),Jd=i("We use the CLIP embeddings of images and texts to filter out to the possible extent the illegal content."),Zl=d(),na=l("h2"),Kd=i("Dataset preparation pipeline"),er=d(),T=l("p"),$d=i("After processing and filtering common crawl, 5,85B of URL/text samples remained. We did additional steps after that in order to prepare the dataset. See this "),fa=l("a"),Qd=i("semantic search blogpost"),Yd=i(" and the readme of "),da=l("a"),Xd=i("clip-retrieval"),Zd=i(" for additional details about this process. See also "),ha=l("a"),eh=i("semantic search at billions scale"),th=i(" for more technical details of the process that was done for laion5B."),tr=d(),P=l("ol"),bo=l("li"),ah=i("Downloading the data as webdataset with distributed img2dataset"),ih=d(),Lo=l("li"),oh=i("Computing Vit-L/14 embeddings with distributed clip-inference"),lh=d(),_o=l("li"),rh=i("Computing a KNN index from these embeddings using autofaiss"),sh=d(),yo=l("li"),nh=i("Computing additional tags (NSFW and watermark) using clip embeddings"),ar=d(),ua=l("h3"),fh=i("Distributed img2dataset"),ir=d(),R=l("p"),dh=i("We developed the "),ma=l("a"),hh=i("img2dataset"),uh=i(" library to comfortably download from a given set of URLs, resize and store the images and captions in the webdataset format. This allows downloading 100 million images from our list of URLs in 20 hours with a single node (1Gbps connection speed, 32GB of RAM, an i7 CPU with 16 cores), which allows anyone to obtain the whole dataset or a smaller subset. For LAION-5B we introduced a "),pa=l("a"),mh=i("distributed mode"),ph=i(" for this tool, allowing to downloading the 5,85B samples in a week using 10 nodes."),or=d(),ca=l("h3"),ch=i("Distributed clip inference"),lr=d(),H=l("p"),gh=i("From these images, the "),ga=l("a"),wh=i("clip retrieval"),vh=i(" inference tool was used to compute ViT-L/14 embeddings, allowing for a better analysis capacity of the data. In particular, a "),wa=l("a"),bh=i("distributed mode"),Lh=i(" made it possible to compute these embeddings in a week using 32 A100: this larger clip model can only be computed at a speed of 312 sample/s per GPU, compared to 1800 sample/s for ViT-B/32. The resulting embeddings are available for everyone to use e.g. for clustering, indexing, linear inference."),rr=d(),va=l("h3"),_h=i("Distributed indexing"),sr=d(),j=l("p"),yh=i("We then used these 9 TB of image embeddings to build a large PQ128 knn index using the "),ba=l("a"),Eh=i("autofaiss"),Ih=i(" tool. To make this run faster, a "),La=l("a"),kh=i("distributed mode"),Ah=i(" is available."),nr=d(),_a=l("h3"),Bh=i("Integration in the search UI"),fr=d(),V=l("p"),xh=i("In order to demonstrate the value of this data, we integrated this index into the "),ya=l("a"),Th=i("knn search UI"),Ph=i(". It is powered by the code called "),Ea=l("a"),Ch=i("clip back"),Mh=i(". The knn index is 800GB and the metadata (URL and captions) as well, so memory mapping is used for both in order to use no ram, only an SSD drive of that capacity is required."),dr=d(),Ia=l("h3"),Nh=i("Watermark and safety inference"),hr=d(),F=l("p"),Oh=i("We wanted to give users the ability to remove unsafe examples, and watermarked examples. To do that we collected training and test sets. The training set was augmented with examples retrieved from the knn index, while the test set samples were selected to represent well the dataset distribution, but were all manually annotated. The inference is done using the "),ka=l("a"),Wh=i("embedding-reader"),Sh=i(" module for NSFW and "),Aa=l("a"),Dh=i("LAION-5B-WatermarkDetection"),Gh=i(" for watermarks These tags were also integrated into the UI, allowing everyone to observe that the safety tags indeed filter out almost all the unsafe results, and giving confidence that training a generative model on this data will not result in unexpectedly unsafe images."),ur=d(),Ba=l("h3"),qh=i("Watermarks"),mr=d(),xa=l("img"),pr=d(),Ta=l("p"),Uh=i("The training dataset is 90000 samples (45222 watermarks, 44778 clear)."),cr=d(),Pa=l("p"),zh=i("Watermarked images are a big problem when training generative models like DALL-E or GLIDE. To tackle this problem we trained a watermark detection model and used it to calculate confidence scores for every image in LAION-5B. Therefore we created a training dataset consisting of 90.000 images with 50% watermarked and 50% clean images. The majority of the watermarked images have been extracted from the LAION-400M KNN index through the use of several text prompts like \u201Cclip art watermark\u201D, \u201Ccat watermark\u201D or \u201Clandscape watermark\u201D."),gr=d(),Ca=l("p"),Rh=i("The images in the cleaned category were composed of images from the Open Images dataset and images that contained texts, but no watermarks, like PPT slides and memes, also retrieved from the kNN indices of LAION-400M. While we tried to curate a test set to evaluate the quality of our watermark detection model, we realized that it is almost impossible to draw a clear line between what actually is a watermark and what is not. For example pictures with small transparent texts at the bottom had been considered by some people as watermarked, by others not."),wr=d(),Ma=l("p"),Hh=i("In the end we decided to choose a model based on our consensual judgment. It seems to be \u201Cgood\u201D at spotting obvious watermarks like those used on popular stock image sites. The creation of high-quality, openly accessible watermark detection test sets with clear and plausible definitions of what should be considered a watermark and what not, remains a challenge for future projects. Nevertheless we are convinced that removing images with a high confidence score for containing a watermark based on our model will significantly reduce the percentage of images that would be considered as obvious watermarks."),vr=d(),Z=l("p"),jh=i("The model is available at "),ke=l("a"),Vh=i("https://github.com/LAION-AI/watermark-detection"),Fh=i(" and "),Ae=l("a"),Jh=i("https://github.com/LAION-AI/LAION-5B-WatermarkDetection/releases/tag/1.0"),br=d(),Na=l("h3"),Kh=i("Safety"),Lr=d(),Oa=l("p"),$h=i("On a balanced manually annotated safety test set with 3000 samples:"),_r=d(),fe=l("ul"),Eo=l("li"),Qh=i("the accuracy of the B32 NSFW classifier is: 0.960"),Yh=d(),Io=l("li"),Xh=i("the accuracy of the ViT L 14 NSFW classifier is: 0.961"),yr=d(),_=l("p"),Zh=i("The model, as well as the training code, are available at "),Wa=l("a"),eu=i("CLIP-based-NSFW-Detector"),tu=i(". The tags are available at "),Sa=l("a"),au=i("laion2B-en-safety"),iu=d(),Da=l("a"),ou=i("laion2B-multi-safety"),lu=d(),Ga=l("a"),ru=i("laion1B-nolang-safety"),su=i(". Demo at "),qa=l("a"),nu=i("clip-retrieval"),fu=i(" (check/uncheck safe mode)"),Er=d(),Ua=l("h2"),du=i("Using LAION datasets"),Ir=d(),za=l("p"),hu=i("Laion5B and LAION-400M could e.g. be used to train"),kr=d(),J=l("ul"),ko=l("li"),uu=i("Generative models: training image/text generative models, e.g autoregressive models like DALL-E or diffusion models like GLIDE"),mu=d(),Ao=l("li"),pu=i("Models with contrastive losses: self-supervised training on image/text pairs using contrastive losses, e.g CLIP"),cu=d(),Bo=l("li"),gu=i("Classification models: e.g, performing zero-shot classification by extracting pseudo labels from queries on the dataset"),Ar=d(),Ra=l("p"),wu=i("We present here a few examples of models that were trained on our LAION datasets with success:"),Br=d(),Ha=l("h3"),vu=i("CLIP"),xr=d(),ja=l("p"),bu=i("We, LAION, are currently working together with the Cross Sectional Team Deep Learning (CST-DL), Scalable Learning and Multi-Purpose AI Lab (SLAMPAI) at the J\xFClich Supercomputing Centre (JSC) and the Open CLIP team in the replication of OpenAI\u2019s CLIP results."),Tr=d(),Va=l("img"),Pr=d(),Fa=l("p"),Lu=i("( The results in the right column are from our model. \u2013 huge thanks to Cade Gordon & Ross Wightman for performing the training run )"),Cr=d(),Be=l("p"),_u=i("The repository with the training code and the model checkpoints can be found here: "),xe=l("a"),yu=i("https://github.com/mlfoundations/open_clip"),Mr=d(),de=l("p"),Eu=i("We gratefully acknowledge the Gauss Centre for Supercomputing e.V. ("),Te=l("a"),Iu=i("www.gauss-centre.eu"),ku=i(") for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J\xFClich Supercomputing Centre (JSC)."),Nr=d(),Ja=l("h3"),Au=i("BLIP inference tuning"),Or=d(),ee=l("p"),Ka=l("a"),Bu=i("BLIP"),xu=i(" is a model that was trained for both image-text matching and image captioning. It was trained on a 115M subset of LAION-400M. To improve the results of the generated captions we (LAION) performed over 100 experiments to determine the hyperparameters that maximize the BLEU-4 score compared to MS COCO captions. Here you can see some of our "),$a=l("a"),Tu=i("results"),Pu=i("."),Wr=d(),Qa=l("img"),Sr=i(`
eval_best_auto0185: An orange cat is looking at its reflection in the mirror.
`),Ya=l("img"),Dr=i(`
eval_best_auto0190: A green highway sign with the words Queens Bronx.
 
 
We found that we can significantly improve the quality of the captions by generating 40 (or more) candidate captions for each image and then ranking them using OpenAI\u2019s CLIP ViT-L/14 & CLIP-Resnet50x64. First we ranked all candidates with ViT-L/14 and then we ranked the top-5 results again using Resnet50x64. Preliminary results of human evaluations indicate that:
`),he=l("ol"),xo=l("li"),Cu=i("Our evaluators gave the generated captions an average quality rating of 3,8 on a scale from 0 to 5, with a standard deviation of 0,9 ( in this particular hyperparameter configuration n= 600)"),Mu=d(),To=l("li"),Nu=i("Our evaluators gave original human captions from MS COCO an average quality rating of 3,9 with a standard deviation of 0,8 ( n = 2100 )"),Gr=d(),ue=l("p"),Ou=i("\u2014> We hypothesize that the generated captions match (& sometimes even surpass) the average quality of the human captions of MS COCO (which are sometimes also far from perfect) in most cases, but sometimes ( in less than "),Po=l("code"),Wu=i("<10%"),Su=i(" ) contain obvious mistakes, that humans would not make, because deeper kind of world knowledge & \u201Ecommon sense\u201C would be necessary in those cases."),qr=d(),Xa=l("h3"),Du=i("GLIDE"),Ur=d(),K=l("p"),Gu=i("Clay Mullis (alias "),Za=l("a"),qu=i("afiaka87"),Uu=i(") used subsets of LAON-2B to fine-tune the OpenAi "),ei=l("a"),zu=i("Glide"),Ru=i(" model and managed to reintroduce human generations. Samples"),zr=d(),$=l("p"),Pe=l("a"),Hu=i("https://replicate.com/afiaka87/laionide-v3"),ju=d(),Ce=l("a"),Vu=i("https://wandb.ai/afiaka87/glide_compare/reports/Finetuning-GLIDE-on-Laion5B\u2013VmlldzoxNTg3MTkz"),Fu=d(),Me=l("a"),Ju=i("https://wandb.ai/afiaka87/laionide-v3-glide/reports/Laionide-Version-3-Benchmark\u2013VmlldzoxNjE0MTE3"),Rr=d(),ti=l("img"),Hr=d(),ai=l("h3"),Ku=i("Semantic search and subset extraction"),jr=d(),me=l("p"),$u=i("The "),ii=l("a"),Qu=i("clip-retrieval"),Yu=i(" interface allows a user to search images and texts based on a query image or text using the CLIP embeddings of the input and our precomputed kNN indices. It demonstrates the diversity of images and captions that can be found in LAION-5B as well as high semantic relevance shows the distribution of image sizes of LAION-5B. Given the abundance of high-resolution images, one can produce subsets of images for training various customized models, and also choose image resolution that is suitable for the purpose of particular training."),Vr=d(),oi=l("h3"),Xu=i("CLOOB"),Fr=d(),Ne=l("p"),Zu=i("Katherine Crowson and John David Pressman recently trained a CLOOB ViT-B/16, variant of CLIP, for 32 epochs on LAION-400M and got preliminary results, that come close to the performance of OpenAI\u2019s ViT-B/32, even though this was an early run with unoptimized hyperparameters. The checkpoints can be found here: "),Oe=l("a"),em=i("https://github.com/crowsonkb/cloob-training"),Jr=d(),li=l("img"),Kr=i(`
(zero-shot accuracies on Imagenet-1K )
`),ri=l("p"),tm=i("We are in touch with Andreas F\xFCrst, one of the original CLOOB authors, and learned from him that their team is currently (at the time of writing) training a CLOOB ViT-B/32 with LAION-400M with optimized hyperparameters and very promising results so far (53% zero-shot accuracy on Imagenet after 7 epochs)."),$r=d(),si=l("h2"),am=i("Papers citing LAION 400M"),Qr=d(),ni=l("p"),im=i("After the release of LAION-400M, several papers used LAION-400M for image generation, text to image generation, image to text generation and text image matching:"),Yr=d(),I=l("ul"),fi=l("li"),di=l("a"),om=i("Vector Quantized Diffusion Model for Text-to-Image Synthesis"),lm=i(" used LAION-400M to train VQ diffusion text to image generation models"),rm=d(),hi=l("li"),ui=l("a"),sm=i("High-Resolution Image Synthesis with Latent Diffusion Models"),nm=i(" used a subset of LAION-400M to train latent diffusion models"),fm=d(),mi=l("li"),pi=l("a"),dm=i("General Facial Representation Learning in a Visual-Linguistic Manner"),hm=i(" LAION-400M face subset to train a face clip"),um=d(),ci=l("li"),gi=l("a"),mm=i("BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"),pm=i(" image captioning using LAION-400M subset"),cm=d(),wi=l("li"),vi=l("a"),gm=i("MAGMA \u2013 Multimodal Augmentation of Generative Models through Adapter-based Finetuning"),wm=i(" was trained on image question answering using a LAION-400M subset"),Xr=d(),bi=l("h2"),vm=i("Conclusion"),Zr=d(),Li=l("p"),bm=i("By releasing an updated version of an openly available dataset that contains 5 billion image-text pairs, we have set new Standards for the scale of openly available datasets and enable researchers from all over the world to train state-of-the-art language-vision models like GLIDE or Turing Bletchley. As proof of concept, we demonstrated that a subset of our dataset can be used to train various CLIP-like models, producing samples of sufficient quality. This dataset extends the possibilities in multi-language large-scale training and research of language-vision models, that were previously restricted to those having access to proprietary large datasets, to the broad community."),es=d(),_i=l("h2"),Lm=i("What\u2019s next?"),ts=d(),yi=l("p"),_m=i("This is only the beginning! Now that this huge and open dataset is released, it can be used to train many models, such as gigantic clip models, image/text generation models and much more. We have so many projects going on that it\u2019s probably best, if you are interested, to join our Discord server and check out what\u2019s going on. We are and always will be a grassroots community that works openly and welcomes everyone who is kind and passionate and for machine learning."),as=d(),pe=l("p"),ym=i("Join us in "),Ei=l("a"),Em=i("discord"),Im=i(" and help us to train models like CLIP, BLIP, GLIDE, Dall-E, SimMIM, AudioCLIP and don\u2019t hesitate to share your ideas for new projects with us."),is=d(),Ii=l("p"),km=i("Become a part of our constantly growing crowd of supporters who help us to make machine learning dreams come true!"),os=d(),ki=l("h2"),Am=i("Credit Assignment"),ls=d(),m=l("ul"),rs=l("li"),Bm=d(),Co=l("li"),xm=i("Christoph Schuhmann: He led this project and built POCs for most of its components including clip filtering,the safety model, the watermark model and the Blip inference tuning project."),Tm=d(),Mo=l("li"),Pm=i("Richard Vencu: System architecture and download script optimizations, GPU assisted filtering. Set up the AWS infrastructure."),Cm=d(),No=l("li"),Mm=i("Romain Beaumont: Guidance on scaling for the common crawl filtering pipeline. Built and ran the dataset preparation pipeline: pyspark deduplication job, img2dataset, clip inference, autofaiss, safety tags."),Nm=d(),Oo=l("li"),Om=i("Clayton Mullis: DALLE-pytorch training/analysis, glide training, WDS filtering"),Wm=d(),Wo=l("li"),Sm=i("Jenia Jitsev: scientific organization & writing, experiments planning and design, compute resource acquisition, general supervision"),Dm=d(),So=l("li"),Gm=i("Robert Kaczmarczyk: Established WDS architecture, performed DALL-E training runs, balancing calculation, sample (NSFW, watermark, caption quality) annotation and manuscript revision"),qm=d(),Do=l("li"),Um=i("Andreas K\xF6pf: He conducted the hyperparameter search for the inference strategies with the BLIP image-captioning model"),zm=d(),Go=l("li"),Rm=i("Theo Coomber: He was one of our first contributors & build the first versions of our worker swarm system. Without his enthusiasm this project might never have taken off."),Hm=d(),qo=l("li"),jm=i("Aarush Katta: Trained the watermark model"),Vm=d(),Uo=l("li"),Fm=i("Cade Gordon: Run distributed inference for the watermark tags & trained the CLIP B/32 model on JUWELS Booster"),Jm=d(),zo=l("li"),Km=i("Ross Wightman: Ross helped Cade with the debugging & training of the CLIP-B/32 model and executed experiments on JUWELS Booster"),$m=d(),Ro=l("li"),Qm=i("Katherine Crowson and John David Pressman: Trained the CLOOB model"),Ym=d(),Ho=l("li"),Xm=i("Aran Komatsuzaki: Led an image-text-pair dataset building project, which inspired this project."),Zm=d(),jo=l("li"),ep=i("Bokai Yu: Accomplished most of the work to make the knn index building tool autofaiss work in a distributed setting"),this.h()},l(e){w=r(e,"DIV",{class:!0});var n=s(w);A=r(n,"IMG",{src:!0}),n.forEach(t),ae=h(e),c=r(e,"P",{});var ce=s(c);M=o(ce,"Large image-text models like ALIGN, BASIC, Turing Bletchly, FLORENCE & GLIDE have shown better and better performance compared to previous flagship models like CLIP and DALL-E. Most of them had been trained on billions of image-text pairs and unfortunately, no datasets of this size had been openly available until now. To address this problem we present LAION 5B, a large-scale dataset for research purposes consisting of 5,85B CLIP-filtered image-text pairs. 2,3B contain English language, 2,2B samples from 100+ other languages and 1B samples have texts that do not allow a certain language assignment (e.g. names ). Additionally, we provide several nearest neighbor indices, an improved web interface for exploration & subset creation as well as detection scores for watermark and NSFW. We also announce a full reproduction of a clip training trained on LAION-400M at "),X=r(ce,"A",{href:!0});var Ip=s(X);_s=o(Ip,"open_clip"),Ip.forEach(t),ys=o(ce,". Explore the dataset at the "),Se=r(ce,"A",{href:!0});var kp=s(Se);Es=o(kp,"search demo"),kp.forEach(t),Is=o(ce,". See also the "),De=r(ce,"A",{href:!0});var Ap=s(De);ks=o(Ap,"same post on laion website"),Ap.forEach(t),As=o(ce,"."),ce.forEach(t),Jo=h(e),B=r(e,"P",{});var ge=s(B);Bs=o(ge,"We thank our sponsors "),Ge=r(ge,"A",{href:!0});var Bp=s(Ge);xs=o(Bp,"hugging face"),Bp.forEach(t),Ts=o(ge,", "),qe=r(ge,"A",{href:!0});var xp=s(qe);Ps=o(xp,"doodlebot"),xp.forEach(t),Cs=o(ge," and "),Ue=r(ge,"A",{href:!0});var Tp=s(Ue);Ms=o(Tp,"stability"),Tp.forEach(t),Ns=o(ge," for providing us with computing resources to produce this dataset! We also thank the-eye.eu for hosting the image embeddings and a copy of the whole dataset."),ge.forEach(t),Ko=h(e),ze=r(e,"H3",{});var Pp=s(ze);Os=o(Pp,"Disclaimer on dataset purpose and content warning"),Pp.forEach(t),$o=h(e),Re=r(e,"P",{});var Cp=s(Re);Ws=o(Cp,"The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a \u201Csafe\u201D subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress."),Cp.forEach(t),Qo=h(e),He=r(e,"H2",{});var Mp=s(He);Ss=o(Mp,"Introduction"),Mp.forEach(t),Yo=h(e),je=r(e,"P",{});var Np=s(je);Ds=o(Np,"Since the release of CLIP & DALL-E in January 2021, several similar large multi-modal language-vision models have been trained by large groups. Models like FLORENCE, Turing Bletchley, ALIGN & BASIC demonstrated very strong transfer capabilities on novel datasets in absence of per-sample labels, which also steadily improved when growing training data amount, following scaling laws observed in previous research work. These models require billions of image-text pairs to achieve competitive performances and unfortunately, no billion-scale image-text pair dataset had been openly available up until now. To address this problem we release LAION 5B, a CLIP-filtered dataset of 5,85 billion high-quality image-text pairs, their CLIP ViT-L/14 embeddings, kNN-indices, a web interface for exploration & subset-creation and NSFW- and watermark-detection scores and tools. We describe the procedure to create the dataset and demonstrate successful training of DALL-E architecture. Having sufficiently large scales, the dataset opens venues for research on multi-modal language-vision models to a broad community."),Np.forEach(t),Xo=h(e),Ve=r(e,"H2",{});var Op=s(Ve);Gs=o(Op,"Download the data"),Op.forEach(t),Zo=h(e),Fe=r(e,"P",{});var Wp=s(Fe);qs=o(Wp,"We release the following packages under the LAION-5B project:"),Wp.forEach(t),el=h(e),D=r(e,"UL",{});var Ai=s(D);Je=r(Ai,"LI",{});var tp=s(Je);Ke=r(tp,"A",{href:!0});var Sp=s(Ke);Us=o(Sp,"laion2B-en"),Sp.forEach(t),zs=o(tp," 2.32 billion of these contain texts in the English language"),tp.forEach(t),Rs=h(Ai),$e=r(Ai,"LI",{});var ap=s($e);Qe=r(ap,"A",{href:!0});var Dp=s(Qe);Hs=o(Dp,"laion2B-multi"),Dp.forEach(t),js=o(ap," 2.26 billion contain texts from 100+ other languages"),ap.forEach(t),Vs=h(Ai),Ye=r(Ai,"LI",{});var ip=s(Ye);Xe=r(ip,"A",{href:!0});var Gp=s(Xe);Fs=o(Gp,"laion1B-nolang"),Gp.forEach(t),Js=o(ip," 1.27 billion have texts where a particular language couldn\u2019t be clearly detected."),ip.forEach(t),Ai.forEach(t),tl=h(e),ie=r(e,"P",{});var ss=s(ie);Ks=o(ss,"The data can comfortably be downloaded with "),Ze=r(ss,"A",{href:!0});var qp=s(Ze);$s=o(qp,"img2dataset"),qp.forEach(t),Qs=o(ss," (240TB in 384, 80TB in 224)"),ss.forEach(t),al=h(e),oe=r(e,"P",{});var ns=s(oe);Ys=o(ns,"For training usage, we recommend reading the "),et=r(ns,"A",{href:!0});var Up=s(et);Xs=o(Up,"usage guide for training"),Up.forEach(t),Zs=o(ns,"."),ns.forEach(t),il=h(e),tt=r(e,"P",{});var zp=s(tt);en=o(zp,"In particular, we release this data:"),zp.forEach(t),ol=h(e),L=r(e,"UL",{});var k=s(L);N=r(k,"LI",{});var we=s(N);tn=o(we,"5.85 billion pairs of image URLs and the corresponding metadata at "),at=r(we,"A",{href:!0});var Rp=s(at);an=o(Rp,"laion2B-en"),Rp.forEach(t),on=h(we),it=r(we,"A",{href:!0});var Hp=s(it);ln=o(Hp,"laion2B-multi"),Hp.forEach(t),rn=h(we),ot=r(we,"A",{href:!0});var jp=s(ot);sn=o(jp,"laion1B-nolang"),jp.forEach(t),nn=o(we," (800GB)"),we.forEach(t),fn=h(k),Ee=r(k,"LI",{});var fs=s(Ee);dn=o(fs,"A "),lt=r(fs,"A",{href:!0});var Vp=s(lt);hn=o(Vp,"knn index"),Vp.forEach(t),un=o(fs," that enables quick search in the laion5B dataset (1.6TB)"),fs.forEach(t),mn=h(k),rt=r(k,"LI",{});var op=s(rt);st=r(op,"A",{href:!0});var Fp=s(st);pn=o(Fp,"Indices"),Fp.forEach(t),cn=o(op," for laion2B-en, laion2B-multi, laion1B-nolang (2TB)"),op.forEach(t),gn=h(k),Ie=r(k,"LI",{});var ds=s(Ie);wn=o(ds,"Clip ViT-L/14 "),nt=r(ds,"A",{href:!0});var Jp=s(nt);vn=o(Jp,"image embeddings"),Jp.forEach(t),bn=o(ds," (9TB)"),ds.forEach(t),Ln=h(k),ft=r(k,"LI",{});var lp=s(ft);_n=o(lp,"Web demo of image-text search on LAION-5B "),dt=r(lp,"A",{href:!0});var Kp=s(dt);yn=o(Kp,"clip-retrieval"),Kp.forEach(t),lp.forEach(t),En=h(k),O=r(k,"LI",{});var ve=s(O);In=o(ve,"Safety tags at "),ht=r(ve,"A",{href:!0});var $p=s(ht);kn=o($p,"laion2B-en-safety"),$p.forEach(t),An=h(ve),ut=r(ve,"A",{href:!0});var Qp=s(ut);Bn=o(Qp,"laion2B-multi-safety"),Qp.forEach(t),xn=h(ve),mt=r(ve,"A",{href:!0});var Yp=s(mt);Tn=o(Yp,"laion1B-nolang-safety"),Yp.forEach(t),Pn=o(ve," (50GB)"),ve.forEach(t),Cn=h(k),W=r(k,"LI",{});var be=s(W);Mn=o(be,"Watermark tags at "),pt=r(be,"A",{href:!0});var Xp=s(pt);Nn=o(Xp,"laion2B-en-watermark"),Xp.forEach(t),On=h(be),ct=r(be,"A",{href:!0});var Zp=s(ct);Wn=o(Zp,"laion2B-multi-watermark"),Zp.forEach(t),Sn=h(be),gt=r(be,"A",{href:!0});var ec=s(gt);Dn=o(ec,"laion1B-nolang-watermark"),ec.forEach(t),Gn=o(be," (50GB)"),be.forEach(t),k.forEach(t),ll=h(e),le=r(e,"P",{});var hs=s(le);qn=o(hs,"The metadata files are parquet files that contain the following attributes: URL, TEXT, the cosine similarity score between the text and image embedding and height and width of the image. Watermark and safety tags can be joined with the metadata prior to downloading by using "),wt=r(hs,"A",{href:!0});var tc=s(wt);Un=o(tc,"this script"),tc.forEach(t),zn=o(hs,". Once that is done, they can easily be filtered upon with a probability threshold at your choice (we recommend 0.5 for safety and 0.8 for watermark)."),hs.forEach(t),rl=h(e),x=r(e,"P",{});var Le=s(x);Rn=o(Le,"You can also find the prejoined files at "),vt=r(Le,"A",{href:!0});var ac=s(vt);Hn=o(ac,"laion2B-en-joined"),ac.forEach(t),jn=h(Le),bt=r(Le,"A",{href:!0});var ic=s(bt);Vn=o(ic,"laion2B-multi-joined"),ic.forEach(t),Fn=h(Le),Lt=r(Le,"A",{href:!0});var oc=s(Lt);Jn=o(oc,"laion1B-nolang-joined"),oc.forEach(t),Kn=o(Le," (800GB)."),Le.forEach(t),sl=h(e),_t=r(e,"H2",{});var lc=s(_t);$n=o(lc,"License"),lc.forEach(t),nl=h(e),re=r(e,"P",{});var us=s(re);Qn=o(us,"We distribute the metadata dataset (the parquet files) under the "),yt=r(us,"A",{href:!0});var rc=s(yt);Yn=o(rc,"Creative Common CC-BY 4.0"),rc.forEach(t),Xn=o(us," license, which poses no particular restriction. The images are under their copyright."),us.forEach(t),fl=h(e),Et=r(e,"H2",{});var sc=s(Et);Zn=o(sc,"Dataset columns"),sc.forEach(t),dl=h(e),It=r(e,"P",{});var nc=s(It);ef=o(nc,"We provide these columns :"),nc.forEach(t),hl=h(e),b=r(e,"UL",{});var y=s(b);qi=r(y,"LI",{});var fc=s(qi);tf=o(fc,"URL: the image url, millions of domains are covered"),fc.forEach(t),af=h(y),Ui=r(y,"LI",{});var dc=s(Ui);of=o(dc,"TEXT: captions, in english for en, other languages for multi and nolang"),dc.forEach(t),lf=h(y),zi=r(y,"LI",{});var hc=s(zi);rf=o(hc,"WIDTH: picture width"),hc.forEach(t),sf=h(y),Ri=r(y,"LI",{});var uc=s(Ri);nf=o(uc,"HEIGHT: picture height"),uc.forEach(t),ff=h(y),Hi=r(y,"LI",{});var mc=s(Hi);df=o(mc,"LANGUAGE: the language of the sample, only for laion2B-multi, computed using cld3"),mc.forEach(t),hf=h(y),ji=r(y,"LI",{});var pc=s(ji);uf=o(pc,"similarity: cosine between text and image ViT-B/32 embeddings, clip for en, mclip for multi and nolang"),pc.forEach(t),mf=h(y),Vi=r(y,"LI",{});var cc=s(Vi);pf=o(cc,"pwatermark: probability of being a watermarked image, computed using our watermark detector"),cc.forEach(t),cf=h(y),Fi=r(y,"LI",{});var gc=s(Fi);gf=o(gc,"punsafe: probability of being an unsafe image, computed using our clip based detector"),gc.forEach(t),y.forEach(t),ul=h(e),se=r(e,"P",{});var ms=s(se);wf=o(ms,"pwatermark and punsafe are available either as individual collections that must be "),kt=r(ms,"A",{href:!0});var wc=s(kt);vf=o(wc,"joined"),wc.forEach(t),bf=o(ms," with the hash of url+text, or as prejoined collections."),ms.forEach(t),ml=h(e),At=r(e,"H2",{});var vc=s(At);Lf=o(vc,"Dataset Statistics"),vc.forEach(t),pl=h(e),ne=r(e,"P",{});var ps=s(ne);_f=o(ps,"We "),Bt=r(ps,"A",{href:!0});var bc=s(Bt);yf=o(bc,"computed"),bc.forEach(t),Ef=o(ps," some statistics on the datasets to let people understand better: Samples are considered unsafe if the model predicts it as unsafe with a probability of more than 0.5. More than 0.8 for watermark. These values are pretty conservative, so the estimated safeness and watermark proportion may be higher than the truth. Other thresholds may be chosen to get a different precision/recall tradeoff."),ps.forEach(t),cl=h(e),xt=r(e,"P",{});var Lc=s(xt);If=o(Lc,"Computed quantiles are quantiles from 0.05 to 0.95."),Lc.forEach(t),gl=h(e),G=r(e,"P",{});var Bi=s(G);kf=o(Bi,"Also see the whole "),Tt=r(Bi,"A",{href:!0});var _c=s(Tt);Af=o(_c,"sheet"),_c.forEach(t),Bf=o(Bi," and the whole "),Pt=r(Bi,"A",{href:!0});var yc=s(Pt);xf=o(yc,"dashboard"),yc.forEach(t),Tf=o(Bi,"."),Bi.forEach(t),wl=h(e),Ct=r(e,"H3",{});var Ec=s(Ct);Pf=o(Ec,"Laion2B-en"),Ec.forEach(t),vl=h(e),Mt=r(e,"P",{});var Ic=s(Mt);Cf=o(Ic,"Total: 2.3B samples"),Ic.forEach(t),bl=h(e),Nt=r(e,"IMG",{src:!0}),Ll=h(e),Ot=r(e,"P",{});var kc=s(Ot);Mf=o(kc,"Number with height and width bigger than"),kc.forEach(t),_l=h(e),q=r(e,"UL",{});var xi=s(q);Ji=r(xi,"LI",{});var Ac=s(Ji);Nf=o(Ac,"256 -> 1324M"),Ac.forEach(t),Of=h(xi),Ki=r(xi,"LI",{});var Bc=s(Ki);Wf=o(Bc,"512 -> 488M"),Bc.forEach(t),Sf=h(xi),$i=r(xi,"LI",{});var xc=s($i);Df=o(xc,`1024 -> 76M
Width quantiles: 132.0, 160.0, 180.0, 210.0, 225.0, 240.0, 262.0, 300.0, 309.0, 340.0, 400.0, 450.0, 480.0, 512.0, 600.0, 656.0, 760.0, 960.0, 1050.0
Height quantiles: 125.0, 150.0, 166.0, 188.0, 208.0, 225.0, 250.0, 270.0, 300.0, 320.0, 350.0, 380.0, 418.0, 470.0, 500.0, 600.0, 672.0, 800.0, 1014.0`),xc.forEach(t),xi.forEach(t),yl=h(e),Wt=r(e,"P",{});var Tc=s(Wt);Gf=o(Tc,"Unsafe proportion: 2.9%"),Tc.forEach(t),El=h(e),St=r(e,"P",{});var Pc=s(St);qf=o(Pc,"Watermark proportion: 6.1%"),Pc.forEach(t),Il=h(e),Dt=r(e,"P",{});var Cc=s(Dt);Uf=o(Cc,"Average text length: 67"),Cc.forEach(t),kl=h(e),Gt=r(e,"P",{});var Mc=s(Gt);zf=o(Mc,"Text length quantiles: 21.0, 25.0, 30.0, 33.0, 37.0, 40.0, 43.0, 47.0, 50.0, 54.0, 58.0, 62.0, 67.0, 72.0, 78.0, 85.0, 96.0, 114.0, 152.0"),Mc.forEach(t),Al=h(e),qt=r(e,"H3",{});var Nc=s(qt);Rf=o(Nc,"Laion2B-multi"),Nc.forEach(t),Bl=h(e),Ut=r(e,"P",{});var Oc=s(Ut);Hf=o(Oc,"Total: 2.2B samples"),Oc.forEach(t),xl=h(e),zt=r(e,"IMG",{src:!0}),Tl=h(e),Rt=r(e,"P",{});var Wc=s(Rt);jf=o(Wc,"Number with height and width bigger than"),Wc.forEach(t),Pl=h(e),U=r(e,"UL",{});var Ti=s(U);Qi=r(Ti,"LI",{});var Sc=s(Qi);Vf=o(Sc,"256 -> 1299M"),Sc.forEach(t),Ff=h(Ti),Yi=r(Ti,"LI",{});var Dc=s(Yi);Jf=o(Dc,"512 -> 480M"),Dc.forEach(t),Kf=h(Ti),Xi=r(Ti,"LI",{});var Gc=s(Xi);$f=o(Gc,`1024 -> 57M
Width quantiles: 140.0, 160.0, 188.0, 205.0, 235.0, 250.0, 284.0, 300.0, 324.0, 366.0, 420.0, 480.0, 520.0, 600.0, 640.0, 720.0, 800.0, 960.0, 1080.0
Height quantiles: 120.0, 144.0, 160.0, 180.0, 200.0, 217.0, 240.0, 262.0, 300.0, 320.0, 350.0, 394.0, 416.0, 458.0, 500.0, 564.0, 636.0, 725.0, 1000.0`),Gc.forEach(t),Ti.forEach(t),Cl=h(e),Ht=r(e,"P",{});var qc=s(Ht);Qf=o(qc,"Top 10 languages: LANGUAGE count proportion:"),qc.forEach(t),Ml=h(e),g=r(e,"UL",{});var v=s(g);Zi=r(v,"LI",{});var Uc=s(Zi);Yf=o(Uc,"ru 241M 0.106"),Uc.forEach(t),Xf=h(v),eo=r(v,"LI",{});var zc=s(eo);Zf=o(zc,"fr 168M 0.074"),zc.forEach(t),ed=h(v),to=r(v,"LI",{});var Rc=s(to);td=o(Rc,"de 150M 0.066"),Rc.forEach(t),ad=h(v),ao=r(v,"LI",{});var Hc=s(ao);id=o(Hc,"es 149M 0.066"),Hc.forEach(t),od=h(v),io=r(v,"LI",{});var jc=s(io);ld=o(jc,"zh 143M 0.063"),jc.forEach(t),rd=h(v),oo=r(v,"LI",{});var Vc=s(oo);sd=o(Vc,"ja 131M 0.057"),Vc.forEach(t),nd=h(v),lo=r(v,"LI",{});var Fc=s(lo);fd=o(Fc,"it 95M 0.042"),Fc.forEach(t),dd=h(v),ro=r(v,"LI",{});var Jc=s(ro);hd=o(Jc,"pt 88M 0.038"),Jc.forEach(t),ud=h(v),so=r(v,"LI",{});var Kc=s(so);md=o(Kc,"nl 66M 0.029"),Kc.forEach(t),pd=h(v),no=r(v,"LI",{});var $c=s(no);cd=o($c,"pl 62M 0.027"),$c.forEach(t),gd=h(v),fo=r(v,"LI",{});var Qc=s(fo);wd=o(Qc,"no 49M 0.021"),Qc.forEach(t),v.forEach(t),Nl=h(e),jt=r(e,"P",{});var Yc=s(jt);vd=o(Yc,`Unsafe proportion: 3.3%
Watermark proportion: 5.6%
Average text length: 52
Text length quantiles: 12.0, 16.0, 20.0, 23.0, 27.0, 30.0, 33.0, 37.0, 40.0, 44.0, 48.0, 52.0, 57.0, 61.0, 67.0, 74.0, 81.0, 93.0, 120.0`),Yc.forEach(t),Ol=h(e),Vt=r(e,"H3",{});var Xc=s(Vt);bd=o(Xc,"Laion1B-nolang"),Xc.forEach(t),Wl=h(e),Ft=r(e,"P",{});var Zc=s(Ft);Ld=o(Zc,"Total: 1.2B samples"),Zc.forEach(t),Sl=h(e),Jt=r(e,"IMG",{src:!0}),Dl=h(e),Kt=r(e,"P",{});var e0=s(Kt);_d=o(e0,"Number with height and width bigger than"),e0.forEach(t),Gl=h(e),z=r(e,"UL",{});var Pi=s(z);ho=r(Pi,"LI",{});var t0=s(ho);yd=o(t0,"256 -> 1324M"),t0.forEach(t),Ed=h(Pi),uo=r(Pi,"LI",{});var a0=s(uo);Id=o(a0,"512 -> 488M"),a0.forEach(t),kd=h(Pi),mo=r(Pi,"LI",{});var i0=s(mo);Ad=o(i0,`1024 -> 76M
Width quantiles: 135.0, 160.0, 181.0, 207.0, 225.0, 241.0, 264.0, 300.0, 306.0, 338.0, 398.0, 426.0, 499.0, 520.0, 600.0, 655.0, 768.0, 940.0, 1080.0
Height quantiles: 118.0, 144.0, 160.0, 186.0, 200.0, 220.0, 240.0, 260.0, 292.0, 305.0, 338.0, 368.0, 405.0, 456.0, 500.0, 562.0, 637.0, 768.0, 1000.0`),i0.forEach(t),Pi.forEach(t),ql=h(e),$t=r(e,"P",{});var o0=s($t);Bd=o(o0,"Unsafe proportion: 3%"),o0.forEach(t),Ul=h(e),Qt=r(e,"P",{});var l0=s(Qt);xd=o(l0,"Watermark proportion: 4%"),l0.forEach(t),zl=h(e),Yt=r(e,"P",{});var r0=s(Yt);Td=o(r0,"Average text length: 46"),r0.forEach(t),Rl=h(e),Xt=r(e,"P",{});var s0=s(Xt);Pd=o(s0,"Text length quantiles: 13.0, 17.0, 20.0, 23.0, 26.0, 29.0, 32.0, 35.0, 38.0, 41.0, 44.0, 48.0, 51.0, 56.0, 60.0, 67.0, 73.0, 82.0, 99.0"),s0.forEach(t),Hl=h(e),Zt=r(e,"H2",{});var n0=s(Zt);Cd=o(n0,"Acquisition pipeline"),n0.forEach(t),jl=h(e),ea=r(e,"IMG",{src:!0}),Vl=o(e,`
 
The acquisition pipeline follows the flowchart above and can be split into three major components:
* Distributed processing of petabyte-scale Common Crawl dataset, which produces a collection of matching URLs and captions (preprocessing phase)
* The distributed download of images based on shuffled data to pick a correct distribution of URLs, to avoid too heavy request loads on single websites
* Few GPU node post-processing of the data, which is much lighter and can be run in a few days, producing the final dataset.
`),ta=r(e,"P",{});var f0=s(ta);Md=o(f0,`###Distributed processing of Common Crawl
To create image-text pairs, we parse through WAT files from Common Crawl and parse out all HTML IMG tags containing an alt-text attribute. At the same time, we perform a language detection on text with three possible outputs: English language with confidence, another language with confidence, no language which contains \u201Cno detection\u201D and \u201Cdetection under the confidence threshold\u201D. The \u201Cno language\u201D set often contains short texts, mostly with names of people and places. All extracted information by the preprocessing workers were packed and sent to the Postgresql node for storage using the COPY command. The Postgresql server was maintained to keep about 500M records at all times by means of balancing the ingress and egress of data from the database.`),f0.forEach(t),Fl=h(e),aa=r(e,"H3",{});var d0=s(aa);Nd=o(d0,"Distributed downloading of the images"),d0.forEach(t),Jl=h(e),ia=r(e,"P",{});var h0=s(ia);Od=o(h0,"We download the raw images from the parsed URLs with asynchronous requests using Trio and Asks libraries in order to maximize all resources usage: vCPUs, RAM and bandwidth. We found that a single node in the cloud with 1-2 vCPUs, 0.5-1GB RAM and 5-10Mbps download bandwidth is inexpensive enough to allow downloading on a limited budget. Such a unit can process 10000 links in about 10-15 minutes. Each batch consisted of 10000 links taken from the Postgresql server by using the TABLESAMPLE technique, ensuring that the distribution among the 10000 links was following the distribution of the existing 500M records available on the database. We found that the distribution is still good when in the database are still above 20M records to be processed given that we had some 300 downloading workers at any time. The above techniques allowed both maximizing downloading speed and minimizing IP reputation damages."),h0.forEach(t),Kl=h(e),oa=r(e,"H3",{});var u0=s(oa);Wd=o(u0,"CLIP inference at the post-processing stage"),u0.forEach(t),$l=h(e),la=r(e,"P",{});var m0=s(la);Sd=o(m0,"The data pipeline continued with GPU nodes doing inference on the collected image-text pairs, and calculating the similarity of the embeddings for the image and the text. After the similarity score was established we removed the pairs under the threshold we decided to use, i.e 0.28 for the English dataset ( with CLIP ViT B/32 ) and 0.26 for the rest (with mCLIP). As an estimation, we removed about 90% of the samples, trimming the 50+ billion of candidates to just below 6 billion."),m0.forEach(t),Ql=h(e),ra=r(e,"H3",{});var p0=s(ra);Dd=o(p0,"Filtering out unsuitable image-text pairs"),p0.forEach(t),Yl=h(e),sa=r(e,"P",{});var c0=s(sa);Gd=o(c0,"After downloading the WAT files from Common Crawl, we apply the following filtering conditions:"),c0.forEach(t),Xl=h(e),E=r(e,"UL",{});var Q=s(E);po=r(Q,"LI",{});var g0=s(po);qd=o(g0,"All samples with less than 5 characters alt-text length or less than 5 KB image size are dropped."),g0.forEach(t),Ud=h(Q),co=r(Q,"LI",{});var w0=s(co);zd=o(w0,"All images with the too big resolution, potentially DOS bombs, were dropped before attempting to process them."),w0.forEach(t),Rd=h(Q),go=r(Q,"LI",{});var v0=s(go);Hd=o(v0,"Duplicate removal is performed with a bloom filter based on URL. Future runs would include more variate deduplication rules, such as URL + language for the multilanguage dataset."),v0.forEach(t),jd=h(Q),wo=r(Q,"LI",{});var b0=s(wo);Vd=o(b0,"We use CLIP respectively MCLIP to compute embeddings of the image and alt-text. Then we compute the cosine similarity of both embeddings and drop all samples with cosine similarity below 0.28 for the English language ( with CLIP B/32) and 0.26 for the multilingual dataset (MCLIP). These thresholds were selected based on human inspection of the test results."),b0.forEach(t),Fd=h(Q),vo=r(Q,"LI",{});var L0=s(vo);Jd=o(L0,"We use the CLIP embeddings of images and texts to filter out to the possible extent the illegal content."),L0.forEach(t),Q.forEach(t),Zl=h(e),na=r(e,"H2",{});var _0=s(na);Kd=o(_0,"Dataset preparation pipeline"),_0.forEach(t),er=h(e),T=r(e,"P",{});var _e=s(T);$d=o(_e,"After processing and filtering common crawl, 5,85B of URL/text samples remained. We did additional steps after that in order to prepare the dataset. See this "),fa=r(_e,"A",{href:!0});var y0=s(fa);Qd=o(y0,"semantic search blogpost"),y0.forEach(t),Yd=o(_e," and the readme of "),da=r(_e,"A",{href:!0});var E0=s(da);Xd=o(E0,"clip-retrieval"),E0.forEach(t),Zd=o(_e," for additional details about this process. See also "),ha=r(_e,"A",{href:!0});var I0=s(ha);eh=o(I0,"semantic search at billions scale"),I0.forEach(t),th=o(_e," for more technical details of the process that was done for laion5B."),_e.forEach(t),tr=h(e),P=r(e,"OL",{});var ye=s(P);bo=r(ye,"LI",{});var k0=s(bo);ah=o(k0,"Downloading the data as webdataset with distributed img2dataset"),k0.forEach(t),ih=h(ye),Lo=r(ye,"LI",{});var A0=s(Lo);oh=o(A0,"Computing Vit-L/14 embeddings with distributed clip-inference"),A0.forEach(t),lh=h(ye),_o=r(ye,"LI",{});var B0=s(_o);rh=o(B0,"Computing a KNN index from these embeddings using autofaiss"),B0.forEach(t),sh=h(ye),yo=r(ye,"LI",{});var x0=s(yo);nh=o(x0,"Computing additional tags (NSFW and watermark) using clip embeddings"),x0.forEach(t),ye.forEach(t),ar=h(e),ua=r(e,"H3",{});var T0=s(ua);fh=o(T0,"Distributed img2dataset"),T0.forEach(t),ir=h(e),R=r(e,"P",{});var Ci=s(R);dh=o(Ci,"We developed the "),ma=r(Ci,"A",{href:!0});var P0=s(ma);hh=o(P0,"img2dataset"),P0.forEach(t),uh=o(Ci," library to comfortably download from a given set of URLs, resize and store the images and captions in the webdataset format. This allows downloading 100 million images from our list of URLs in 20 hours with a single node (1Gbps connection speed, 32GB of RAM, an i7 CPU with 16 cores), which allows anyone to obtain the whole dataset or a smaller subset. For LAION-5B we introduced a "),pa=r(Ci,"A",{href:!0});var C0=s(pa);mh=o(C0,"distributed mode"),C0.forEach(t),ph=o(Ci," for this tool, allowing to downloading the 5,85B samples in a week using 10 nodes."),Ci.forEach(t),or=h(e),ca=r(e,"H3",{});var M0=s(ca);ch=o(M0,"Distributed clip inference"),M0.forEach(t),lr=h(e),H=r(e,"P",{});var Mi=s(H);gh=o(Mi,"From these images, the "),ga=r(Mi,"A",{href:!0});var N0=s(ga);wh=o(N0,"clip retrieval"),N0.forEach(t),vh=o(Mi," inference tool was used to compute ViT-L/14 embeddings, allowing for a better analysis capacity of the data. In particular, a "),wa=r(Mi,"A",{href:!0});var O0=s(wa);bh=o(O0,"distributed mode"),O0.forEach(t),Lh=o(Mi," made it possible to compute these embeddings in a week using 32 A100: this larger clip model can only be computed at a speed of 312 sample/s per GPU, compared to 1800 sample/s for ViT-B/32. The resulting embeddings are available for everyone to use e.g. for clustering, indexing, linear inference."),Mi.forEach(t),rr=h(e),va=r(e,"H3",{});var W0=s(va);_h=o(W0,"Distributed indexing"),W0.forEach(t),sr=h(e),j=r(e,"P",{});var Ni=s(j);yh=o(Ni,"We then used these 9 TB of image embeddings to build a large PQ128 knn index using the "),ba=r(Ni,"A",{href:!0});var S0=s(ba);Eh=o(S0,"autofaiss"),S0.forEach(t),Ih=o(Ni," tool. To make this run faster, a "),La=r(Ni,"A",{href:!0});var D0=s(La);kh=o(D0,"distributed mode"),D0.forEach(t),Ah=o(Ni," is available."),Ni.forEach(t),nr=h(e),_a=r(e,"H3",{});var G0=s(_a);Bh=o(G0,"Integration in the search UI"),G0.forEach(t),fr=h(e),V=r(e,"P",{});var Oi=s(V);xh=o(Oi,"In order to demonstrate the value of this data, we integrated this index into the "),ya=r(Oi,"A",{href:!0});var q0=s(ya);Th=o(q0,"knn search UI"),q0.forEach(t),Ph=o(Oi,". It is powered by the code called "),Ea=r(Oi,"A",{href:!0});var U0=s(Ea);Ch=o(U0,"clip back"),U0.forEach(t),Mh=o(Oi,". The knn index is 800GB and the metadata (URL and captions) as well, so memory mapping is used for both in order to use no ram, only an SSD drive of that capacity is required."),Oi.forEach(t),dr=h(e),Ia=r(e,"H3",{});var z0=s(Ia);Nh=o(z0,"Watermark and safety inference"),z0.forEach(t),hr=h(e),F=r(e,"P",{});var Wi=s(F);Oh=o(Wi,"We wanted to give users the ability to remove unsafe examples, and watermarked examples. To do that we collected training and test sets. The training set was augmented with examples retrieved from the knn index, while the test set samples were selected to represent well the dataset distribution, but were all manually annotated. The inference is done using the "),ka=r(Wi,"A",{href:!0});var R0=s(ka);Wh=o(R0,"embedding-reader"),R0.forEach(t),Sh=o(Wi," module for NSFW and "),Aa=r(Wi,"A",{href:!0});var H0=s(Aa);Dh=o(H0,"LAION-5B-WatermarkDetection"),H0.forEach(t),Gh=o(Wi," for watermarks These tags were also integrated into the UI, allowing everyone to observe that the safety tags indeed filter out almost all the unsafe results, and giving confidence that training a generative model on this data will not result in unexpectedly unsafe images."),Wi.forEach(t),ur=h(e),Ba=r(e,"H3",{});var j0=s(Ba);qh=o(j0,"Watermarks"),j0.forEach(t),mr=h(e),xa=r(e,"IMG",{src:!0}),pr=h(e),Ta=r(e,"P",{});var V0=s(Ta);Uh=o(V0,"The training dataset is 90000 samples (45222 watermarks, 44778 clear)."),V0.forEach(t),cr=h(e),Pa=r(e,"P",{});var F0=s(Pa);zh=o(F0,"Watermarked images are a big problem when training generative models like DALL-E or GLIDE. To tackle this problem we trained a watermark detection model and used it to calculate confidence scores for every image in LAION-5B. Therefore we created a training dataset consisting of 90.000 images with 50% watermarked and 50% clean images. The majority of the watermarked images have been extracted from the LAION-400M KNN index through the use of several text prompts like \u201Cclip art watermark\u201D, \u201Ccat watermark\u201D or \u201Clandscape watermark\u201D."),F0.forEach(t),gr=h(e),Ca=r(e,"P",{});var J0=s(Ca);Rh=o(J0,"The images in the cleaned category were composed of images from the Open Images dataset and images that contained texts, but no watermarks, like PPT slides and memes, also retrieved from the kNN indices of LAION-400M. While we tried to curate a test set to evaluate the quality of our watermark detection model, we realized that it is almost impossible to draw a clear line between what actually is a watermark and what is not. For example pictures with small transparent texts at the bottom had been considered by some people as watermarked, by others not."),J0.forEach(t),wr=h(e),Ma=r(e,"P",{});var K0=s(Ma);Hh=o(K0,"In the end we decided to choose a model based on our consensual judgment. It seems to be \u201Cgood\u201D at spotting obvious watermarks like those used on popular stock image sites. The creation of high-quality, openly accessible watermark detection test sets with clear and plausible definitions of what should be considered a watermark and what not, remains a challenge for future projects. Nevertheless we are convinced that removing images with a high confidence score for containing a watermark based on our model will significantly reduce the percentage of images that would be considered as obvious watermarks."),K0.forEach(t),vr=h(e),Z=r(e,"P",{});var Vo=s(Z);jh=o(Vo,"The model is available at "),ke=r(Vo,"A",{href:!0,rel:!0});var $0=s(ke);Vh=o($0,"https://github.com/LAION-AI/watermark-detection"),$0.forEach(t),Fh=o(Vo," and "),Ae=r(Vo,"A",{href:!0,rel:!0});var Q0=s(Ae);Jh=o(Q0,"https://github.com/LAION-AI/LAION-5B-WatermarkDetection/releases/tag/1.0"),Q0.forEach(t),Vo.forEach(t),br=h(e),Na=r(e,"H3",{});var Y0=s(Na);Kh=o(Y0,"Safety"),Y0.forEach(t),Lr=h(e),Oa=r(e,"P",{});var X0=s(Oa);$h=o(X0,"On a balanced manually annotated safety test set with 3000 samples:"),X0.forEach(t),_r=h(e),fe=r(e,"UL",{});var cs=s(fe);Eo=r(cs,"LI",{});var Z0=s(Eo);Qh=o(Z0,"the accuracy of the B32 NSFW classifier is: 0.960"),Z0.forEach(t),Yh=h(cs),Io=r(cs,"LI",{});var e2=s(Io);Xh=o(e2,"the accuracy of the ViT L 14 NSFW classifier is: 0.961"),e2.forEach(t),cs.forEach(t),yr=h(e),_=r(e,"P",{});var C=s(_);Zh=o(C,"The model, as well as the training code, are available at "),Wa=r(C,"A",{href:!0});var t2=s(Wa);eu=o(t2,"CLIP-based-NSFW-Detector"),t2.forEach(t),tu=o(C,". The tags are available at "),Sa=r(C,"A",{href:!0});var a2=s(Sa);au=o(a2,"laion2B-en-safety"),a2.forEach(t),iu=h(C),Da=r(C,"A",{href:!0});var i2=s(Da);ou=o(i2,"laion2B-multi-safety"),i2.forEach(t),lu=h(C),Ga=r(C,"A",{href:!0});var o2=s(Ga);ru=o(o2,"laion1B-nolang-safety"),o2.forEach(t),su=o(C,". Demo at "),qa=r(C,"A",{href:!0});var l2=s(qa);nu=o(l2,"clip-retrieval"),l2.forEach(t),fu=o(C," (check/uncheck safe mode)"),C.forEach(t),Er=h(e),Ua=r(e,"H2",{});var r2=s(Ua);du=o(r2,"Using LAION datasets"),r2.forEach(t),Ir=h(e),za=r(e,"P",{});var s2=s(za);hu=o(s2,"Laion5B and LAION-400M could e.g. be used to train"),s2.forEach(t),kr=h(e),J=r(e,"UL",{});var Si=s(J);ko=r(Si,"LI",{});var n2=s(ko);uu=o(n2,"Generative models: training image/text generative models, e.g autoregressive models like DALL-E or diffusion models like GLIDE"),n2.forEach(t),mu=h(Si),Ao=r(Si,"LI",{});var f2=s(Ao);pu=o(f2,"Models with contrastive losses: self-supervised training on image/text pairs using contrastive losses, e.g CLIP"),f2.forEach(t),cu=h(Si),Bo=r(Si,"LI",{});var d2=s(Bo);gu=o(d2,"Classification models: e.g, performing zero-shot classification by extracting pseudo labels from queries on the dataset"),d2.forEach(t),Si.forEach(t),Ar=h(e),Ra=r(e,"P",{});var h2=s(Ra);wu=o(h2,"We present here a few examples of models that were trained on our LAION datasets with success:"),h2.forEach(t),Br=h(e),Ha=r(e,"H3",{});var u2=s(Ha);vu=o(u2,"CLIP"),u2.forEach(t),xr=h(e),ja=r(e,"P",{});var m2=s(ja);bu=o(m2,"We, LAION, are currently working together with the Cross Sectional Team Deep Learning (CST-DL), Scalable Learning and Multi-Purpose AI Lab (SLAMPAI) at the J\xFClich Supercomputing Centre (JSC) and the Open CLIP team in the replication of OpenAI\u2019s CLIP results."),m2.forEach(t),Tr=h(e),Va=r(e,"IMG",{src:!0}),Pr=h(e),Fa=r(e,"P",{});var p2=s(Fa);Lu=o(p2,"( The results in the right column are from our model. \u2013 huge thanks to Cade Gordon & Ross Wightman for performing the training run )"),p2.forEach(t),Cr=h(e),Be=r(e,"P",{});var rp=s(Be);_u=o(rp,"The repository with the training code and the model checkpoints can be found here: "),xe=r(rp,"A",{href:!0,rel:!0});var c2=s(xe);yu=o(c2,"https://github.com/mlfoundations/open_clip"),c2.forEach(t),rp.forEach(t),Mr=h(e),de=r(e,"P",{});var gs=s(de);Eu=o(gs,"We gratefully acknowledge the Gauss Centre for Supercomputing e.V. ("),Te=r(gs,"A",{href:!0,rel:!0});var g2=s(Te);Iu=o(g2,"www.gauss-centre.eu"),g2.forEach(t),ku=o(gs,") for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J\xFClich Supercomputing Centre (JSC)."),gs.forEach(t),Nr=h(e),Ja=r(e,"H3",{});var w2=s(Ja);Au=o(w2,"BLIP inference tuning"),w2.forEach(t),Or=h(e),ee=r(e,"P",{});var Fo=s(ee);Ka=r(Fo,"A",{href:!0});var v2=s(Ka);Bu=o(v2,"BLIP"),v2.forEach(t),xu=o(Fo," is a model that was trained for both image-text matching and image captioning. It was trained on a 115M subset of LAION-400M. To improve the results of the generated captions we (LAION) performed over 100 experiments to determine the hyperparameters that maximize the BLEU-4 score compared to MS COCO captions. Here you can see some of our "),$a=r(Fo,"A",{href:!0});var b2=s($a);Tu=o(b2,"results"),b2.forEach(t),Pu=o(Fo,"."),Fo.forEach(t),Wr=h(e),Qa=r(e,"IMG",{src:!0}),Sr=o(e,`
eval_best_auto0185: An orange cat is looking at its reflection in the mirror.
`),Ya=r(e,"IMG",{src:!0}),Dr=o(e,`
eval_best_auto0190: A green highway sign with the words Queens Bronx.
 
 
We found that we can significantly improve the quality of the captions by generating 40 (or more) candidate captions for each image and then ranking them using OpenAI\u2019s CLIP ViT-L/14 & CLIP-Resnet50x64. First we ranked all candidates with ViT-L/14 and then we ranked the top-5 results again using Resnet50x64. Preliminary results of human evaluations indicate that:
`),he=r(e,"OL",{});var ws=s(he);xo=r(ws,"LI",{});var L2=s(xo);Cu=o(L2,"Our evaluators gave the generated captions an average quality rating of 3,8 on a scale from 0 to 5, with a standard deviation of 0,9 ( in this particular hyperparameter configuration n= 600)"),L2.forEach(t),Mu=h(ws),To=r(ws,"LI",{});var _2=s(To);Nu=o(_2,"Our evaluators gave original human captions from MS COCO an average quality rating of 3,9 with a standard deviation of 0,8 ( n = 2100 )"),_2.forEach(t),ws.forEach(t),Gr=h(e),ue=r(e,"P",{});var vs=s(ue);Ou=o(vs,"\u2014> We hypothesize that the generated captions match (& sometimes even surpass) the average quality of the human captions of MS COCO (which are sometimes also far from perfect) in most cases, but sometimes ( in less than "),Po=r(vs,"CODE",{});var y2=s(Po);Wu=o(y2,"<10%"),y2.forEach(t),Su=o(vs," ) contain obvious mistakes, that humans would not make, because deeper kind of world knowledge & \u201Ecommon sense\u201C would be necessary in those cases."),vs.forEach(t),qr=h(e),Xa=r(e,"H3",{});var E2=s(Xa);Du=o(E2,"GLIDE"),E2.forEach(t),Ur=h(e),K=r(e,"P",{});var Di=s(K);Gu=o(Di,"Clay Mullis (alias "),Za=r(Di,"A",{href:!0});var I2=s(Za);qu=o(I2,"afiaka87"),I2.forEach(t),Uu=o(Di,") used subsets of LAON-2B to fine-tune the OpenAi "),ei=r(Di,"A",{href:!0});var k2=s(ei);zu=o(k2,"Glide"),k2.forEach(t),Ru=o(Di," model and managed to reintroduce human generations. Samples"),Di.forEach(t),zr=h(e),$=r(e,"P",{});var Gi=s($);Pe=r(Gi,"A",{href:!0,rel:!0});var A2=s(Pe);Hu=o(A2,"https://replicate.com/afiaka87/laionide-v3"),A2.forEach(t),ju=h(Gi),Ce=r(Gi,"A",{href:!0,rel:!0});var B2=s(Ce);Vu=o(B2,"https://wandb.ai/afiaka87/glide_compare/reports/Finetuning-GLIDE-on-Laion5B\u2013VmlldzoxNTg3MTkz"),B2.forEach(t),Fu=h(Gi),Me=r(Gi,"A",{href:!0,rel:!0});var x2=s(Me);Ju=o(x2,"https://wandb.ai/afiaka87/laionide-v3-glide/reports/Laionide-Version-3-Benchmark\u2013VmlldzoxNjE0MTE3"),x2.forEach(t),Gi.forEach(t),Rr=h(e),ti=r(e,"IMG",{src:!0}),Hr=h(e),ai=r(e,"H3",{});var T2=s(ai);Ku=o(T2,"Semantic search and subset extraction"),T2.forEach(t),jr=h(e),me=r(e,"P",{});var bs=s(me);$u=o(bs,"The "),ii=r(bs,"A",{href:!0});var P2=s(ii);Qu=o(P2,"clip-retrieval"),P2.forEach(t),Yu=o(bs," interface allows a user to search images and texts based on a query image or text using the CLIP embeddings of the input and our precomputed kNN indices. It demonstrates the diversity of images and captions that can be found in LAION-5B as well as high semantic relevance shows the distribution of image sizes of LAION-5B. Given the abundance of high-resolution images, one can produce subsets of images for training various customized models, and also choose image resolution that is suitable for the purpose of particular training."),bs.forEach(t),Vr=h(e),oi=r(e,"H3",{});var C2=s(oi);Xu=o(C2,"CLOOB"),C2.forEach(t),Fr=h(e),Ne=r(e,"P",{});var sp=s(Ne);Zu=o(sp,"Katherine Crowson and John David Pressman recently trained a CLOOB ViT-B/16, variant of CLIP, for 32 epochs on LAION-400M and got preliminary results, that come close to the performance of OpenAI\u2019s ViT-B/32, even though this was an early run with unoptimized hyperparameters. The checkpoints can be found here: "),Oe=r(sp,"A",{href:!0,rel:!0});var M2=s(Oe);em=o(M2,"https://github.com/crowsonkb/cloob-training"),M2.forEach(t),sp.forEach(t),Jr=h(e),li=r(e,"IMG",{src:!0}),Kr=o(e,`
(zero-shot accuracies on Imagenet-1K )
`),ri=r(e,"P",{});var N2=s(ri);tm=o(N2,"We are in touch with Andreas F\xFCrst, one of the original CLOOB authors, and learned from him that their team is currently (at the time of writing) training a CLOOB ViT-B/32 with LAION-400M with optimized hyperparameters and very promising results so far (53% zero-shot accuracy on Imagenet after 7 epochs)."),N2.forEach(t),$r=h(e),si=r(e,"H2",{});var O2=s(si);am=o(O2,"Papers citing LAION 400M"),O2.forEach(t),Qr=h(e),ni=r(e,"P",{});var W2=s(ni);im=o(W2,"After the release of LAION-400M, several papers used LAION-400M for image generation, text to image generation, image to text generation and text image matching:"),W2.forEach(t),Yr=h(e),I=r(e,"UL",{});var Y=s(I);fi=r(Y,"LI",{});var np=s(fi);di=r(np,"A",{href:!0});var S2=s(di);om=o(S2,"Vector Quantized Diffusion Model for Text-to-Image Synthesis"),S2.forEach(t),lm=o(np," used LAION-400M to train VQ diffusion text to image generation models"),np.forEach(t),rm=h(Y),hi=r(Y,"LI",{});var fp=s(hi);ui=r(fp,"A",{href:!0});var D2=s(ui);sm=o(D2,"High-Resolution Image Synthesis with Latent Diffusion Models"),D2.forEach(t),nm=o(fp," used a subset of LAION-400M to train latent diffusion models"),fp.forEach(t),fm=h(Y),mi=r(Y,"LI",{});var dp=s(mi);pi=r(dp,"A",{href:!0});var G2=s(pi);dm=o(G2,"General Facial Representation Learning in a Visual-Linguistic Manner"),G2.forEach(t),hm=o(dp," LAION-400M face subset to train a face clip"),dp.forEach(t),um=h(Y),ci=r(Y,"LI",{});var hp=s(ci);gi=r(hp,"A",{href:!0});var q2=s(gi);mm=o(q2,"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"),q2.forEach(t),pm=o(hp," image captioning using LAION-400M subset"),hp.forEach(t),cm=h(Y),wi=r(Y,"LI",{});var up=s(wi);vi=r(up,"A",{href:!0});var U2=s(vi);gm=o(U2,"MAGMA \u2013 Multimodal Augmentation of Generative Models through Adapter-based Finetuning"),U2.forEach(t),wm=o(up," was trained on image question answering using a LAION-400M subset"),up.forEach(t),Y.forEach(t),Xr=h(e),bi=r(e,"H2",{});var z2=s(bi);vm=o(z2,"Conclusion"),z2.forEach(t),Zr=h(e),Li=r(e,"P",{});var R2=s(Li);bm=o(R2,"By releasing an updated version of an openly available dataset that contains 5 billion image-text pairs, we have set new Standards for the scale of openly available datasets and enable researchers from all over the world to train state-of-the-art language-vision models like GLIDE or Turing Bletchley. As proof of concept, we demonstrated that a subset of our dataset can be used to train various CLIP-like models, producing samples of sufficient quality. This dataset extends the possibilities in multi-language large-scale training and research of language-vision models, that were previously restricted to those having access to proprietary large datasets, to the broad community."),R2.forEach(t),es=h(e),_i=r(e,"H2",{});var H2=s(_i);Lm=o(H2,"What\u2019s next?"),H2.forEach(t),ts=h(e),yi=r(e,"P",{});var j2=s(yi);_m=o(j2,"This is only the beginning! Now that this huge and open dataset is released, it can be used to train many models, such as gigantic clip models, image/text generation models and much more. We have so many projects going on that it\u2019s probably best, if you are interested, to join our Discord server and check out what\u2019s going on. We are and always will be a grassroots community that works openly and welcomes everyone who is kind and passionate and for machine learning."),j2.forEach(t),as=h(e),pe=r(e,"P",{});var Ls=s(pe);ym=o(Ls,"Join us in "),Ei=r(Ls,"A",{href:!0});var V2=s(Ei);Em=o(V2,"discord"),V2.forEach(t),Im=o(Ls," and help us to train models like CLIP, BLIP, GLIDE, Dall-E, SimMIM, AudioCLIP and don\u2019t hesitate to share your ideas for new projects with us."),Ls.forEach(t),is=h(e),Ii=r(e,"P",{});var F2=s(Ii);km=o(F2,"Become a part of our constantly growing crowd of supporters who help us to make machine learning dreams come true!"),F2.forEach(t),os=h(e),ki=r(e,"H2",{});var J2=s(ki);Am=o(J2,"Credit Assignment"),J2.forEach(t),ls=h(e),m=r(e,"UL",{});var p=s(m);rs=r(p,"LI",{}),s(rs).forEach(t),Bm=h(p),Co=r(p,"LI",{});var K2=s(Co);xm=o(K2,"Christoph Schuhmann: He led this project and built POCs for most of its components including clip filtering,the safety model, the watermark model and the Blip inference tuning project."),K2.forEach(t),Tm=h(p),Mo=r(p,"LI",{});var $2=s(Mo);Pm=o($2,"Richard Vencu: System architecture and download script optimizations, GPU assisted filtering. Set up the AWS infrastructure."),$2.forEach(t),Cm=h(p),No=r(p,"LI",{});var Q2=s(No);Mm=o(Q2,"Romain Beaumont: Guidance on scaling for the common crawl filtering pipeline. Built and ran the dataset preparation pipeline: pyspark deduplication job, img2dataset, clip inference, autofaiss, safety tags."),Q2.forEach(t),Nm=h(p),Oo=r(p,"LI",{});var Y2=s(Oo);Om=o(Y2,"Clayton Mullis: DALLE-pytorch training/analysis, glide training, WDS filtering"),Y2.forEach(t),Wm=h(p),Wo=r(p,"LI",{});var X2=s(Wo);Sm=o(X2,"Jenia Jitsev: scientific organization & writing, experiments planning and design, compute resource acquisition, general supervision"),X2.forEach(t),Dm=h(p),So=r(p,"LI",{});var Z2=s(So);Gm=o(Z2,"Robert Kaczmarczyk: Established WDS architecture, performed DALL-E training runs, balancing calculation, sample (NSFW, watermark, caption quality) annotation and manuscript revision"),Z2.forEach(t),qm=h(p),Do=r(p,"LI",{});var eg=s(Do);Um=o(eg,"Andreas K\xF6pf: He conducted the hyperparameter search for the inference strategies with the BLIP image-captioning model"),eg.forEach(t),zm=h(p),Go=r(p,"LI",{});var tg=s(Go);Rm=o(tg,"Theo Coomber: He was one of our first contributors & build the first versions of our worker swarm system. Without his enthusiasm this project might never have taken off."),tg.forEach(t),Hm=h(p),qo=r(p,"LI",{});var ag=s(qo);jm=o(ag,"Aarush Katta: Trained the watermark model"),ag.forEach(t),Vm=h(p),Uo=r(p,"LI",{});var ig=s(Uo);Fm=o(ig,"Cade Gordon: Run distributed inference for the watermark tags & trained the CLIP B/32 model on JUWELS Booster"),ig.forEach(t),Jm=h(p),zo=r(p,"LI",{});var og=s(zo);Km=o(og,"Ross Wightman: Ross helped Cade with the debugging & training of the CLIP-B/32 model and executed experiments on JUWELS Booster"),og.forEach(t),$m=h(p),Ro=r(p,"LI",{});var lg=s(Ro);Qm=o(lg,"Katherine Crowson and John David Pressman: Trained the CLOOB model"),lg.forEach(t),Ym=h(p),Ho=r(p,"LI",{});var rg=s(Ho);Xm=o(rg,"Aran Komatsuzaki: Led an image-text-pair dataset building project, which inspired this project."),rg.forEach(t),Zm=h(p),jo=r(p,"LI",{});var sg=s(jo);ep=o(sg,"Bokai Yu: Accomplished most of the work to make the knn index building tool autofaiss work in a distributed setting"),sg.forEach(t),p.forEach(t),this.h()},h(){S(A.src,te=xg)||u(A,"src",te),u(w,"class","w-32 h-32"),u(X,"href",""),u(Se,"href",""),u(De,"href",""),u(Ge,"href",""),u(qe,"href",""),u(Ue,"href",""),u(Ke,"href",""),u(Qe,"href",""),u(Xe,"href",""),u(Ze,"href",""),u(et,"href",""),u(at,"href",""),u(it,"href",""),u(ot,"href",""),u(lt,"href",""),u(st,"href",""),u(nt,"href",""),u(dt,"href",""),u(ht,"href",""),u(ut,"href",""),u(mt,"href",""),u(pt,"href",""),u(ct,"href",""),u(gt,"href",""),u(wt,"href",""),u(vt,"href",""),u(bt,"href",""),u(Lt,"href",""),u(yt,"href",""),u(kt,"href",""),u(Bt,"href",""),u(Tt,"href",""),u(Pt,"href",""),S(Nt.src,pp=Pg)||u(Nt,"src",pp),S(zt.src,cp=Cg)||u(zt,"src",cp),S(Jt.src,gp=Tg)||u(Jt,"src",gp),S(ea.src,wp=Eg)||u(ea,"src",wp),u(fa,"href",""),u(da,"href",""),u(ha,"href",""),u(ma,"href",""),u(pa,"href",""),u(ga,"href",""),u(wa,"href",""),u(ba,"href",""),u(La,"href",""),u(ya,"href",""),u(Ea,"href",""),u(ka,"href",""),u(Aa,"href",""),S(xa.src,vp=Ng)||u(xa,"src",vp),u(ke,"href","https://github.com/LAION-AI/watermark-detection"),u(ke,"rel","nofollow"),u(Ae,"href","https://github.com/LAION-AI/LAION-5B-WatermarkDetection/releases/tag/1.0"),u(Ae,"rel","nofollow"),u(Wa,"href",""),u(Sa,"href",""),u(Da,"href",""),u(Ga,"href",""),u(qa,"href",""),S(Va.src,bp=Ig)||u(Va,"src",bp),u(xe,"href","https://github.com/mlfoundations/open_clip"),u(xe,"rel","nofollow"),u(Te,"href","http://www.gauss-centre.eu"),u(Te,"rel","nofollow"),u(Ka,"href",""),u($a,"href",""),S(Qa.src,Lp=Ag)||u(Qa,"src",Lp),S(Ya.src,_p=Bg)||u(Ya,"src",_p),u(Za,"href",""),u(ei,"href",""),u(Pe,"href","https://replicate.com/afiaka87/laionide-v3"),u(Pe,"rel","nofollow"),u(Ce,"href","https://wandb.ai/afiaka87/glide_compare/reports/Finetuning-GLIDE-on-Laion5B%E2%80%93VmlldzoxNTg3MTkz"),u(Ce,"rel","nofollow"),u(Me,"href","https://wandb.ai/afiaka87/laionide-v3-glide/reports/Laionide-Version-3-Benchmark%E2%80%93VmlldzoxNjE0MTE3"),u(Me,"rel","nofollow"),S(ti.src,yp=Mg)||u(ti,"src",yp),u(ii,"href",""),u(Oe,"href","https://github.com/crowsonkb/cloob-training"),u(Oe,"rel","nofollow"),S(li.src,Ep=kg)||u(li,"src",Ep),u(di,"href",""),u(ui,"href",""),u(pi,"href",""),u(gi,"href",""),u(vi,"href",""),u(Ei,"href","")},m(e,n){f(e,w,n),a(w,A),f(e,ae,n),f(e,c,n),a(c,M),a(c,X),a(X,_s),a(c,ys),a(c,Se),a(Se,Es),a(c,Is),a(c,De),a(De,ks),a(c,As),f(e,Jo,n),f(e,B,n),a(B,Bs),a(B,Ge),a(Ge,xs),a(B,Ts),a(B,qe),a(qe,Ps),a(B,Cs),a(B,Ue),a(Ue,Ms),a(B,Ns),f(e,Ko,n),f(e,ze,n),a(ze,Os),f(e,$o,n),f(e,Re,n),a(Re,Ws),f(e,Qo,n),f(e,He,n),a(He,Ss),f(e,Yo,n),f(e,je,n),a(je,Ds),f(e,Xo,n),f(e,Ve,n),a(Ve,Gs),f(e,Zo,n),f(e,Fe,n),a(Fe,qs),f(e,el,n),f(e,D,n),a(D,Je),a(Je,Ke),a(Ke,Us),a(Je,zs),a(D,Rs),a(D,$e),a($e,Qe),a(Qe,Hs),a($e,js),a(D,Vs),a(D,Ye),a(Ye,Xe),a(Xe,Fs),a(Ye,Js),f(e,tl,n),f(e,ie,n),a(ie,Ks),a(ie,Ze),a(Ze,$s),a(ie,Qs),f(e,al,n),f(e,oe,n),a(oe,Ys),a(oe,et),a(et,Xs),a(oe,Zs),f(e,il,n),f(e,tt,n),a(tt,en),f(e,ol,n),f(e,L,n),a(L,N),a(N,tn),a(N,at),a(at,an),a(N,on),a(N,it),a(it,ln),a(N,rn),a(N,ot),a(ot,sn),a(N,nn),a(L,fn),a(L,Ee),a(Ee,dn),a(Ee,lt),a(lt,hn),a(Ee,un),a(L,mn),a(L,rt),a(rt,st),a(st,pn),a(rt,cn),a(L,gn),a(L,Ie),a(Ie,wn),a(Ie,nt),a(nt,vn),a(Ie,bn),a(L,Ln),a(L,ft),a(ft,_n),a(ft,dt),a(dt,yn),a(L,En),a(L,O),a(O,In),a(O,ht),a(ht,kn),a(O,An),a(O,ut),a(ut,Bn),a(O,xn),a(O,mt),a(mt,Tn),a(O,Pn),a(L,Cn),a(L,W),a(W,Mn),a(W,pt),a(pt,Nn),a(W,On),a(W,ct),a(ct,Wn),a(W,Sn),a(W,gt),a(gt,Dn),a(W,Gn),f(e,ll,n),f(e,le,n),a(le,qn),a(le,wt),a(wt,Un),a(le,zn),f(e,rl,n),f(e,x,n),a(x,Rn),a(x,vt),a(vt,Hn),a(x,jn),a(x,bt),a(bt,Vn),a(x,Fn),a(x,Lt),a(Lt,Jn),a(x,Kn),f(e,sl,n),f(e,_t,n),a(_t,$n),f(e,nl,n),f(e,re,n),a(re,Qn),a(re,yt),a(yt,Yn),a(re,Xn),f(e,fl,n),f(e,Et,n),a(Et,Zn),f(e,dl,n),f(e,It,n),a(It,ef),f(e,hl,n),f(e,b,n),a(b,qi),a(qi,tf),a(b,af),a(b,Ui),a(Ui,of),a(b,lf),a(b,zi),a(zi,rf),a(b,sf),a(b,Ri),a(Ri,nf),a(b,ff),a(b,Hi),a(Hi,df),a(b,hf),a(b,ji),a(ji,uf),a(b,mf),a(b,Vi),a(Vi,pf),a(b,cf),a(b,Fi),a(Fi,gf),f(e,ul,n),f(e,se,n),a(se,wf),a(se,kt),a(kt,vf),a(se,bf),f(e,ml,n),f(e,At,n),a(At,Lf),f(e,pl,n),f(e,ne,n),a(ne,_f),a(ne,Bt),a(Bt,yf),a(ne,Ef),f(e,cl,n),f(e,xt,n),a(xt,If),f(e,gl,n),f(e,G,n),a(G,kf),a(G,Tt),a(Tt,Af),a(G,Bf),a(G,Pt),a(Pt,xf),a(G,Tf),f(e,wl,n),f(e,Ct,n),a(Ct,Pf),f(e,vl,n),f(e,Mt,n),a(Mt,Cf),f(e,bl,n),f(e,Nt,n),f(e,Ll,n),f(e,Ot,n),a(Ot,Mf),f(e,_l,n),f(e,q,n),a(q,Ji),a(Ji,Nf),a(q,Of),a(q,Ki),a(Ki,Wf),a(q,Sf),a(q,$i),a($i,Df),f(e,yl,n),f(e,Wt,n),a(Wt,Gf),f(e,El,n),f(e,St,n),a(St,qf),f(e,Il,n),f(e,Dt,n),a(Dt,Uf),f(e,kl,n),f(e,Gt,n),a(Gt,zf),f(e,Al,n),f(e,qt,n),a(qt,Rf),f(e,Bl,n),f(e,Ut,n),a(Ut,Hf),f(e,xl,n),f(e,zt,n),f(e,Tl,n),f(e,Rt,n),a(Rt,jf),f(e,Pl,n),f(e,U,n),a(U,Qi),a(Qi,Vf),a(U,Ff),a(U,Yi),a(Yi,Jf),a(U,Kf),a(U,Xi),a(Xi,$f),f(e,Cl,n),f(e,Ht,n),a(Ht,Qf),f(e,Ml,n),f(e,g,n),a(g,Zi),a(Zi,Yf),a(g,Xf),a(g,eo),a(eo,Zf),a(g,ed),a(g,to),a(to,td),a(g,ad),a(g,ao),a(ao,id),a(g,od),a(g,io),a(io,ld),a(g,rd),a(g,oo),a(oo,sd),a(g,nd),a(g,lo),a(lo,fd),a(g,dd),a(g,ro),a(ro,hd),a(g,ud),a(g,so),a(so,md),a(g,pd),a(g,no),a(no,cd),a(g,gd),a(g,fo),a(fo,wd),f(e,Nl,n),f(e,jt,n),a(jt,vd),f(e,Ol,n),f(e,Vt,n),a(Vt,bd),f(e,Wl,n),f(e,Ft,n),a(Ft,Ld),f(e,Sl,n),f(e,Jt,n),f(e,Dl,n),f(e,Kt,n),a(Kt,_d),f(e,Gl,n),f(e,z,n),a(z,ho),a(ho,yd),a(z,Ed),a(z,uo),a(uo,Id),a(z,kd),a(z,mo),a(mo,Ad),f(e,ql,n),f(e,$t,n),a($t,Bd),f(e,Ul,n),f(e,Qt,n),a(Qt,xd),f(e,zl,n),f(e,Yt,n),a(Yt,Td),f(e,Rl,n),f(e,Xt,n),a(Xt,Pd),f(e,Hl,n),f(e,Zt,n),a(Zt,Cd),f(e,jl,n),f(e,ea,n),f(e,Vl,n),f(e,ta,n),a(ta,Md),f(e,Fl,n),f(e,aa,n),a(aa,Nd),f(e,Jl,n),f(e,ia,n),a(ia,Od),f(e,Kl,n),f(e,oa,n),a(oa,Wd),f(e,$l,n),f(e,la,n),a(la,Sd),f(e,Ql,n),f(e,ra,n),a(ra,Dd),f(e,Yl,n),f(e,sa,n),a(sa,Gd),f(e,Xl,n),f(e,E,n),a(E,po),a(po,qd),a(E,Ud),a(E,co),a(co,zd),a(E,Rd),a(E,go),a(go,Hd),a(E,jd),a(E,wo),a(wo,Vd),a(E,Fd),a(E,vo),a(vo,Jd),f(e,Zl,n),f(e,na,n),a(na,Kd),f(e,er,n),f(e,T,n),a(T,$d),a(T,fa),a(fa,Qd),a(T,Yd),a(T,da),a(da,Xd),a(T,Zd),a(T,ha),a(ha,eh),a(T,th),f(e,tr,n),f(e,P,n),a(P,bo),a(bo,ah),a(P,ih),a(P,Lo),a(Lo,oh),a(P,lh),a(P,_o),a(_o,rh),a(P,sh),a(P,yo),a(yo,nh),f(e,ar,n),f(e,ua,n),a(ua,fh),f(e,ir,n),f(e,R,n),a(R,dh),a(R,ma),a(ma,hh),a(R,uh),a(R,pa),a(pa,mh),a(R,ph),f(e,or,n),f(e,ca,n),a(ca,ch),f(e,lr,n),f(e,H,n),a(H,gh),a(H,ga),a(ga,wh),a(H,vh),a(H,wa),a(wa,bh),a(H,Lh),f(e,rr,n),f(e,va,n),a(va,_h),f(e,sr,n),f(e,j,n),a(j,yh),a(j,ba),a(ba,Eh),a(j,Ih),a(j,La),a(La,kh),a(j,Ah),f(e,nr,n),f(e,_a,n),a(_a,Bh),f(e,fr,n),f(e,V,n),a(V,xh),a(V,ya),a(ya,Th),a(V,Ph),a(V,Ea),a(Ea,Ch),a(V,Mh),f(e,dr,n),f(e,Ia,n),a(Ia,Nh),f(e,hr,n),f(e,F,n),a(F,Oh),a(F,ka),a(ka,Wh),a(F,Sh),a(F,Aa),a(Aa,Dh),a(F,Gh),f(e,ur,n),f(e,Ba,n),a(Ba,qh),f(e,mr,n),f(e,xa,n),f(e,pr,n),f(e,Ta,n),a(Ta,Uh),f(e,cr,n),f(e,Pa,n),a(Pa,zh),f(e,gr,n),f(e,Ca,n),a(Ca,Rh),f(e,wr,n),f(e,Ma,n),a(Ma,Hh),f(e,vr,n),f(e,Z,n),a(Z,jh),a(Z,ke),a(ke,Vh),a(Z,Fh),a(Z,Ae),a(Ae,Jh),f(e,br,n),f(e,Na,n),a(Na,Kh),f(e,Lr,n),f(e,Oa,n),a(Oa,$h),f(e,_r,n),f(e,fe,n),a(fe,Eo),a(Eo,Qh),a(fe,Yh),a(fe,Io),a(Io,Xh),f(e,yr,n),f(e,_,n),a(_,Zh),a(_,Wa),a(Wa,eu),a(_,tu),a(_,Sa),a(Sa,au),a(_,iu),a(_,Da),a(Da,ou),a(_,lu),a(_,Ga),a(Ga,ru),a(_,su),a(_,qa),a(qa,nu),a(_,fu),f(e,Er,n),f(e,Ua,n),a(Ua,du),f(e,Ir,n),f(e,za,n),a(za,hu),f(e,kr,n),f(e,J,n),a(J,ko),a(ko,uu),a(J,mu),a(J,Ao),a(Ao,pu),a(J,cu),a(J,Bo),a(Bo,gu),f(e,Ar,n),f(e,Ra,n),a(Ra,wu),f(e,Br,n),f(e,Ha,n),a(Ha,vu),f(e,xr,n),f(e,ja,n),a(ja,bu),f(e,Tr,n),f(e,Va,n),f(e,Pr,n),f(e,Fa,n),a(Fa,Lu),f(e,Cr,n),f(e,Be,n),a(Be,_u),a(Be,xe),a(xe,yu),f(e,Mr,n),f(e,de,n),a(de,Eu),a(de,Te),a(Te,Iu),a(de,ku),f(e,Nr,n),f(e,Ja,n),a(Ja,Au),f(e,Or,n),f(e,ee,n),a(ee,Ka),a(Ka,Bu),a(ee,xu),a(ee,$a),a($a,Tu),a(ee,Pu),f(e,Wr,n),f(e,Qa,n),f(e,Sr,n),f(e,Ya,n),f(e,Dr,n),f(e,he,n),a(he,xo),a(xo,Cu),a(he,Mu),a(he,To),a(To,Nu),f(e,Gr,n),f(e,ue,n),a(ue,Ou),a(ue,Po),a(Po,Wu),a(ue,Su),f(e,qr,n),f(e,Xa,n),a(Xa,Du),f(e,Ur,n),f(e,K,n),a(K,Gu),a(K,Za),a(Za,qu),a(K,Uu),a(K,ei),a(ei,zu),a(K,Ru),f(e,zr,n),f(e,$,n),a($,Pe),a(Pe,Hu),a($,ju),a($,Ce),a(Ce,Vu),a($,Fu),a($,Me),a(Me,Ju),f(e,Rr,n),f(e,ti,n),f(e,Hr,n),f(e,ai,n),a(ai,Ku),f(e,jr,n),f(e,me,n),a(me,$u),a(me,ii),a(ii,Qu),a(me,Yu),f(e,Vr,n),f(e,oi,n),a(oi,Xu),f(e,Fr,n),f(e,Ne,n),a(Ne,Zu),a(Ne,Oe),a(Oe,em),f(e,Jr,n),f(e,li,n),f(e,Kr,n),f(e,ri,n),a(ri,tm),f(e,$r,n),f(e,si,n),a(si,am),f(e,Qr,n),f(e,ni,n),a(ni,im),f(e,Yr,n),f(e,I,n),a(I,fi),a(fi,di),a(di,om),a(fi,lm),a(I,rm),a(I,hi),a(hi,ui),a(ui,sm),a(hi,nm),a(I,fm),a(I,mi),a(mi,pi),a(pi,dm),a(mi,hm),a(I,um),a(I,ci),a(ci,gi),a(gi,mm),a(ci,pm),a(I,cm),a(I,wi),a(wi,vi),a(vi,gm),a(wi,wm),f(e,Xr,n),f(e,bi,n),a(bi,vm),f(e,Zr,n),f(e,Li,n),a(Li,bm),f(e,es,n),f(e,_i,n),a(_i,Lm),f(e,ts,n),f(e,yi,n),a(yi,_m),f(e,as,n),f(e,pe,n),a(pe,ym),a(pe,Ei),a(Ei,Em),a(pe,Im),f(e,is,n),f(e,Ii,n),a(Ii,km),f(e,os,n),f(e,ki,n),a(ki,Am),f(e,ls,n),f(e,m,n),a(m,rs),a(m,Bm),a(m,Co),a(Co,xm),a(m,Tm),a(m,Mo),a(Mo,Pm),a(m,Cm),a(m,No),a(No,Mm),a(m,Nm),a(m,Oo),a(Oo,Om),a(m,Wm),a(m,Wo),a(Wo,Sm),a(m,Dm),a(m,So),a(So,Gm),a(m,qm),a(m,Do),a(Do,Um),a(m,zm),a(m,Go),a(Go,Rm),a(m,Hm),a(m,qo),a(qo,jm),a(m,Vm),a(m,Uo),a(Uo,Fm),a(m,Jm),a(m,zo),a(zo,Km),a(m,$m),a(m,Ro),a(Ro,Qm),a(m,Ym),a(m,Ho),a(Ho,Xm),a(m,Zm),a(m,jo),a(jo,ep)},p:_g,d(e){e&&t(w),e&&t(ae),e&&t(c),e&&t(Jo),e&&t(B),e&&t(Ko),e&&t(ze),e&&t($o),e&&t(Re),e&&t(Qo),e&&t(He),e&&t(Yo),e&&t(je),e&&t(Xo),e&&t(Ve),e&&t(Zo),e&&t(Fe),e&&t(el),e&&t(D),e&&t(tl),e&&t(ie),e&&t(al),e&&t(oe),e&&t(il),e&&t(tt),e&&t(ol),e&&t(L),e&&t(ll),e&&t(le),e&&t(rl),e&&t(x),e&&t(sl),e&&t(_t),e&&t(nl),e&&t(re),e&&t(fl),e&&t(Et),e&&t(dl),e&&t(It),e&&t(hl),e&&t(b),e&&t(ul),e&&t(se),e&&t(ml),e&&t(At),e&&t(pl),e&&t(ne),e&&t(cl),e&&t(xt),e&&t(gl),e&&t(G),e&&t(wl),e&&t(Ct),e&&t(vl),e&&t(Mt),e&&t(bl),e&&t(Nt),e&&t(Ll),e&&t(Ot),e&&t(_l),e&&t(q),e&&t(yl),e&&t(Wt),e&&t(El),e&&t(St),e&&t(Il),e&&t(Dt),e&&t(kl),e&&t(Gt),e&&t(Al),e&&t(qt),e&&t(Bl),e&&t(Ut),e&&t(xl),e&&t(zt),e&&t(Tl),e&&t(Rt),e&&t(Pl),e&&t(U),e&&t(Cl),e&&t(Ht),e&&t(Ml),e&&t(g),e&&t(Nl),e&&t(jt),e&&t(Ol),e&&t(Vt),e&&t(Wl),e&&t(Ft),e&&t(Sl),e&&t(Jt),e&&t(Dl),e&&t(Kt),e&&t(Gl),e&&t(z),e&&t(ql),e&&t($t),e&&t(Ul),e&&t(Qt),e&&t(zl),e&&t(Yt),e&&t(Rl),e&&t(Xt),e&&t(Hl),e&&t(Zt),e&&t(jl),e&&t(ea),e&&t(Vl),e&&t(ta),e&&t(Fl),e&&t(aa),e&&t(Jl),e&&t(ia),e&&t(Kl),e&&t(oa),e&&t($l),e&&t(la),e&&t(Ql),e&&t(ra),e&&t(Yl),e&&t(sa),e&&t(Xl),e&&t(E),e&&t(Zl),e&&t(na),e&&t(er),e&&t(T),e&&t(tr),e&&t(P),e&&t(ar),e&&t(ua),e&&t(ir),e&&t(R),e&&t(or),e&&t(ca),e&&t(lr),e&&t(H),e&&t(rr),e&&t(va),e&&t(sr),e&&t(j),e&&t(nr),e&&t(_a),e&&t(fr),e&&t(V),e&&t(dr),e&&t(Ia),e&&t(hr),e&&t(F),e&&t(ur),e&&t(Ba),e&&t(mr),e&&t(xa),e&&t(pr),e&&t(Ta),e&&t(cr),e&&t(Pa),e&&t(gr),e&&t(Ca),e&&t(wr),e&&t(Ma),e&&t(vr),e&&t(Z),e&&t(br),e&&t(Na),e&&t(Lr),e&&t(Oa),e&&t(_r),e&&t(fe),e&&t(yr),e&&t(_),e&&t(Er),e&&t(Ua),e&&t(Ir),e&&t(za),e&&t(kr),e&&t(J),e&&t(Ar),e&&t(Ra),e&&t(Br),e&&t(Ha),e&&t(xr),e&&t(ja),e&&t(Tr),e&&t(Va),e&&t(Pr),e&&t(Fa),e&&t(Cr),e&&t(Be),e&&t(Mr),e&&t(de),e&&t(Nr),e&&t(Ja),e&&t(Or),e&&t(ee),e&&t(Wr),e&&t(Qa),e&&t(Sr),e&&t(Ya),e&&t(Dr),e&&t(he),e&&t(Gr),e&&t(ue),e&&t(qr),e&&t(Xa),e&&t(Ur),e&&t(K),e&&t(zr),e&&t($),e&&t(Rr),e&&t(ti),e&&t(Hr),e&&t(ai),e&&t(jr),e&&t(me),e&&t(Vr),e&&t(oi),e&&t(Fr),e&&t(Ne),e&&t(Jr),e&&t(li),e&&t(Kr),e&&t(ri),e&&t($r),e&&t(si),e&&t(Qr),e&&t(ni),e&&t(Yr),e&&t(I),e&&t(Xr),e&&t(bi),e&&t(Zr),e&&t(Li),e&&t(es),e&&t(_i),e&&t(ts),e&&t(yi),e&&t(as),e&&t(pe),e&&t(is),e&&t(Ii),e&&t(os),e&&t(ki),e&&t(ls),e&&t(m)}}}function Wg(We){let w,A;const te=[We[0],dg];let ae={$$slots:{default:[Og]},$$scope:{ctx:We}};for(let c=0;c<te.length;c+=1)ae=mp(ae,te[c]);return w=new yg({props:ae}),{c(){pg(w.$$.fragment)},l(c){cg(w.$$.fragment,c)},m(c,M){gg(w,c,M),A=!0},p(c,[M]){const X=M&1?wg(te,[M&1&&ng(c[0]),M&0&&ng(dg)]):{};M&2&&(X.$$scope={dirty:M,ctx:c}),w.$set(X)},i(c){A||(vg(w.$$.fragment,c),A=!0)},o(c){bg(w.$$.fragment,c),A=!1},d(c){Lg(w,c)}}}const dg={title:"LAION-5B: A new era of open large-scale multi-modal datasets",author:"Romain Beaumont",date:"2022-03-31T00:00:00.000Z",tagline:"We present a dataset of 5,85 billion CLIP-filtered image-text pairs, 14x bigger than LAION-400M, previously the biggest openly accessible image-text dataset in the world.",authors:"Authors: Christoph Schuhmann, Richard Vencu, Romain Beaumont, Theo Coombes, Cade Gordon, Aarush Katta, Robert Kaczmarczyk, Jenia Jitsev"};function Sg(We,w,A){return We.$$set=te=>{A(0,w=mp(mp({},w),fg(te)))},w=fg(w),[w]}class qg extends hg{constructor(w){super(),ug(this,w,Sg,Wg,mg,{})}}export{qg as default,dg as metadata};

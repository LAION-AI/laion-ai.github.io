import{S as Sc,i as Uc,s as Dc,C as mu,w as Mc,x as Oc,y as Bc,z as Gc,A as Lc,q as Fc,o as qc,B as Hc,L as Ic,e as l,t as o,k as d,c as s,a as r,h as i,d as a,m as f,b as u,M as kc,g as h,G as t,E as Rc}from"../../../chunks/index-3013e6a1.js";import{M as zc}from"../../../chunks/MarkdownPost-0cffa593.js";var jc="/_app/assets/400m-architecture-5230a733.svg",Yc="/_app/assets/blue-cat-56020978.webp";function Kc(At){let p,N,W,x,c,b,y,eo,Is,ks,to,As,Ts,mi,X,Ps,Ee,Cs,Ns,pi,J,Ws,_e,xs,Ss,ci,Tt,Us,wi,Pt,Ds,vi,S,Ms,ao,Os,Bs,Le,Gs,Fs,gi,Ct,qs,bi,Nt,Hs,yi,U,oo,Rs,zs,io,js,Ys,lo,Ks,Ei,Z,Vs,so,$s,Xs,_i,Ie,Tc='<code class="language-undefined">aria2c &quot;https://the-eye.eu/public/AI/cah/laion400m-met-release.torrent&quot;</code>',Li,D,Js,ro,Zs,Qs,no,er,tr,Ii,Q,ar,ke,or,ir,ki,ee,lr,Ae,sr,rr,Ai,te,nr,Te,hr,dr,Ti,M,fr,Pe,ur,mr,Ce,pr,cr,Pi,Wt,wr,Ci,xt,vr,Ni,Ne,Pc=`<code class="language-undefined">    number of unique samples 413M
    Number with height or width &gt;= 1024 26M
    Number with height and width &gt;= 1024 9.6M
    Number with height or width &gt;= 512 112M
    Number with height and width &gt;= 512 67M
    Number with height or width &gt;= 256 268M
    Number with height and width &gt;= 256 211M</code>`,Wi,St,gr,xi,Ut,br,Si,O,yr,ho,Er,_r,fo,Lr,Ir,Ui,Dt,kr,Di,Mt,Ar,Mi,Ot,Tr,Oi,ae,Pr,We,Cr,Nr,Bi,Bt,Wr,Gi,Gt,xr,Fi,E,uo,Sr,Ur,mo,Dr,Mr,po,Or,Br,co,Gr,qi,Ft,Fr,Hi,qt,qr,Ri,xe,Cc='<code class="language-undefined">SAMPLE_ID | URL | TEXT  | LICENSE | NSFW | similarity | WIDTH | HEIGHT</code>',zi,Ht,Hr,ji,v,Rt,wo,Rr,zr,jr,Y,vo,Yr,Kr,go,Vr,$r,bo,Xr,Jr,_,yo,Zr,Qr,Eo,en,tn,_o,an,on,Lo,ln,sn,rn,zt,Io,nn,hn,dn,oe,ko,fn,un,Ao,mn,pn,Yi,ie,cn,Se,wn,vn,Ki,jt,gn,Vi,Yt,bn,$i,le,Ue,To,yn,En,K,Po,Co,_n,Ln,No,Wo,In,kn,Kt,xo,An,Tn,Pn,Vt,So,Cn,Nn,Xi,$t,Wn,Ji,Xt,xn,Zi,Jt,Sn,Qi,Zt,Un,el,B,Dn,De,Mn,On,Me,Bn,Gn,tl,Qt,Fn,al,ea,qn,ol,ta,G,Hn,Oe,Rn,zn,Be,jn,Yn,Ge,Fe,Kn,qe,Vn,$n,Xn,Uo,Jn,il,L,Zn,He,Qn,eh,Re,th,ah,ze,oh,ih,ll,aa,lh,sl,se,sh,je,rh,nh,rl,oa,hh,nl,ia,dh,hl,g,Do,fh,uh,Mo,mh,ph,Oo,ch,wh,Bo,vh,gh,Go,bh,dl,la,yh,fl,sa,Eh,ul,ra,_h,ml,re,Fo,Lh,Ih,qo,kh,pl,na,Ah,cl,F,Th,Ye,Ph,Ch,Ke,Nh,Wh,wl,ha,xh,vl,q,Sh,Ve,Uh,Dh,$e,Mh,Oh,gl,da,Bh,bl,fa,Gh,yl,m,Ho,Fh,qh,Ro,Hh,Rh,zo,zh,jh,jo,Yh,Kh,Yo,Vh,$h,Ko,Xh,Jh,Vo,Zh,Qh,$o,ed,td,Xo,ad,od,Jo,id,El,ua,ld,_l,ma,sd,Ll,pa,rd,Il,Xe,pu,kl,ca,nd,Al,wa,hd,Tl,va,dd,Pl,ga,fd,Cl,ba,ud,Nl,ya,md,Wl,Ea,pd,xl,_a,cd,Sl,ne,Zo,wd,vd,Qo,gd,Ul,La,bd,Dl,Ia,yd,Ml,I,Je,Ed,Ze,_d,Ld,Id,Qe,kd,et,Ad,Td,Pd,ka,tt,Cd,Nd,Wd,ei,xd,Ol,Aa,Sd,Bl,Ta,Ud,Gl,k,Dd,at,Md,Od,ot,Bd,Gd,it,Fd,qd,Fl,Pa,Hd,ql,A,Rd,lt,zd,jd,st,Yd,Kd,rt,Vd,$d,Hl,Ca,Xd,Rl,H,Jd,nt,Zd,Qd,ht,ef,tf,zl,he,dt,ti,af,of,V,ai,oi,lf,sf,ii,li,rf,nf,Na,si,hf,df,ff,Wa,ri,uf,mf,jl,de,pf,ft,cf,wf,Yl,xa,vf,Kl,T,gf,ut,bf,yf,mt,Ef,_f,pt,Lf,If,Vl,ct,cu,$l,Sa,kf,Xl,fe,Af,wt,Tf,Pf,Jl,Ua,Cf,Zl,Da,Nf,Ql,Ma,Wf,es,R,C,xf,vt,Sf,Uf,gt,Df,Mf,bt,Of,Bf,Gf,ni,Ff,qf,hi,Hf,ts,Oa,Rf,as,P,ue,zf,yt,jf,Yf,Et,Kf,Vf,Ba,$f,_t,Xf,Jf,Ga,Zf,Lt,Qf,eu,Fa,tu,It,au;return{c(){p=l("h2"),N=o("Concept and Content"),W=d(),x=l("p"),c=o("The LAION-400M dataset is entirely openly, freely accessible."),b=d(),y=l("p"),eo=l("strong"),Is=o("WARNING"),ks=o(": Be aware that the this large-scale dataset is non-curated. It was built for research purposes to enable testing model training on larger scale for broad researcher and other interested communities, and is "),to=l("strong"),As=o("not"),Ts=o(" meant for any real-world production or application."),mi=d(),X=l("p"),Ps=o("We have filtered all images and texts in the LAION-400M dataset with OpenAI\u2019s "),Ee=l("a"),Cs=o("CLIP"),Ns=o(" by calculating the cosine similarity between the text and image embeddings and dropping those with a similarity below 0.3. The threshold of 0.3 had been determined through human evaluations and seemed to be a good heuristic for estimating semantic image-text-content matching."),pi=d(),J=l("p"),Ws=o("The image-text pairs have been extracted from the "),_e=l("a"),xs=o("Common Crawl"),Ss=o(" web data dump and are from random web pages crawled between 2014 and 2021."),ci=d(),Tt=l("h3"),Us=o("Download Information"),wi=d(),Pt=l("h4"),Ds=o("Update 16 dec 2021"),vi=d(),S=l("p"),Ms=o("While "),ao=l("strong"),Os=o("the eye"),Bs=o(" experiences technical difficulties, we provide an alternate download server for this dataset at this link: "),Le=l("a"),Gs=o("LAION400M at deploy.laion.ai"),Fs=o("."),gi=d(),Ct=l("h4"),qs=o("Original Information"),bi=d(),Nt=l("p"),Hs=o("You can find:"),yi=d(),U=l("ul"),oo=l("li"),Rs=o("The CLIP image embeddings (NumPy files)"),zs=d(),io=l("li"),js=o("The parquet files"),Ys=d(),lo=l("li"),Ks=o("KNN index of image embeddings"),Ei=d(),Z=l("p"),Vs=o("To download from "),so=l("strong"),$s=o("the eye"),Xs=o(", run this command:"),_i=d(),Ie=l("pre"),Li=d(),D=l("p"),Js=o("You may want to use the "),ro=l("code"),Zs=o("--show-files"),Qs=o(" and "),no=l("code"),er=o("--select-file"),tr=o(" options to download only some files."),Ii=d(),Q=l("p"),ar=o("You can also find the files in "),ke=l("a"),or=o("LAION400M-met-release"),ir=o("."),ki=d(),ee=l("p"),lr=o("Some more significant KNN indices are present in "),Ae=l("a"),sr=o("LAION400M-indexes"),rr=o(". We advise using the 128GB ones."),Ai=d(),te=l("p"),nr=o("The parquet files in Kaggle: "),Te=l("a"),hr=o("LAION400M on Kaggle"),dr=o("."),Ti=d(),M=l("p"),fr=o("After downloading the metadata as indicated above, you can run "),Pe=l("a"),ur=o("this command"),mr=o(" to download the images and generate the webdataset fields (command using "),Ce=l("a"),pr=o("img2dataset"),cr=o(")."),Pi=d(),Wt=l("h3"),wr=o("LAION-400M Dataset Statistics"),Ci=d(),xt=l("p"),vr=o("The LAION-400M and future even bigger ones are, in fact, datasets of datasets. For instance, we can filter it out by image sizes into smaller datasets like this:"),Ni=d(),Ne=l("pre"),Wi=d(),St=l("p"),gr=o("By using the KNN index, we can extract specialized datasets by domains of interest. They are (or will be) sufficient to train technical domain models."),xi=d(),Ut=l("h3"),br=o("Disclaimer & Content Warning"),Si=d(),O=l("p"),yr=o("Our filtering protocols only removed NSFW images deteced as illegal, but the dataset stil lhas NSFW content accordingly marked in the metadata. When freely navigating through the dataset, keep in mind that it is a large-scale, "),ho=l("strong"),Er=o("non-curated"),_r=o(" set crawled from the internet for research purposes, such that collected links may lead to discomforting and disturbing content. Therefore, please use the demo liks with "),fo=l("strong"),Lr=o("caution"),Ir=o(". You can extract a \u201Csafe\u201D subset by filtering out samples drawn with NSFW or via stricter CLIP filtering."),Ui=d(),Dt=l("p"),kr=o("There is a certain degree of duplication because we used URL+text as deduplication criteria. The same image with the same caption may sit at different URLs, causing duplicates. THe same image with other captions is not, however, considered duplicated."),Di=d(),Mt=l("p"),Ar=o("Using KNN clustering should make it easy to further deduplicate by image content."),Mi=d(),Ot=l("h3"),Tr=o("Random Non-NSFW samples from the LAION-400M dataset."),Oi=d(),ae=l("p"),Pr=o("Also, use "),We=l("a"),Cr=o("the retrieval UI"),Nr=o(" for simple visualization of the dataset. There you can search among the dataset using CLIP and a knn index."),Bi=d(),Bt=l("h3"),Wr=o("LAION-400M Open Dataset structure"),Gi=d(),Gt=l("p"),xr=o("We produced the dataset in several formats to address the various use cases:"),Fi=d(),E=l("ul"),uo=l("li"),Sr=o("A 50GB url+caption metadata dataset in parquet files. We can use the metadata to compute statistics and redownload part of the dataset."),Ur=d(),mo=l("li"),Dr=o("A 10TB webdataset with 256x256 images, captions and metadata. It is a full version of teh dataset that can be used directly for training (this one is for internal use, you need to redownload images yourself due to licensing issues)"),Mr=d(),po=l("li"),Or=o("A 1TB set of the 400M text and image clip embeddings, useful to rebuild new knn indices"),Br=d(),co=l("li"),Gr=o("Pairs of 16G, 64G, and 128G knn indices (running in the web demo)"),qi=d(),Ft=l("h4"),Fr=o("URL and caption metadata dataset"),Hi=d(),qt=l("p"),qr=o("We provide 32 parquet files of size around 1GB (total 50GB) with the image URLs, the associated texts and additional metadata in the following format:"),Ri=d(),xe=l("pre"),zi=d(),Ht=l("p"),Hr=o("where:"),ji=d(),v=l("ul"),Rt=l("li"),wo=l("code"),Rr=o("SAMPLE_ID"),zr=o(": A unique identifier"),jr=d(),Y=l("li"),vo=l("code"),Yr=o("LICENSE"),Kr=o(": Where we found a reative Commons License in the image data, we named it here like, e.g. "),go=l("code"),Vr=o('"creativecommons.org/licenses/by-nc-sa/3.0"'),$r=o(" - otherwise you\u2019ll find it here a "),bo=l("code"),Xr=o('"?"'),Jr=d(),_=l("li"),yo=l("code"),Zr=o("NSFW"),Qr=o(": We used CLIP to estimate if the image has NSFW content. The esitmation has been pretty conservative, reducing false negatives at the cost of more false positives. Possible values are "),Eo=l("code"),en=o('"UNLIKELY"'),tn=o(", "),_o=l("code"),an=o('"UNSURE"'),on=o(", and "),Lo=l("code"),ln=o('"NSFW"'),sn=o("."),rn=d(),zt=l("li"),Io=l("code"),nn=o("similarity"),hn=o(": Value of the cosine similarity between the text and image embedding."),dn=d(),oe=l("li"),ko=l("code"),fn=o("WIDTH"),un=o(" and "),Ao=l("code"),mn=o("HEIGHT"),pn=o(": Image size as the image was embedded. We downsized originals that were larger than 4K to 4K."),Yi=d(),ie=l("p"),cn=o("The metadata dataset purpose is to download the images for the whole dataset or a subset of it by supplying it to the very efficient "),Se=l("a"),wn=o("img2dataset"),vn=o(" tool."),Ki=d(),jt=l("h4"),gn=o("10TB webdataset with images and captions"),Vi=d(),Yt=l("p"),bn=o("By running the img2dataset tool, we can download a 10TB webdataset. It will resize all images at 256x256 resolution, will append the corresponding caption and will generate a collection of tar files (the dataset format is called webdataset) containing images, captions, and metadata and related parquet files containing the same metadata."),$i=d(),le=l("ul"),Ue=l("li"),To=l("code"),yn=o("00000.tar"),En=o(" of size 270MB containing at most 10k samples"),K=l("ul"),Po=l("li"),Co=l("code"),_n=o("0.jpg"),Ln=d(),No=l("li"),Wo=l("code"),In=o("0.txt"),kn=d(),Kt=l("li"),xo=l("code"),An=o("0.json"),Tn=o(" containing metadata such as the URL, original width, EXIF data, whether the image is NSFW."),Pn=d(),Vt=l("li"),So=l("code"),Cn=o("00000.parquet"),Nn=o(" of size 1.6MB containing the same metadata as the JSON file. USeful to compute statistics without reading all the tar files."),Xi=d(),$t=l("p"),Wn=o("The 400M dataset will therefore have 41455 tar files and 41455 parquet files. This dataset purpose is to train multimodal models like CLIP or DALL-E."),Ji=d(),Xt=l("h4"),xn=o("1TB of CLIP embeddings"),Zi=d(),Jt=l("p"),Sn=o("The CLIP imbeddings are stored in NPY files next to parquet files in the same order. Since this dataset is much smaller than the image one, each NPY file stores 1M samples. Each NPY file is 1GB, and each parquet file is 150MB. There are a total of 400 such files. THe embeddings purpose is to compute statistics on the dataset, for example, using clustering or KNN indices."),Qi=d(),Zt=l("h4"),Un=o("Two small 6GB knn indices"),el=d(),B=l("p"),Dn=o("We provide two 6GB indices built using the "),De=l("a"),Mn=o("autofaiss"),On=o(". WE can us them to compute a subset of the dataset and, more generally, to search among it efficiently. See the wearch "),Me=l("a"),Bn=o("web demo"),Gn=o(" of it. We can use the CLIP filter tool along with this index to produce subsets using search tearms efficiently. We also provide two 16GB indices of higher quality."),tl=d(),Qt=l("h3"),Fn=o("What can we do with the LAION-400M dataset?"),al=d(),ea=l("p"),qn=o("Vision and language modeling has been taking off in 2021. Here are some pointers about what this kind of image + text datasets unlocks and why it sesms interesting:"),ol=d(),ta=l("ul"),G=l("li"),Hn=o("Six months ago, OPENAI released two blog posts and papers, "),Oe=l("a"),Rn=o("CLIP"),zn=o(" and "),Be=l("a"),jn=o("DALL-E"),Yn=o(". Both models rely on a large amount of (text, image) pairs. They used an unreleased 400M pairs dataset."),Ge=l("ul"),Fe=l("li"),Kn=o("CLIP is a model that computes how related are a text and an image. IT makes it possible to build large text to image search, and makes it possible to create that kind of crazy text to image art "),qe=l("a"),Vn=o("clip-art"),$n=o(". They released a smalla nd medium version of the model but no training code."),Xn=d(),Uo=l("li"),Jn=o("DALL-E is a model that directly generates images from texts. As can be seen from the blog post, it achieves awe-inspiring results that could directly impact the world for anything that needs drawing and illustrations. OpenAI did not release any model, even through an API."),il=d(),L=l("p"),Zn=o("Since then, various researchers have organized several efforts to replicate DALL-E. People gathered initially around this excellent DALL-E replication repository "),He=l("a"),Qn=o("DALLE-Pytorch"),eh=o(" with some fantastic results visible in the readme. More refcently, as part of huggingrace events, new developments have been achieved (see "),Re=l("a"),th=o("DALL-E-mini report"),ah=o("), and an online demo is now available at "),ze=l("a"),oh=o("DALL-E-mini demo"),ih=o("."),ll=d(),aa=l("p"),lh=o("The replication effort is still far from achieving the same performance as the original DALL-E, and it seems possible to go even further. Some people also want to make a better CLIP to produce even better-generated art."),sl=d(),se=l("p"),sh=o("A large part of the results that we can achieve with such models is thanks to a large amount of data. Before LAION-400M, the largest open dataset for (image, text) pairs are in the order or 10M (see "),je=l("a"),rh=o("DALL-E datasets"),nh=o("), which is enough to train exciting models but not enough to reach the best performance. Having a public dataset with hundreds of millions of pairs will help build these image+text models."),rl=d(),oa=l("h3"),hh=o("Analysis of the LAION-400M data"),nl=d(),ia=l("p"),dh=o("We annotated 3456 samples of the dataset and got the following results:"),hl=d(),g=l("ul"),Do=l("li"),fh=o("Correct positive NSFW: 4"),uh=d(),Mo=l("li"),mh=o("Correct negative NSFW: 3371"),ph=d(),Oo=l("li"),ch=o("False-positive NSFW: 73"),wh=d(),Bo=l("li"),vh=o("False-negative NSFW: 8"),gh=d(),Go=l("li"),bh=o("Bad captions: 3 (0.09%)"),dl=d(),la=l("p"),yh=o("The matching is excellent, thanks to CLIP. We could improve the NSFW automatic tagging in the future; however, the NSFW total rate is low enough (less than 1%) to make this not an issue."),fl=d(),sa=l("h2"),Eh=o("Technical Details"),ul=d(),ra=l("p"),_h=o("The dataset acquisition has two significant parts:"),ml=d(),re=l("ol"),Fo=l("li"),Lh=o("A distributed preprocessin gof the vast (many PBs) Common Crawl datasets, which produces a collection of matching URL and caption."),Ih=d(),qo=l("li"),kh=o("A single node much lighter post-processing of the data that anyone can run in a few days and which produces the final dataset."),pl=d(),na=l("h3"),Ah=o("1. Distributed processing of Common Crawl"),cl=d(),F=l("p"),Th=o("We acquire the raw web data for the creation of our dataset from Common Crawl. Common Crawl is non-profit organization dedicated to providing a copy of the internet to internet researchers, companies, and individuals at no cost for research and analysis. They regularly release dumps of HTML-like data parsed from billions of public websites found "),Ye=l("a"),Ph=o("on the Common Crawl website"),Ch=o(". To create image-text pairs, we parse through the data from Common Crawl and parse out all HTML IMG tags containing an "),Ke=l("a"),Nh=o("alt text attribute"),Wh=o(". Common Crawl provides its data in several formats. For our purpose, we chose to use the data in WAT format. The WAT files contain only the metadata of the crawled sites, which includes all links and IMG tags contained in the website. Parsing only this metadata is much faster than parsing the whole HTML text (provided in the WARC format)."),wl=d(),ha=l("h4"),xh=o("Downloading original images"),vl=d(),q=l("p"),Sh=o("We download the raw images from the URLs we parsed from the Common Crawl with asynchronous requests using the libraries "),Ve=l("a"),Uh=o("Trio"),Dh=o(" and "),$e=l("a"),Mh=o("Asks"),Oh=o(". They allow us to go multithreading for a single CPU. Usually, a home internet link will be exhausted by a single or two CPUs. A data centre node can scale up benefits from guaranteed internet speed with a multiprocessing pool much faster than a single CPU node. At this time, we were able to use 50 cores with a full, secured 1Gbps connection to the public internet. This bandwidth must be avaialable to the downloading node, not shared among many nodes or apps. We have optimised the sript for speed while mitigating various errors we encountered. Usually, to satisfy a high-end demanding node such as above, we must take additional steps to provide DNS caching capabilities. We found that the knot-resolver ran with two processes and configured with caching option can solve this problem."),gl=d(),da=l("h4"),Bh=o("Filtering out unusable image-text pairs"),bl=d(),fa=l("p"),Gh=o("After downloading the WAT fiels from Common Crawl, we filter the samples in the following steps:"),yl=d(),m=l("ol"),Ho=l("li"),Fh=o("We dropped all samples with less than five character alt text length"),qh=d(),Ro=l("li"),Hh=o("We dropped all samples with less than 5 KB image size"),Rh=d(),zo=l("li"),zh=o("We use continuously updated bloom filters to drop samples that are already in our dataset. The bloom filters deduplicate by concatenating the URL and the alt text."),jh=d(),jo=l("li"),Yh=o("We use continuously updated bloom filters to drop samples from URLs that had timed out previously and therefore seem unreachable (or at least not reachable in an efficient way)"),Kh=d(),Yo=l("li"),Vh=o("We use OpenAI\u2019s CLIP model (the \u2018ViT-B-32\u2018 version) to compute the image and alt text embeddings. Then we calculate the cosine similarity of both embedding vectors and drop all samples with a similarity below 0.3. We chose this threshold after trying different values and using human evaluations of how well the texts fit the images. Lower values like 0.28 or 0.29 also seemed okay in many cases, but after further inspections, we decided to choose the conservative value of 0.3."),$h=d(),Ko=l("li"),Xh=o("We use the CLIP embeddings of the images to estimate if their contents contain NSFW content. We do this by calculating CLIP embeddings for a list of image categories like, e.g. \u201Cselfie\u201D, \u201Cillustration\u201D, or \u201Clandscape\u201D, which also contains categories that indicate NSFW content like \u201Cporn\u201D and \u201Csex\u201D."),Jh=d(),Vo=l("li"),Zh=o("Then we compute the cosine similarities between the embedding image we are currently filtering and each of these category keywords. If the category with the highest similarity and the keyword with the second-highest similarity belong both to NSFW keywords, we tag the sample as \u201CNSFW\u201D. If only one of them belongs to an NSFW keyword, we categorise the sample as \u201CUNSURE\u201D. If both keywords with the highest similarities are not NSFW, we tag the sample as \u201CUNLIKELY\u201D."),Qh=d(),$o=l("li"),ed=o("In the next step, we look at all samples with either the \u201CNSFW\u201D or \u201CUNSURE\u201D tag and drop those with any keywords in their text related to kids, teens, or other semantically related content."),td=d(),Xo=l("li"),ad=o("In step 8, we repeat the procedure of computing the cosine similarities from step 6 with the difference that we now use category texts that indicate contents semantically related to kids and teens on a CLIP embedding level. If either the highest similarity or the second-highest similarity between a sample\u2019s image embedding and a text of the precomputed categories belongs to a text that indicates content related to under-aged persons, we drop this sample."),od=d(),Jo=l("li"),id=o("Finally, we repeat the procedure from step 8 with texts semantically related to animal categories like e.g. \u201Canimal\u201D, \u201Cbird\u201D, etc."),El=d(),ua=l("p"),ld=o("We perform these rigorous filtering steps for NSFW with potentially illegal content because we cannot guarantee that the contents of Common Crawl are free of such. We feel obligated to try our best to filter out such content. Inspections of samples filtered out by steps 7 to 9 have shown that our filtering procedure is very conservative and produces many false positives (samples it drops, which are not problematic). This process is okay because the number of potential samples waiting for us to crawl is vast."),_l=d(),ma=l("h3"),sd=o("System Architecture"),Ll=d(),pa=l("p"),rd=o("To orchestrate the interactions of the many crawling scripts (called workers) in our project, we use a server that keeps track of processed WAT files and of which worker gets which unprocessed WAT. We call this orchestrating server a tracker. Its functions are offering jobs to both download workers and inference workers, confirming cleanup requests from the DL staging server, maintaining ACLs for the Bloom server, and some more. We also employ several staging servers as buffers for jobs on their way to the storage location. The staging servers continuously update filters in the central bloom server where we use RedisBloom for high-performance reasons."),Il=d(),Xe=l("img"),kl=d(),ca=l("h4"),nd=o("Workflow"),Al=d(),wa=l("p"),hd=o("During the evolution of our crawling project, we applied two different workflows:"),Tl=d(),va=l("h4"),dd=o("Workflow 1 (\u201CHybrid\u201D - workers)"),Pl=d(),ga=l("p"),fd=o("This worker performs all computation steps during one job and then submits the result to the staging server. It then queues the results for release to the storage area."),Cl=d(),ba=l("h4"),ud=o("Workflow 2(\u201CCPU - GPU - 2 stages\u201D - workflow)"),Nl=d(),ya=l("p"),md=o("We soon discovered that the best way to utilise resources is to split the workload into CPU + networking tasks (downloading steps) and GPU tasks (CLIP inference steps). Hence, the 2 stage approach uses \u201CCPU workers\u201D to download images, create image-text pairs, and save the intermediate result to a staging server. Then \u201CGPU workers\u201D pick up jobs, concatenate a number of them to group around 20000 pairs per final result file. The 2 stage workflow proved to be most efficient, with speeds up to 25 million pairs added to the dataset per day when using 100 CPU workers with one core and one GPU worker employing an NVidia RTX 3090 graphic card utilising all 16 lanes of PCIe bus. The GPU node also needs about CPU 24 threads to keep up with the GPU processing capacity."),Wl=d(),Ea=l("h3"),pd=o("Removing abuse alerts"),xl=d(),_a=l("p"),cd=o("During downloading, we encountered abuse alerts from manual and automated tools that protect websites. After some learning curve, we reduced most of the issues by employing these mitigation techniques:"),Sl=d(),ne=l("ul"),Zo=l("li"),wd=o("By far, the most efficient one was to use centralised bloom filters that eliminate requests going to the duplicate URLs over and over. Of course, the efficiency of these filters dramatically depends on how fast they are updated and used by the workers. By definition, having multiple downloading workers performing jobs in parallel makes them prone to overlap requests to the same URL even if the bloom filters are up to date at the beginning of the job."),vd=d(),Qo=l("li"),gd=o("Therefore the second technique significantly reduced the problem of parallel workers via randomising the jobs at the tracker server level. While executing jobs in sequence (with the oldest WAT files from 2013), we discovered that adjacent jobs were overlapping considerably. When we randomised jobs, we saw a dramatic decrease in such overlapping."),Ul=d(),La=l("h3"),bd=o("Who ran this?"),Dl=d(),Ia=l("p"),yd=o("We want to thank:"),Ml=d(),I=l("ul"),Je=l("li"),Ed=o("The "),Ze=l("a"),_d=o("LAION folks"),Ld=o(", via so many worker nodes everywhere in the cloud."),Id=d(),Qe=l("li"),kd=o("The "),et=l("a"),Ad=o("data hoarders"),Td=o(" Reddit community."),Pd=d(),ka=l("li"),tt=l("a"),Cd=o("the eye"),Nd=o(" community."),Wd=d(),ei=l("li"),xd=o("as well as all our friends and relatives that did not know what they were helping with"),Ol=d(),Aa=l("p"),Sd=o("for running the workers to produce this vast dataset in a few months."),Bl=d(),Ta=l("h2"),Ud=o("2. Post-processing of the dataset"),Gl=d(),k=l("p"),Dd=o("Once the distributed pipeline has run, resulting in a sizeable caption+url dataset, it\u2019s time to package it in the best way. The objective of this second pipeline is to produce a version of the dataset that is easy to use for multimodal training. For this, we built tools that anyone can run out of a collection of caption+url. The exact command line to run is available in "),at=l("a"),Md=o("cah-prepro"),Od=o(" (which uses mainly "),ot=l("a"),Bd=o("img2dataset"),Gd=o(" and "),it=l("a"),Fd=o("clip-retrieval"),qd=o(" )"),Fl=d(),Pa=l("h3"),Hd=o("Pyspark preprocessing of the CSV files"),ql=d(),A=l("p"),Rd=o("After a fast run of a script to "),lt=l("a"),zd=o("download the CSV files"),jd=o(", the first step of this post-processing pipeline is to do deduplication by url+caption. The first pipeline does some partial deduplication using a bloom filter, but it is approximate, and some duplicates remain. Doing that pyspark post-processing also makes it possible to reduce the number of metadata files from hundred of thousands to 32 parquet files of size 1.7GB. See this "),st=l("a"),Yd=o("deduplication script there"),Kd=o(". Pyspark would be an excellent way to do any further filtering, and we "),rt=l("a"),Vd=o("provide"),$d=o(" an example to compute some statistics. The resulting output is 32 parquet files containing columns such as URL, text, NSFW described at the beginning of the post."),Hl=d(),Ca=l("h3"),Xd=o("Img2dataset"),Rl=d(),H=l("p"),Jd=o("Once this set of 50GB parquet files has is ready, we can use the "),nt=l("a"),Zd=o("img2dataset"),Qd=o(" tool to download, resize and store the images and captions as "),ht=l("a"),ef=o("webdataset"),tf=o(". This tool can download 100M images in 20h in a single node (1Gbps 32GB of ram 16 i7 cores), so anyone can run this for the whole dataset or a smaller subset. The format this tool outputs is a collection of tar files (that dataset format is called webdataset) containing images, captions, and metadata and corresponding parquet files containing the same metadata."),zl=d(),he=l("ul"),dt=l("li"),ti=l("code"),af=o("00000.tar"),of=o(" of size 270MB containing at most 10k samples"),V=l("ul"),ai=l("li"),oi=l("code"),lf=o("0.jpg"),sf=d(),ii=l("li"),li=l("code"),rf=o("0.txt"),nf=d(),Na=l("li"),si=l("code"),hf=o("0.json"),df=o(" containing metadata such as the URL, original width, EXIF data, whether the image is NSFW."),ff=d(),Wa=l("li"),ri=l("code"),uf=o("00000.parquet"),mf=o(" of size 1.6MB containing the same metadata as the JSON file. USeful to compute statistics without reading all the tar files."),jl=d(),de=l("p"),pf=o("The size of the tars of 270MB is when using the options of img2dataset indicated there "),ft=l("a"),cf=o("download_images.sh"),wf=o(" (resizing all images to 256\xD7256 with padding for maximum file uniformity and avoid losing information). If using different options, you may have larger or smaller tar files."),Yl=d(),xa=l("h3"),vf=o("CLIP retrieval and autofaiss"),Kl=d(),T=l("p"),gf=o("Finally, the tar dataset aims to compute and package clip embeddings and compute a KNN index over the clip embeddings. The "),ut=l("a"),bf=o("clip-retrieval"),yf=o(" tool makes it fast to compute 100M embeddings per 20h with a single 3080 GPU, so it\u2019s possible to rerun this part on the whole dataset or a subset at a low cost. The embeddings are stored in NPY files next to parquet files in the same order. Since this dataset is much smaller than image one, each NPY file stores 1M samples. NPY files are 1GB in size, and parquet files are 150MB. There are a total of 400 such files. These embeddings help build text and an image knn index using the "),mt=l("a"),Ef=o("autofaiss"),_f=o(" tool, making it possible to produce a quantised index of an arbitrary file. The chosen index type is 6GB, so it\u2019s cheap for anyone to load and run fast (10ms) queries over the whole dataset. We also generated another kind of index of size 16GB. Thanks to memory mapping, it\u2019s also possible to load it at no ram usage. A simple "),pt=l("a"),Lf=o("web demo"),If=o(" shows the results."),Vl=d(),ct=l("img"),$l=d(),Sa=l("h3"),kf=o("License"),Xl=d(),fe=l("p"),Af=o("We distribute the metadata dataset (the parquet files) under the most open "),wt=l("a"),Tf=o("Creative Common CC-BY 4.0"),Pf=o(" license, which poses no particular restriction. The images are under their copyright."),Jl=d(),Ua=l("h2"),Cf=o("Contributing"),Zl=d(),Da=l("p"),Nf=o("You can contribute to the project to help us release the following dataset sizes at 1 billion pairs, 2 billion pairs and so on."),Ql=d(),Ma=l("p"),Wf=o("Choose one or more methods that suit you or your company:"),es=d(),R=l("ol"),C=l("li"),xf=o("Donate either "),vt=l("a"),Sf=o("cash"),Uf=o(" or "),gt=l("a"),Df=o("computing time"),Mf=o(". We also launched a "),bt=l("a"),Of=o("Go Get Funding campaign"),Bf=o("."),Gf=d(),ni=l("li"),Ff=o("Participate in the development effort."),qf=d(),hi=l("li"),Hf=o("Spread the word. At best, use the dataset, get nice results and mention it in your papers."),ts=d(),Oa=l("p"),Rf=o("Useful links:"),as=d(),P=l("ul"),ue=l("li"),zf=o("Dataset progress "),yt=l("a"),jf=o("Crawling@Home Dashboard"),Yf=o(" and "),Et=l("a"),Kf=o("leaderboard"),Vf=d(),Ba=l("li"),$f=o("Reddit "),_t=l("a"),Xf=o("post"),Jf=d(),Ga=l("li"),Zf=o("DALL-E PyTorch "),Lt=l("a"),Qf=o("Discord server"),eu=d(),Fa=l("li"),tu=o("Dall-E PyTorch "),It=l("a"),au=o("GitHub Repository"),this.h()},l(e){p=s(e,"H2",{});var n=r(p);N=i(n,"Concept and Content"),n.forEach(a),W=f(e),x=s(e,"P",{});var wu=r(x);c=i(wu,"The LAION-400M dataset is entirely openly, freely accessible."),wu.forEach(a),b=f(e),y=s(e,"P",{});var di=r(y);eo=s(di,"STRONG",{});var vu=r(eo);Is=i(vu,"WARNING"),vu.forEach(a),ks=i(di,": Be aware that the this large-scale dataset is non-curated. It was built for research purposes to enable testing model training on larger scale for broad researcher and other interested communities, and is "),to=s(di,"STRONG",{});var gu=r(to);As=i(gu,"not"),gu.forEach(a),Ts=i(di," meant for any real-world production or application."),di.forEach(a),mi=f(e),X=s(e,"P",{});var os=r(X);Ps=i(os,"We have filtered all images and texts in the LAION-400M dataset with OpenAI\u2019s "),Ee=s(os,"A",{href:!0,rel:!0});var bu=r(Ee);Cs=i(bu,"CLIP"),bu.forEach(a),Ns=i(os," by calculating the cosine similarity between the text and image embeddings and dropping those with a similarity below 0.3. The threshold of 0.3 had been determined through human evaluations and seemed to be a good heuristic for estimating semantic image-text-content matching."),os.forEach(a),pi=f(e),J=s(e,"P",{});var is=r(J);Ws=i(is,"The image-text pairs have been extracted from the "),_e=s(is,"A",{href:!0,rel:!0});var yu=r(_e);xs=i(yu,"Common Crawl"),yu.forEach(a),Ss=i(is," web data dump and are from random web pages crawled between 2014 and 2021."),is.forEach(a),ci=f(e),Tt=s(e,"H3",{});var Eu=r(Tt);Us=i(Eu,"Download Information"),Eu.forEach(a),wi=f(e),Pt=s(e,"H4",{});var _u=r(Pt);Ds=i(_u,"Update 16 dec 2021"),_u.forEach(a),vi=f(e),S=s(e,"P",{});var qa=r(S);Ms=i(qa,"While "),ao=s(qa,"STRONG",{});var Lu=r(ao);Os=i(Lu,"the eye"),Lu.forEach(a),Bs=i(qa," experiences technical difficulties, we provide an alternate download server for this dataset at this link: "),Le=s(qa,"A",{href:!0,rel:!0});var Iu=r(Le);Gs=i(Iu,"LAION400M at deploy.laion.ai"),Iu.forEach(a),Fs=i(qa,"."),qa.forEach(a),gi=f(e),Ct=s(e,"H4",{});var ku=r(Ct);qs=i(ku,"Original Information"),ku.forEach(a),bi=f(e),Nt=s(e,"P",{});var Au=r(Nt);Hs=i(Au,"You can find:"),Au.forEach(a),yi=f(e),U=s(e,"UL",{});var Ha=r(U);oo=s(Ha,"LI",{});var Tu=r(oo);Rs=i(Tu,"The CLIP image embeddings (NumPy files)"),Tu.forEach(a),zs=f(Ha),io=s(Ha,"LI",{});var Pu=r(io);js=i(Pu,"The parquet files"),Pu.forEach(a),Ys=f(Ha),lo=s(Ha,"LI",{});var Cu=r(lo);Ks=i(Cu,"KNN index of image embeddings"),Cu.forEach(a),Ha.forEach(a),Ei=f(e),Z=s(e,"P",{});var ls=r(Z);Vs=i(ls,"To download from "),so=s(ls,"STRONG",{});var Nu=r(so);$s=i(Nu,"the eye"),Nu.forEach(a),Xs=i(ls,", run this command:"),ls.forEach(a),_i=f(e),Ie=s(e,"PRE",{class:!0});var Nc=r(Ie);Nc.forEach(a),Li=f(e),D=s(e,"P",{});var Ra=r(D);Js=i(Ra,"You may want to use the "),ro=s(Ra,"CODE",{});var Wu=r(ro);Zs=i(Wu,"--show-files"),Wu.forEach(a),Qs=i(Ra," and "),no=s(Ra,"CODE",{});var xu=r(no);er=i(xu,"--select-file"),xu.forEach(a),tr=i(Ra," options to download only some files."),Ra.forEach(a),Ii=f(e),Q=s(e,"P",{});var ss=r(Q);ar=i(ss,"You can also find the files in "),ke=s(ss,"A",{href:!0,rel:!0});var Su=r(ke);or=i(Su,"LAION400M-met-release"),Su.forEach(a),ir=i(ss,"."),ss.forEach(a),ki=f(e),ee=s(e,"P",{});var rs=r(ee);lr=i(rs,"Some more significant KNN indices are present in "),Ae=s(rs,"A",{href:!0,rel:!0});var Uu=r(Ae);sr=i(Uu,"LAION400M-indexes"),Uu.forEach(a),rr=i(rs,". We advise using the 128GB ones."),rs.forEach(a),Ai=f(e),te=s(e,"P",{});var ns=r(te);nr=i(ns,"The parquet files in Kaggle: "),Te=s(ns,"A",{href:!0,rel:!0});var Du=r(Te);hr=i(Du,"LAION400M on Kaggle"),Du.forEach(a),dr=i(ns,"."),ns.forEach(a),Ti=f(e),M=s(e,"P",{});var za=r(M);fr=i(za,"After downloading the metadata as indicated above, you can run "),Pe=s(za,"A",{href:!0,rel:!0});var Mu=r(Pe);ur=i(Mu,"this command"),Mu.forEach(a),mr=i(za," to download the images and generate the webdataset fields (command using "),Ce=s(za,"A",{href:!0,rel:!0});var Ou=r(Ce);pr=i(Ou,"img2dataset"),Ou.forEach(a),cr=i(za,")."),za.forEach(a),Pi=f(e),Wt=s(e,"H3",{});var Bu=r(Wt);wr=i(Bu,"LAION-400M Dataset Statistics"),Bu.forEach(a),Ci=f(e),xt=s(e,"P",{});var Gu=r(xt);vr=i(Gu,"The LAION-400M and future even bigger ones are, in fact, datasets of datasets. For instance, we can filter it out by image sizes into smaller datasets like this:"),Gu.forEach(a),Ni=f(e),Ne=s(e,"PRE",{class:!0});var Wc=r(Ne);Wc.forEach(a),Wi=f(e),St=s(e,"P",{});var Fu=r(St);gr=i(Fu,"By using the KNN index, we can extract specialized datasets by domains of interest. They are (or will be) sufficient to train technical domain models."),Fu.forEach(a),xi=f(e),Ut=s(e,"H3",{});var qu=r(Ut);br=i(qu,"Disclaimer & Content Warning"),qu.forEach(a),Si=f(e),O=s(e,"P",{});var ja=r(O);yr=i(ja,"Our filtering protocols only removed NSFW images deteced as illegal, but the dataset stil lhas NSFW content accordingly marked in the metadata. When freely navigating through the dataset, keep in mind that it is a large-scale, "),ho=s(ja,"STRONG",{});var Hu=r(ho);Er=i(Hu,"non-curated"),Hu.forEach(a),_r=i(ja," set crawled from the internet for research purposes, such that collected links may lead to discomforting and disturbing content. Therefore, please use the demo liks with "),fo=s(ja,"STRONG",{});var Ru=r(fo);Lr=i(Ru,"caution"),Ru.forEach(a),Ir=i(ja,". You can extract a \u201Csafe\u201D subset by filtering out samples drawn with NSFW or via stricter CLIP filtering."),ja.forEach(a),Ui=f(e),Dt=s(e,"P",{});var zu=r(Dt);kr=i(zu,"There is a certain degree of duplication because we used URL+text as deduplication criteria. The same image with the same caption may sit at different URLs, causing duplicates. THe same image with other captions is not, however, considered duplicated."),zu.forEach(a),Di=f(e),Mt=s(e,"P",{});var ju=r(Mt);Ar=i(ju,"Using KNN clustering should make it easy to further deduplicate by image content."),ju.forEach(a),Mi=f(e),Ot=s(e,"H3",{});var Yu=r(Ot);Tr=i(Yu,"Random Non-NSFW samples from the LAION-400M dataset."),Yu.forEach(a),Oi=f(e),ae=s(e,"P",{});var hs=r(ae);Pr=i(hs,"Also, use "),We=s(hs,"A",{href:!0,rel:!0});var Ku=r(We);Cr=i(Ku,"the retrieval UI"),Ku.forEach(a),Nr=i(hs," for simple visualization of the dataset. There you can search among the dataset using CLIP and a knn index."),hs.forEach(a),Bi=f(e),Bt=s(e,"H3",{});var Vu=r(Bt);Wr=i(Vu,"LAION-400M Open Dataset structure"),Vu.forEach(a),Gi=f(e),Gt=s(e,"P",{});var $u=r(Gt);xr=i($u,"We produced the dataset in several formats to address the various use cases:"),$u.forEach(a),Fi=f(e),E=s(e,"UL",{});var me=r(E);uo=s(me,"LI",{});var Xu=r(uo);Sr=i(Xu,"A 50GB url+caption metadata dataset in parquet files. We can use the metadata to compute statistics and redownload part of the dataset."),Xu.forEach(a),Ur=f(me),mo=s(me,"LI",{});var Ju=r(mo);Dr=i(Ju,"A 10TB webdataset with 256x256 images, captions and metadata. It is a full version of teh dataset that can be used directly for training (this one is for internal use, you need to redownload images yourself due to licensing issues)"),Ju.forEach(a),Mr=f(me),po=s(me,"LI",{});var Zu=r(po);Or=i(Zu,"A 1TB set of the 400M text and image clip embeddings, useful to rebuild new knn indices"),Zu.forEach(a),Br=f(me),co=s(me,"LI",{});var Qu=r(co);Gr=i(Qu,"Pairs of 16G, 64G, and 128G knn indices (running in the web demo)"),Qu.forEach(a),me.forEach(a),qi=f(e),Ft=s(e,"H4",{});var em=r(Ft);Fr=i(em,"URL and caption metadata dataset"),em.forEach(a),Hi=f(e),qt=s(e,"P",{});var tm=r(qt);qr=i(tm,"We provide 32 parquet files of size around 1GB (total 50GB) with the image URLs, the associated texts and additional metadata in the following format:"),tm.forEach(a),Ri=f(e),xe=s(e,"PRE",{class:!0});var xc=r(xe);xc.forEach(a),zi=f(e),Ht=s(e,"P",{});var am=r(Ht);Hr=i(am,"where:"),am.forEach(a),ji=f(e),v=s(e,"UL",{});var z=r(v);Rt=s(z,"LI",{});var ou=r(Rt);wo=s(ou,"CODE",{});var om=r(wo);Rr=i(om,"SAMPLE_ID"),om.forEach(a),zr=i(ou,": A unique identifier"),ou.forEach(a),jr=f(z),Y=s(z,"LI",{});var Ya=r(Y);vo=s(Ya,"CODE",{});var im=r(vo);Yr=i(im,"LICENSE"),im.forEach(a),Kr=i(Ya,": Where we found a reative Commons License in the image data, we named it here like, e.g. "),go=s(Ya,"CODE",{});var lm=r(go);Vr=i(lm,'"creativecommons.org/licenses/by-nc-sa/3.0"'),lm.forEach(a),$r=i(Ya," - otherwise you\u2019ll find it here a "),bo=s(Ya,"CODE",{});var sm=r(bo);Xr=i(sm,'"?"'),sm.forEach(a),Ya.forEach(a),Jr=f(z),_=s(z,"LI",{});var $=r(_);yo=s($,"CODE",{});var rm=r(yo);Zr=i(rm,"NSFW"),rm.forEach(a),Qr=i($,": We used CLIP to estimate if the image has NSFW content. The esitmation has been pretty conservative, reducing false negatives at the cost of more false positives. Possible values are "),Eo=s($,"CODE",{});var nm=r(Eo);en=i(nm,'"UNLIKELY"'),nm.forEach(a),tn=i($,", "),_o=s($,"CODE",{});var hm=r(_o);an=i(hm,'"UNSURE"'),hm.forEach(a),on=i($,", and "),Lo=s($,"CODE",{});var dm=r(Lo);ln=i(dm,'"NSFW"'),dm.forEach(a),sn=i($,"."),$.forEach(a),rn=f(z),zt=s(z,"LI",{});var iu=r(zt);Io=s(iu,"CODE",{});var fm=r(Io);nn=i(fm,"similarity"),fm.forEach(a),hn=i(iu,": Value of the cosine similarity between the text and image embedding."),iu.forEach(a),dn=f(z),oe=s(z,"LI",{});var fi=r(oe);ko=s(fi,"CODE",{});var um=r(ko);fn=i(um,"WIDTH"),um.forEach(a),un=i(fi," and "),Ao=s(fi,"CODE",{});var mm=r(Ao);mn=i(mm,"HEIGHT"),mm.forEach(a),pn=i(fi,": Image size as the image was embedded. We downsized originals that were larger than 4K to 4K."),fi.forEach(a),z.forEach(a),Yi=f(e),ie=s(e,"P",{});var ds=r(ie);cn=i(ds,"The metadata dataset purpose is to download the images for the whole dataset or a subset of it by supplying it to the very efficient "),Se=s(ds,"A",{href:!0,rel:!0});var pm=r(Se);wn=i(pm,"img2dataset"),pm.forEach(a),vn=i(ds," tool."),ds.forEach(a),Ki=f(e),jt=s(e,"H4",{});var cm=r(jt);gn=i(cm,"10TB webdataset with images and captions"),cm.forEach(a),Vi=f(e),Yt=s(e,"P",{});var wm=r(Yt);bn=i(wm,"By running the img2dataset tool, we can download a 10TB webdataset. It will resize all images at 256x256 resolution, will append the corresponding caption and will generate a collection of tar files (the dataset format is called webdataset) containing images, captions, and metadata and related parquet files containing the same metadata."),wm.forEach(a),$i=f(e),le=s(e,"UL",{});var fs=r(le);Ue=s(fs,"LI",{});var us=r(Ue);To=s(us,"CODE",{});var vm=r(To);yn=i(vm,"00000.tar"),vm.forEach(a),En=i(us," of size 270MB containing at most 10k samples"),K=s(us,"UL",{});var Ka=r(K);Po=s(Ka,"LI",{});var gm=r(Po);Co=s(gm,"CODE",{});var bm=r(Co);_n=i(bm,"0.jpg"),bm.forEach(a),gm.forEach(a),Ln=f(Ka),No=s(Ka,"LI",{});var ym=r(No);Wo=s(ym,"CODE",{});var Em=r(Wo);In=i(Em,"0.txt"),Em.forEach(a),ym.forEach(a),kn=f(Ka),Kt=s(Ka,"LI",{});var lu=r(Kt);xo=s(lu,"CODE",{});var _m=r(xo);An=i(_m,"0.json"),_m.forEach(a),Tn=i(lu," containing metadata such as the URL, original width, EXIF data, whether the image is NSFW."),lu.forEach(a),Ka.forEach(a),us.forEach(a),Pn=f(fs),Vt=s(fs,"LI",{});var su=r(Vt);So=s(su,"CODE",{});var Lm=r(So);Cn=i(Lm,"00000.parquet"),Lm.forEach(a),Nn=i(su," of size 1.6MB containing the same metadata as the JSON file. USeful to compute statistics without reading all the tar files."),su.forEach(a),fs.forEach(a),Xi=f(e),$t=s(e,"P",{});var Im=r($t);Wn=i(Im,"The 400M dataset will therefore have 41455 tar files and 41455 parquet files. This dataset purpose is to train multimodal models like CLIP or DALL-E."),Im.forEach(a),Ji=f(e),Xt=s(e,"H4",{});var km=r(Xt);xn=i(km,"1TB of CLIP embeddings"),km.forEach(a),Zi=f(e),Jt=s(e,"P",{});var Am=r(Jt);Sn=i(Am,"The CLIP imbeddings are stored in NPY files next to parquet files in the same order. Since this dataset is much smaller than the image one, each NPY file stores 1M samples. Each NPY file is 1GB, and each parquet file is 150MB. There are a total of 400 such files. THe embeddings purpose is to compute statistics on the dataset, for example, using clustering or KNN indices."),Am.forEach(a),Qi=f(e),Zt=s(e,"H4",{});var Tm=r(Zt);Un=i(Tm,"Two small 6GB knn indices"),Tm.forEach(a),el=f(e),B=s(e,"P",{});var Va=r(B);Dn=i(Va,"We provide two 6GB indices built using the "),De=s(Va,"A",{href:!0,rel:!0});var Pm=r(De);Mn=i(Pm,"autofaiss"),Pm.forEach(a),On=i(Va,". WE can us them to compute a subset of the dataset and, more generally, to search among it efficiently. See the wearch "),Me=s(Va,"A",{href:!0,rel:!0});var Cm=r(Me);Bn=i(Cm,"web demo"),Cm.forEach(a),Gn=i(Va," of it. We can use the CLIP filter tool along with this index to produce subsets using search tearms efficiently. We also provide two 16GB indices of higher quality."),Va.forEach(a),tl=f(e),Qt=s(e,"H3",{});var Nm=r(Qt);Fn=i(Nm,"What can we do with the LAION-400M dataset?"),Nm.forEach(a),al=f(e),ea=s(e,"P",{});var Wm=r(ea);qn=i(Wm,"Vision and language modeling has been taking off in 2021. Here are some pointers about what this kind of image + text datasets unlocks and why it sesms interesting:"),Wm.forEach(a),ol=f(e),ta=s(e,"UL",{});var xm=r(ta);G=s(xm,"LI",{});var kt=r(G);Hn=i(kt,"Six months ago, OPENAI released two blog posts and papers, "),Oe=s(kt,"A",{href:!0,rel:!0});var Sm=r(Oe);Rn=i(Sm,"CLIP"),Sm.forEach(a),zn=i(kt," and "),Be=s(kt,"A",{href:!0,rel:!0});var Um=r(Be);jn=i(Um,"DALL-E"),Um.forEach(a),Yn=i(kt,". Both models rely on a large amount of (text, image) pairs. They used an unreleased 400M pairs dataset."),Ge=s(kt,"UL",{});var ms=r(Ge);Fe=s(ms,"LI",{});var ps=r(Fe);Kn=i(ps,"CLIP is a model that computes how related are a text and an image. IT makes it possible to build large text to image search, and makes it possible to create that kind of crazy text to image art "),qe=s(ps,"A",{href:!0,rel:!0});var Dm=r(qe);Vn=i(Dm,"clip-art"),Dm.forEach(a),$n=i(ps,". They released a smalla nd medium version of the model but no training code."),ps.forEach(a),Xn=f(ms),Uo=s(ms,"LI",{});var Mm=r(Uo);Jn=i(Mm,"DALL-E is a model that directly generates images from texts. As can be seen from the blog post, it achieves awe-inspiring results that could directly impact the world for anything that needs drawing and illustrations. OpenAI did not release any model, even through an API."),Mm.forEach(a),ms.forEach(a),kt.forEach(a),xm.forEach(a),il=f(e),L=s(e,"P",{});var pe=r(L);Zn=i(pe,"Since then, various researchers have organized several efforts to replicate DALL-E. People gathered initially around this excellent DALL-E replication repository "),He=s(pe,"A",{href:!0,rel:!0});var Om=r(He);Qn=i(Om,"DALLE-Pytorch"),Om.forEach(a),eh=i(pe," with some fantastic results visible in the readme. More refcently, as part of huggingrace events, new developments have been achieved (see "),Re=s(pe,"A",{href:!0,rel:!0});var Bm=r(Re);th=i(Bm,"DALL-E-mini report"),Bm.forEach(a),ah=i(pe,"), and an online demo is now available at "),ze=s(pe,"A",{href:!0,rel:!0});var Gm=r(ze);oh=i(Gm,"DALL-E-mini demo"),Gm.forEach(a),ih=i(pe,"."),pe.forEach(a),ll=f(e),aa=s(e,"P",{});var Fm=r(aa);lh=i(Fm,"The replication effort is still far from achieving the same performance as the original DALL-E, and it seems possible to go even further. Some people also want to make a better CLIP to produce even better-generated art."),Fm.forEach(a),sl=f(e),se=s(e,"P",{});var cs=r(se);sh=i(cs,"A large part of the results that we can achieve with such models is thanks to a large amount of data. Before LAION-400M, the largest open dataset for (image, text) pairs are in the order or 10M (see "),je=s(cs,"A",{href:!0,rel:!0});var qm=r(je);rh=i(qm,"DALL-E datasets"),qm.forEach(a),nh=i(cs,"), which is enough to train exciting models but not enough to reach the best performance. Having a public dataset with hundreds of millions of pairs will help build these image+text models."),cs.forEach(a),rl=f(e),oa=s(e,"H3",{});var Hm=r(oa);hh=i(Hm,"Analysis of the LAION-400M data"),Hm.forEach(a),nl=f(e),ia=s(e,"P",{});var Rm=r(ia);dh=i(Rm,"We annotated 3456 samples of the dataset and got the following results:"),Rm.forEach(a),hl=f(e),g=s(e,"UL",{});var j=r(g);Do=s(j,"LI",{});var zm=r(Do);fh=i(zm,"Correct positive NSFW: 4"),zm.forEach(a),uh=f(j),Mo=s(j,"LI",{});var jm=r(Mo);mh=i(jm,"Correct negative NSFW: 3371"),jm.forEach(a),ph=f(j),Oo=s(j,"LI",{});var Ym=r(Oo);ch=i(Ym,"False-positive NSFW: 73"),Ym.forEach(a),wh=f(j),Bo=s(j,"LI",{});var Km=r(Bo);vh=i(Km,"False-negative NSFW: 8"),Km.forEach(a),gh=f(j),Go=s(j,"LI",{});var Vm=r(Go);bh=i(Vm,"Bad captions: 3 (0.09%)"),Vm.forEach(a),j.forEach(a),dl=f(e),la=s(e,"P",{});var $m=r(la);yh=i($m,"The matching is excellent, thanks to CLIP. We could improve the NSFW automatic tagging in the future; however, the NSFW total rate is low enough (less than 1%) to make this not an issue."),$m.forEach(a),fl=f(e),sa=s(e,"H2",{});var Xm=r(sa);Eh=i(Xm,"Technical Details"),Xm.forEach(a),ul=f(e),ra=s(e,"P",{});var Jm=r(ra);_h=i(Jm,"The dataset acquisition has two significant parts:"),Jm.forEach(a),ml=f(e),re=s(e,"OL",{});var ws=r(re);Fo=s(ws,"LI",{});var Zm=r(Fo);Lh=i(Zm,"A distributed preprocessin gof the vast (many PBs) Common Crawl datasets, which produces a collection of matching URL and caption."),Zm.forEach(a),Ih=f(ws),qo=s(ws,"LI",{});var Qm=r(qo);kh=i(Qm,"A single node much lighter post-processing of the data that anyone can run in a few days and which produces the final dataset."),Qm.forEach(a),ws.forEach(a),pl=f(e),na=s(e,"H3",{});var ep=r(na);Ah=i(ep,"1. Distributed processing of Common Crawl"),ep.forEach(a),cl=f(e),F=s(e,"P",{});var $a=r(F);Th=i($a,"We acquire the raw web data for the creation of our dataset from Common Crawl. Common Crawl is non-profit organization dedicated to providing a copy of the internet to internet researchers, companies, and individuals at no cost for research and analysis. They regularly release dumps of HTML-like data parsed from billions of public websites found "),Ye=s($a,"A",{href:!0,rel:!0});var tp=r(Ye);Ph=i(tp,"on the Common Crawl website"),tp.forEach(a),Ch=i($a,". To create image-text pairs, we parse through the data from Common Crawl and parse out all HTML IMG tags containing an "),Ke=s($a,"A",{href:!0,rel:!0});var ap=r(Ke);Nh=i(ap,"alt text attribute"),ap.forEach(a),Wh=i($a,". Common Crawl provides its data in several formats. For our purpose, we chose to use the data in WAT format. The WAT files contain only the metadata of the crawled sites, which includes all links and IMG tags contained in the website. Parsing only this metadata is much faster than parsing the whole HTML text (provided in the WARC format)."),$a.forEach(a),wl=f(e),ha=s(e,"H4",{});var op=r(ha);xh=i(op,"Downloading original images"),op.forEach(a),vl=f(e),q=s(e,"P",{});var Xa=r(q);Sh=i(Xa,"We download the raw images from the URLs we parsed from the Common Crawl with asynchronous requests using the libraries "),Ve=s(Xa,"A",{href:!0,rel:!0});var ip=r(Ve);Uh=i(ip,"Trio"),ip.forEach(a),Dh=i(Xa," and "),$e=s(Xa,"A",{href:!0,rel:!0});var lp=r($e);Mh=i(lp,"Asks"),lp.forEach(a),Oh=i(Xa,". They allow us to go multithreading for a single CPU. Usually, a home internet link will be exhausted by a single or two CPUs. A data centre node can scale up benefits from guaranteed internet speed with a multiprocessing pool much faster than a single CPU node. At this time, we were able to use 50 cores with a full, secured 1Gbps connection to the public internet. This bandwidth must be avaialable to the downloading node, not shared among many nodes or apps. We have optimised the sript for speed while mitigating various errors we encountered. Usually, to satisfy a high-end demanding node such as above, we must take additional steps to provide DNS caching capabilities. We found that the knot-resolver ran with two processes and configured with caching option can solve this problem."),Xa.forEach(a),gl=f(e),da=s(e,"H4",{});var sp=r(da);Bh=i(sp,"Filtering out unusable image-text pairs"),sp.forEach(a),bl=f(e),fa=s(e,"P",{});var rp=r(fa);Gh=i(rp,"After downloading the WAT fiels from Common Crawl, we filter the samples in the following steps:"),rp.forEach(a),yl=f(e),m=s(e,"OL",{});var w=r(m);Ho=s(w,"LI",{});var np=r(Ho);Fh=i(np,"We dropped all samples with less than five character alt text length"),np.forEach(a),qh=f(w),Ro=s(w,"LI",{});var hp=r(Ro);Hh=i(hp,"We dropped all samples with less than 5 KB image size"),hp.forEach(a),Rh=f(w),zo=s(w,"LI",{});var dp=r(zo);zh=i(dp,"We use continuously updated bloom filters to drop samples that are already in our dataset. The bloom filters deduplicate by concatenating the URL and the alt text."),dp.forEach(a),jh=f(w),jo=s(w,"LI",{});var fp=r(jo);Yh=i(fp,"We use continuously updated bloom filters to drop samples from URLs that had timed out previously and therefore seem unreachable (or at least not reachable in an efficient way)"),fp.forEach(a),Kh=f(w),Yo=s(w,"LI",{});var up=r(Yo);Vh=i(up,"We use OpenAI\u2019s CLIP model (the \u2018ViT-B-32\u2018 version) to compute the image and alt text embeddings. Then we calculate the cosine similarity of both embedding vectors and drop all samples with a similarity below 0.3. We chose this threshold after trying different values and using human evaluations of how well the texts fit the images. Lower values like 0.28 or 0.29 also seemed okay in many cases, but after further inspections, we decided to choose the conservative value of 0.3."),up.forEach(a),$h=f(w),Ko=s(w,"LI",{});var mp=r(Ko);Xh=i(mp,"We use the CLIP embeddings of the images to estimate if their contents contain NSFW content. We do this by calculating CLIP embeddings for a list of image categories like, e.g. \u201Cselfie\u201D, \u201Cillustration\u201D, or \u201Clandscape\u201D, which also contains categories that indicate NSFW content like \u201Cporn\u201D and \u201Csex\u201D."),mp.forEach(a),Jh=f(w),Vo=s(w,"LI",{});var pp=r(Vo);Zh=i(pp,"Then we compute the cosine similarities between the embedding image we are currently filtering and each of these category keywords. If the category with the highest similarity and the keyword with the second-highest similarity belong both to NSFW keywords, we tag the sample as \u201CNSFW\u201D. If only one of them belongs to an NSFW keyword, we categorise the sample as \u201CUNSURE\u201D. If both keywords with the highest similarities are not NSFW, we tag the sample as \u201CUNLIKELY\u201D."),pp.forEach(a),Qh=f(w),$o=s(w,"LI",{});var cp=r($o);ed=i(cp,"In the next step, we look at all samples with either the \u201CNSFW\u201D or \u201CUNSURE\u201D tag and drop those with any keywords in their text related to kids, teens, or other semantically related content."),cp.forEach(a),td=f(w),Xo=s(w,"LI",{});var wp=r(Xo);ad=i(wp,"In step 8, we repeat the procedure of computing the cosine similarities from step 6 with the difference that we now use category texts that indicate contents semantically related to kids and teens on a CLIP embedding level. If either the highest similarity or the second-highest similarity between a sample\u2019s image embedding and a text of the precomputed categories belongs to a text that indicates content related to under-aged persons, we drop this sample."),wp.forEach(a),od=f(w),Jo=s(w,"LI",{});var vp=r(Jo);id=i(vp,"Finally, we repeat the procedure from step 8 with texts semantically related to animal categories like e.g. \u201Canimal\u201D, \u201Cbird\u201D, etc."),vp.forEach(a),w.forEach(a),El=f(e),ua=s(e,"P",{});var gp=r(ua);ld=i(gp,"We perform these rigorous filtering steps for NSFW with potentially illegal content because we cannot guarantee that the contents of Common Crawl are free of such. We feel obligated to try our best to filter out such content. Inspections of samples filtered out by steps 7 to 9 have shown that our filtering procedure is very conservative and produces many false positives (samples it drops, which are not problematic). This process is okay because the number of potential samples waiting for us to crawl is vast."),gp.forEach(a),_l=f(e),ma=s(e,"H3",{});var bp=r(ma);sd=i(bp,"System Architecture"),bp.forEach(a),Ll=f(e),pa=s(e,"P",{});var yp=r(pa);rd=i(yp,"To orchestrate the interactions of the many crawling scripts (called workers) in our project, we use a server that keeps track of processed WAT files and of which worker gets which unprocessed WAT. We call this orchestrating server a tracker. Its functions are offering jobs to both download workers and inference workers, confirming cleanup requests from the DL staging server, maintaining ACLs for the Bloom server, and some more. We also employ several staging servers as buffers for jobs on their way to the storage location. The staging servers continuously update filters in the central bloom server where we use RedisBloom for high-performance reasons."),yp.forEach(a),Il=f(e),Xe=s(e,"IMG",{src:!0,alt:!0}),kl=f(e),ca=s(e,"H4",{});var Ep=r(ca);nd=i(Ep,"Workflow"),Ep.forEach(a),Al=f(e),wa=s(e,"P",{});var _p=r(wa);hd=i(_p,"During the evolution of our crawling project, we applied two different workflows:"),_p.forEach(a),Tl=f(e),va=s(e,"H4",{});var Lp=r(va);dd=i(Lp,"Workflow 1 (\u201CHybrid\u201D - workers)"),Lp.forEach(a),Pl=f(e),ga=s(e,"P",{});var Ip=r(ga);fd=i(Ip,"This worker performs all computation steps during one job and then submits the result to the staging server. It then queues the results for release to the storage area."),Ip.forEach(a),Cl=f(e),ba=s(e,"H4",{});var kp=r(ba);ud=i(kp,"Workflow 2(\u201CCPU - GPU - 2 stages\u201D - workflow)"),kp.forEach(a),Nl=f(e),ya=s(e,"P",{});var Ap=r(ya);md=i(Ap,"We soon discovered that the best way to utilise resources is to split the workload into CPU + networking tasks (downloading steps) and GPU tasks (CLIP inference steps). Hence, the 2 stage approach uses \u201CCPU workers\u201D to download images, create image-text pairs, and save the intermediate result to a staging server. Then \u201CGPU workers\u201D pick up jobs, concatenate a number of them to group around 20000 pairs per final result file. The 2 stage workflow proved to be most efficient, with speeds up to 25 million pairs added to the dataset per day when using 100 CPU workers with one core and one GPU worker employing an NVidia RTX 3090 graphic card utilising all 16 lanes of PCIe bus. The GPU node also needs about CPU 24 threads to keep up with the GPU processing capacity."),Ap.forEach(a),Wl=f(e),Ea=s(e,"H3",{});var Tp=r(Ea);pd=i(Tp,"Removing abuse alerts"),Tp.forEach(a),xl=f(e),_a=s(e,"P",{});var Pp=r(_a);cd=i(Pp,"During downloading, we encountered abuse alerts from manual and automated tools that protect websites. After some learning curve, we reduced most of the issues by employing these mitigation techniques:"),Pp.forEach(a),Sl=f(e),ne=s(e,"UL",{});var vs=r(ne);Zo=s(vs,"LI",{});var Cp=r(Zo);wd=i(Cp,"By far, the most efficient one was to use centralised bloom filters that eliminate requests going to the duplicate URLs over and over. Of course, the efficiency of these filters dramatically depends on how fast they are updated and used by the workers. By definition, having multiple downloading workers performing jobs in parallel makes them prone to overlap requests to the same URL even if the bloom filters are up to date at the beginning of the job."),Cp.forEach(a),vd=f(vs),Qo=s(vs,"LI",{});var Np=r(Qo);gd=i(Np,"Therefore the second technique significantly reduced the problem of parallel workers via randomising the jobs at the tracker server level. While executing jobs in sequence (with the oldest WAT files from 2013), we discovered that adjacent jobs were overlapping considerably. When we randomised jobs, we saw a dramatic decrease in such overlapping."),Np.forEach(a),vs.forEach(a),Ul=f(e),La=s(e,"H3",{});var Wp=r(La);bd=i(Wp,"Who ran this?"),Wp.forEach(a),Dl=f(e),Ia=s(e,"P",{});var xp=r(Ia);yd=i(xp,"We want to thank:"),xp.forEach(a),Ml=f(e),I=s(e,"UL",{});var ce=r(I);Je=s(ce,"LI",{});var gs=r(Je);Ed=i(gs,"The "),Ze=s(gs,"A",{href:!0,rel:!0});var Sp=r(Ze);_d=i(Sp,"LAION folks"),Sp.forEach(a),Ld=i(gs,", via so many worker nodes everywhere in the cloud."),gs.forEach(a),Id=f(ce),Qe=s(ce,"LI",{});var bs=r(Qe);kd=i(bs,"The "),et=s(bs,"A",{href:!0,rel:!0});var Up=r(et);Ad=i(Up,"data hoarders"),Up.forEach(a),Td=i(bs," Reddit community."),bs.forEach(a),Pd=f(ce),ka=s(ce,"LI",{});var ru=r(ka);tt=s(ru,"A",{href:!0,rel:!0});var Dp=r(tt);Cd=i(Dp,"the eye"),Dp.forEach(a),Nd=i(ru," community."),ru.forEach(a),Wd=f(ce),ei=s(ce,"LI",{});var Mp=r(ei);xd=i(Mp,"as well as all our friends and relatives that did not know what they were helping with"),Mp.forEach(a),ce.forEach(a),Ol=f(e),Aa=s(e,"P",{});var Op=r(Aa);Sd=i(Op,"for running the workers to produce this vast dataset in a few months."),Op.forEach(a),Bl=f(e),Ta=s(e,"H2",{});var Bp=r(Ta);Ud=i(Bp,"2. Post-processing of the dataset"),Bp.forEach(a),Gl=f(e),k=s(e,"P",{});var we=r(k);Dd=i(we,"Once the distributed pipeline has run, resulting in a sizeable caption+url dataset, it\u2019s time to package it in the best way. The objective of this second pipeline is to produce a version of the dataset that is easy to use for multimodal training. For this, we built tools that anyone can run out of a collection of caption+url. The exact command line to run is available in "),at=s(we,"A",{href:!0,rel:!0});var Gp=r(at);Md=i(Gp,"cah-prepro"),Gp.forEach(a),Od=i(we," (which uses mainly "),ot=s(we,"A",{href:!0,rel:!0});var Fp=r(ot);Bd=i(Fp,"img2dataset"),Fp.forEach(a),Gd=i(we," and "),it=s(we,"A",{href:!0,rel:!0});var qp=r(it);Fd=i(qp,"clip-retrieval"),qp.forEach(a),qd=i(we," )"),we.forEach(a),Fl=f(e),Pa=s(e,"H3",{});var Hp=r(Pa);Hd=i(Hp,"Pyspark preprocessing of the CSV files"),Hp.forEach(a),ql=f(e),A=s(e,"P",{});var ve=r(A);Rd=i(ve,"After a fast run of a script to "),lt=s(ve,"A",{href:!0,rel:!0});var Rp=r(lt);zd=i(Rp,"download the CSV files"),Rp.forEach(a),jd=i(ve,", the first step of this post-processing pipeline is to do deduplication by url+caption. The first pipeline does some partial deduplication using a bloom filter, but it is approximate, and some duplicates remain. Doing that pyspark post-processing also makes it possible to reduce the number of metadata files from hundred of thousands to 32 parquet files of size 1.7GB. See this "),st=s(ve,"A",{href:!0,rel:!0});var zp=r(st);Yd=i(zp,"deduplication script there"),zp.forEach(a),Kd=i(ve,". Pyspark would be an excellent way to do any further filtering, and we "),rt=s(ve,"A",{href:!0,rel:!0});var jp=r(rt);Vd=i(jp,"provide"),jp.forEach(a),$d=i(ve," an example to compute some statistics. The resulting output is 32 parquet files containing columns such as URL, text, NSFW described at the beginning of the post."),ve.forEach(a),Hl=f(e),Ca=s(e,"H3",{});var Yp=r(Ca);Xd=i(Yp,"Img2dataset"),Yp.forEach(a),Rl=f(e),H=s(e,"P",{});var Ja=r(H);Jd=i(Ja,"Once this set of 50GB parquet files has is ready, we can use the "),nt=s(Ja,"A",{href:!0,rel:!0});var Kp=r(nt);Zd=i(Kp,"img2dataset"),Kp.forEach(a),Qd=i(Ja," tool to download, resize and store the images and captions as "),ht=s(Ja,"A",{href:!0,rel:!0});var Vp=r(ht);ef=i(Vp,"webdataset"),Vp.forEach(a),tf=i(Ja,". This tool can download 100M images in 20h in a single node (1Gbps 32GB of ram 16 i7 cores), so anyone can run this for the whole dataset or a smaller subset. The format this tool outputs is a collection of tar files (that dataset format is called webdataset) containing images, captions, and metadata and corresponding parquet files containing the same metadata."),Ja.forEach(a),zl=f(e),he=s(e,"UL",{});var ys=r(he);dt=s(ys,"LI",{});var Es=r(dt);ti=s(Es,"CODE",{});var $p=r(ti);af=i($p,"00000.tar"),$p.forEach(a),of=i(Es," of size 270MB containing at most 10k samples"),V=s(Es,"UL",{});var Za=r(V);ai=s(Za,"LI",{});var Xp=r(ai);oi=s(Xp,"CODE",{});var Jp=r(oi);lf=i(Jp,"0.jpg"),Jp.forEach(a),Xp.forEach(a),sf=f(Za),ii=s(Za,"LI",{});var Zp=r(ii);li=s(Zp,"CODE",{});var Qp=r(li);rf=i(Qp,"0.txt"),Qp.forEach(a),Zp.forEach(a),nf=f(Za),Na=s(Za,"LI",{});var nu=r(Na);si=s(nu,"CODE",{});var ec=r(si);hf=i(ec,"0.json"),ec.forEach(a),df=i(nu," containing metadata such as the URL, original width, EXIF data, whether the image is NSFW."),nu.forEach(a),Za.forEach(a),Es.forEach(a),ff=f(ys),Wa=s(ys,"LI",{});var hu=r(Wa);ri=s(hu,"CODE",{});var tc=r(ri);uf=i(tc,"00000.parquet"),tc.forEach(a),mf=i(hu," of size 1.6MB containing the same metadata as the JSON file. USeful to compute statistics without reading all the tar files."),hu.forEach(a),ys.forEach(a),jl=f(e),de=s(e,"P",{});var _s=r(de);pf=i(_s,"The size of the tars of 270MB is when using the options of img2dataset indicated there "),ft=s(_s,"A",{href:!0,rel:!0});var ac=r(ft);cf=i(ac,"download_images.sh"),ac.forEach(a),wf=i(_s," (resizing all images to 256\xD7256 with padding for maximum file uniformity and avoid losing information). If using different options, you may have larger or smaller tar files."),_s.forEach(a),Yl=f(e),xa=s(e,"H3",{});var oc=r(xa);vf=i(oc,"CLIP retrieval and autofaiss"),oc.forEach(a),Kl=f(e),T=s(e,"P",{});var ge=r(T);gf=i(ge,"Finally, the tar dataset aims to compute and package clip embeddings and compute a KNN index over the clip embeddings. The "),ut=s(ge,"A",{href:!0,rel:!0});var ic=r(ut);bf=i(ic,"clip-retrieval"),ic.forEach(a),yf=i(ge," tool makes it fast to compute 100M embeddings per 20h with a single 3080 GPU, so it\u2019s possible to rerun this part on the whole dataset or a subset at a low cost. The embeddings are stored in NPY files next to parquet files in the same order. Since this dataset is much smaller than image one, each NPY file stores 1M samples. NPY files are 1GB in size, and parquet files are 150MB. There are a total of 400 such files. These embeddings help build text and an image knn index using the "),mt=s(ge,"A",{href:!0,rel:!0});var lc=r(mt);Ef=i(lc,"autofaiss"),lc.forEach(a),_f=i(ge," tool, making it possible to produce a quantised index of an arbitrary file. The chosen index type is 6GB, so it\u2019s cheap for anyone to load and run fast (10ms) queries over the whole dataset. We also generated another kind of index of size 16GB. Thanks to memory mapping, it\u2019s also possible to load it at no ram usage. A simple "),pt=s(ge,"A",{href:!0,rel:!0});var sc=r(pt);Lf=i(sc,"web demo"),sc.forEach(a),If=i(ge," shows the results."),ge.forEach(a),Vl=f(e),ct=s(e,"IMG",{src:!0,alt:!0}),$l=f(e),Sa=s(e,"H3",{});var rc=r(Sa);kf=i(rc,"License"),rc.forEach(a),Xl=f(e),fe=s(e,"P",{});var Ls=r(fe);Af=i(Ls,"We distribute the metadata dataset (the parquet files) under the most open "),wt=s(Ls,"A",{href:!0,rel:!0});var nc=r(wt);Tf=i(nc,"Creative Common CC-BY 4.0"),nc.forEach(a),Pf=i(Ls," license, which poses no particular restriction. The images are under their copyright."),Ls.forEach(a),Jl=f(e),Ua=s(e,"H2",{});var hc=r(Ua);Cf=i(hc,"Contributing"),hc.forEach(a),Zl=f(e),Da=s(e,"P",{});var dc=r(Da);Nf=i(dc,"You can contribute to the project to help us release the following dataset sizes at 1 billion pairs, 2 billion pairs and so on."),dc.forEach(a),Ql=f(e),Ma=s(e,"P",{});var fc=r(Ma);Wf=i(fc,"Choose one or more methods that suit you or your company:"),fc.forEach(a),es=f(e),R=s(e,"OL",{});var Qa=r(R);C=s(Qa,"LI",{});var be=r(C);xf=i(be,"Donate either "),vt=s(be,"A",{href:!0,rel:!0});var uc=r(vt);Sf=i(uc,"cash"),uc.forEach(a),Uf=i(be," or "),gt=s(be,"A",{href:!0,rel:!0});var mc=r(gt);Df=i(mc,"computing time"),mc.forEach(a),Mf=i(be,". We also launched a "),bt=s(be,"A",{href:!0,rel:!0});var pc=r(bt);Of=i(pc,"Go Get Funding campaign"),pc.forEach(a),Bf=i(be,"."),be.forEach(a),Gf=f(Qa),ni=s(Qa,"LI",{});var cc=r(ni);Ff=i(cc,"Participate in the development effort."),cc.forEach(a),qf=f(Qa),hi=s(Qa,"LI",{});var wc=r(hi);Hf=i(wc,"Spread the word. At best, use the dataset, get nice results and mention it in your papers."),wc.forEach(a),Qa.forEach(a),ts=f(e),Oa=s(e,"P",{});var vc=r(Oa);Rf=i(vc,"Useful links:"),vc.forEach(a),as=f(e),P=s(e,"UL",{});var ye=r(P);ue=s(ye,"LI",{});var ui=r(ue);zf=i(ui,"Dataset progress "),yt=s(ui,"A",{href:!0,rel:!0});var gc=r(yt);jf=i(gc,"Crawling@Home Dashboard"),gc.forEach(a),Yf=i(ui," and "),Et=s(ui,"A",{href:!0,rel:!0});var bc=r(Et);Kf=i(bc,"leaderboard"),bc.forEach(a),ui.forEach(a),Vf=f(ye),Ba=s(ye,"LI",{});var du=r(Ba);$f=i(du,"Reddit "),_t=s(du,"A",{href:!0,rel:!0});var yc=r(_t);Xf=i(yc,"post"),yc.forEach(a),du.forEach(a),Jf=f(ye),Ga=s(ye,"LI",{});var fu=r(Ga);Zf=i(fu,"DALL-E PyTorch "),Lt=s(fu,"A",{href:!0,rel:!0});var Ec=r(Lt);Qf=i(Ec,"Discord server"),Ec.forEach(a),fu.forEach(a),eu=f(ye),Fa=s(ye,"LI",{});var uu=r(Fa);tu=i(uu,"Dall-E PyTorch "),It=s(uu,"A",{href:!0,rel:!0});var _c=r(It);au=i(_c,"GitHub Repository"),_c.forEach(a),uu.forEach(a),ye.forEach(a),this.h()},h(){u(Ee,"href","https://openai.com/blog/clip/"),u(Ee,"rel","nofollow"),u(_e,"href","https://commoncrawl.org/"),u(_e,"rel","nofollow"),u(Le,"href","http://deploy.laion.ai/8f83b608504d46bb81708ec86e912220/"),u(Le,"rel","nofollow"),u(Ie,"class","language-undefined"),u(ke,"href","https://the-eye.eu/public/AI/cah/laion400m-met-release/"),u(ke,"rel","nofollow"),u(Ae,"href","https://the-eye.eu/public/AI/cah/laion400m-indexes/"),u(Ae,"rel","nofollow"),u(Te,"href","https://www.kaggle.com/romainbeaumont/laion400m"),u(Te,"rel","nofollow"),u(Pe,"href","https://github.com/rom1504/cah-prepro/blob/main/download_images/download_images.sh"),u(Pe,"rel","nofollow"),u(Ce,"href","https://github.com/rom1504/img2dataset"),u(Ce,"rel","nofollow"),u(Ne,"class","language-undefined"),u(We,"href","https://rom1504.github.io/clip-retrieval"),u(We,"rel","nofollow"),u(xe,"class","language-undefined"),u(Se,"href","https://github.com/rom1504/img2dataset"),u(Se,"rel","nofollow"),u(De,"href","https://github.com/criteo/autofaiss"),u(De,"rel","nofollow"),u(Me,"href","https://rom1504.github.io/clip-retrieval/"),u(Me,"rel","nofollow"),u(Oe,"href","https://openai.com/blog/clip/"),u(Oe,"rel","nofollow"),u(Be,"href","https://openai.com/blog/dall-e/"),u(Be,"rel","nofollow"),u(qe,"href","https://ml.berkeley.edu/blog/posts/clip-art/"),u(qe,"rel","nofollow"),u(He,"href","https://github.com/lucidrains/DALLE-pytorch"),u(He,"rel","nofollow"),u(Re,"href","https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA"),u(Re,"rel","nofollow"),u(ze,"href","https://huggingface.co/spaces/flax-community/dalle-mini"),u(ze,"rel","nofollow"),u(je,"href","https://github.com/robvanvolt/DALLE-datasets"),u(je,"rel","nofollow"),u(Ye,"href","https://commoncrawl.org/the-data/get-started/"),u(Ye,"rel","nofollow"),u(Ke,"href","https://en.wikipedia.org/wiki/Alt_attribute"),u(Ke,"rel","nofollow"),u(Ve,"href","https://github.com/python-trio/trio"),u(Ve,"rel","nofollow"),u($e,"href","https://github.com/theelous3/asks"),u($e,"rel","nofollow"),kc(Xe.src,pu=jc)||u(Xe,"src",pu),u(Xe,"alt","System architecture for LAION's dataset crawling and filtering system."),u(Ze,"href","https://laion.ai/#team"),u(Ze,"rel","nofollow"),u(et,"href","https://www.reddit.com/r/DataHoarder/comments/oyta8q/crawlinghome_help_build_the_worlds_largest/"),u(et,"rel","nofollow"),u(tt,"href","https://the-eye.eu/"),u(tt,"rel","nofollow"),u(at,"href","https://github.com/rom1504/cah-prepro"),u(at,"rel","nofollow"),u(ot,"href","https://github.com/rom1504/img2dataset"),u(ot,"rel","nofollow"),u(it,"href","https://github.com/rom1504/clip-retrieval"),u(it,"rel","nofollow"),u(lt,"href","https://github.com/rom1504/cah-prepro/tree/main/download_csv"),u(lt,"rel","nofollow"),u(st,"href","https://github.com/rom1504/cah-prepro/blob/main/deduplicate/cah_stats_spark.py"),u(st,"rel","nofollow"),u(rt,"href","https://github.com/rom1504/cah-prepro/blob/main/deduplicate/compute_more_stats.py"),u(rt,"rel","nofollow"),u(nt,"href","https://github.com/rom1504/img2dataset"),u(nt,"rel","nofollow"),u(ht,"href","https://github.com/webdataset/webdataset"),u(ht,"rel","nofollow"),u(ft,"href","https://github.com/rom1504/cah-prepro/blob/main/download_images/download_images.sh"),u(ft,"rel","nofollow"),u(ut,"href","https://github.com/rom1504/clip-retrieval/"),u(ut,"rel","nofollow"),u(mt,"href","https://github.com/criteo/autofaiss"),u(mt,"rel","nofollow"),u(pt,"href","https://rom1504.github.io/clip-retrieval/"),u(pt,"rel","nofollow"),kc(ct.src,cu=Yc)||u(ct,"src",cu),u(ct,"alt","Result of semantic search for the text 'blue cat' using the clip-retrieval tool"),u(wt,"href","https://creativecommons.org/licenses/by/4.0/"),u(wt,"rel","nofollow"),u(vt,"href","https://laion.ai/laion-400-open-dataset/#"),u(vt,"rel","nofollow"),u(gt,"href","https://laion.ai/how-to-donate-computing-time/"),u(gt,"rel","nofollow"),u(bt,"href","https://gogetfunding.com/help-us-build-the-worlds-largest-open-billion-scale-image-text-dataset-perfect-for-training-dall-e-clip-other-multimodal-models/"),u(bt,"rel","nofollow"),u(yt,"href","http://crawling.at/"),u(yt,"rel","nofollow"),u(Et,"href","http://crawling.at/leaderboard"),u(Et,"rel","nofollow"),u(_t,"href","https://www.reddit.com/r/DataHoarder/comments/oyta8q/crawlinghome_help_build_the_worlds_largest/?utm_source=share&utm_medium=web2x&context=3"),u(_t,"rel","nofollow"),u(Lt,"href","https://discord.gg/mVcgxMPD7e"),u(Lt,"rel","nofollow"),u(It,"href","https://github.com/lucidrains/DALLE-pytorch"),u(It,"rel","nofollow")},m(e,n){h(e,p,n),t(p,N),h(e,W,n),h(e,x,n),t(x,c),h(e,b,n),h(e,y,n),t(y,eo),t(eo,Is),t(y,ks),t(y,to),t(to,As),t(y,Ts),h(e,mi,n),h(e,X,n),t(X,Ps),t(X,Ee),t(Ee,Cs),t(X,Ns),h(e,pi,n),h(e,J,n),t(J,Ws),t(J,_e),t(_e,xs),t(J,Ss),h(e,ci,n),h(e,Tt,n),t(Tt,Us),h(e,wi,n),h(e,Pt,n),t(Pt,Ds),h(e,vi,n),h(e,S,n),t(S,Ms),t(S,ao),t(ao,Os),t(S,Bs),t(S,Le),t(Le,Gs),t(S,Fs),h(e,gi,n),h(e,Ct,n),t(Ct,qs),h(e,bi,n),h(e,Nt,n),t(Nt,Hs),h(e,yi,n),h(e,U,n),t(U,oo),t(oo,Rs),t(U,zs),t(U,io),t(io,js),t(U,Ys),t(U,lo),t(lo,Ks),h(e,Ei,n),h(e,Z,n),t(Z,Vs),t(Z,so),t(so,$s),t(Z,Xs),h(e,_i,n),h(e,Ie,n),Ie.innerHTML=Tc,h(e,Li,n),h(e,D,n),t(D,Js),t(D,ro),t(ro,Zs),t(D,Qs),t(D,no),t(no,er),t(D,tr),h(e,Ii,n),h(e,Q,n),t(Q,ar),t(Q,ke),t(ke,or),t(Q,ir),h(e,ki,n),h(e,ee,n),t(ee,lr),t(ee,Ae),t(Ae,sr),t(ee,rr),h(e,Ai,n),h(e,te,n),t(te,nr),t(te,Te),t(Te,hr),t(te,dr),h(e,Ti,n),h(e,M,n),t(M,fr),t(M,Pe),t(Pe,ur),t(M,mr),t(M,Ce),t(Ce,pr),t(M,cr),h(e,Pi,n),h(e,Wt,n),t(Wt,wr),h(e,Ci,n),h(e,xt,n),t(xt,vr),h(e,Ni,n),h(e,Ne,n),Ne.innerHTML=Pc,h(e,Wi,n),h(e,St,n),t(St,gr),h(e,xi,n),h(e,Ut,n),t(Ut,br),h(e,Si,n),h(e,O,n),t(O,yr),t(O,ho),t(ho,Er),t(O,_r),t(O,fo),t(fo,Lr),t(O,Ir),h(e,Ui,n),h(e,Dt,n),t(Dt,kr),h(e,Di,n),h(e,Mt,n),t(Mt,Ar),h(e,Mi,n),h(e,Ot,n),t(Ot,Tr),h(e,Oi,n),h(e,ae,n),t(ae,Pr),t(ae,We),t(We,Cr),t(ae,Nr),h(e,Bi,n),h(e,Bt,n),t(Bt,Wr),h(e,Gi,n),h(e,Gt,n),t(Gt,xr),h(e,Fi,n),h(e,E,n),t(E,uo),t(uo,Sr),t(E,Ur),t(E,mo),t(mo,Dr),t(E,Mr),t(E,po),t(po,Or),t(E,Br),t(E,co),t(co,Gr),h(e,qi,n),h(e,Ft,n),t(Ft,Fr),h(e,Hi,n),h(e,qt,n),t(qt,qr),h(e,Ri,n),h(e,xe,n),xe.innerHTML=Cc,h(e,zi,n),h(e,Ht,n),t(Ht,Hr),h(e,ji,n),h(e,v,n),t(v,Rt),t(Rt,wo),t(wo,Rr),t(Rt,zr),t(v,jr),t(v,Y),t(Y,vo),t(vo,Yr),t(Y,Kr),t(Y,go),t(go,Vr),t(Y,$r),t(Y,bo),t(bo,Xr),t(v,Jr),t(v,_),t(_,yo),t(yo,Zr),t(_,Qr),t(_,Eo),t(Eo,en),t(_,tn),t(_,_o),t(_o,an),t(_,on),t(_,Lo),t(Lo,ln),t(_,sn),t(v,rn),t(v,zt),t(zt,Io),t(Io,nn),t(zt,hn),t(v,dn),t(v,oe),t(oe,ko),t(ko,fn),t(oe,un),t(oe,Ao),t(Ao,mn),t(oe,pn),h(e,Yi,n),h(e,ie,n),t(ie,cn),t(ie,Se),t(Se,wn),t(ie,vn),h(e,Ki,n),h(e,jt,n),t(jt,gn),h(e,Vi,n),h(e,Yt,n),t(Yt,bn),h(e,$i,n),h(e,le,n),t(le,Ue),t(Ue,To),t(To,yn),t(Ue,En),t(Ue,K),t(K,Po),t(Po,Co),t(Co,_n),t(K,Ln),t(K,No),t(No,Wo),t(Wo,In),t(K,kn),t(K,Kt),t(Kt,xo),t(xo,An),t(Kt,Tn),t(le,Pn),t(le,Vt),t(Vt,So),t(So,Cn),t(Vt,Nn),h(e,Xi,n),h(e,$t,n),t($t,Wn),h(e,Ji,n),h(e,Xt,n),t(Xt,xn),h(e,Zi,n),h(e,Jt,n),t(Jt,Sn),h(e,Qi,n),h(e,Zt,n),t(Zt,Un),h(e,el,n),h(e,B,n),t(B,Dn),t(B,De),t(De,Mn),t(B,On),t(B,Me),t(Me,Bn),t(B,Gn),h(e,tl,n),h(e,Qt,n),t(Qt,Fn),h(e,al,n),h(e,ea,n),t(ea,qn),h(e,ol,n),h(e,ta,n),t(ta,G),t(G,Hn),t(G,Oe),t(Oe,Rn),t(G,zn),t(G,Be),t(Be,jn),t(G,Yn),t(G,Ge),t(Ge,Fe),t(Fe,Kn),t(Fe,qe),t(qe,Vn),t(Fe,$n),t(Ge,Xn),t(Ge,Uo),t(Uo,Jn),h(e,il,n),h(e,L,n),t(L,Zn),t(L,He),t(He,Qn),t(L,eh),t(L,Re),t(Re,th),t(L,ah),t(L,ze),t(ze,oh),t(L,ih),h(e,ll,n),h(e,aa,n),t(aa,lh),h(e,sl,n),h(e,se,n),t(se,sh),t(se,je),t(je,rh),t(se,nh),h(e,rl,n),h(e,oa,n),t(oa,hh),h(e,nl,n),h(e,ia,n),t(ia,dh),h(e,hl,n),h(e,g,n),t(g,Do),t(Do,fh),t(g,uh),t(g,Mo),t(Mo,mh),t(g,ph),t(g,Oo),t(Oo,ch),t(g,wh),t(g,Bo),t(Bo,vh),t(g,gh),t(g,Go),t(Go,bh),h(e,dl,n),h(e,la,n),t(la,yh),h(e,fl,n),h(e,sa,n),t(sa,Eh),h(e,ul,n),h(e,ra,n),t(ra,_h),h(e,ml,n),h(e,re,n),t(re,Fo),t(Fo,Lh),t(re,Ih),t(re,qo),t(qo,kh),h(e,pl,n),h(e,na,n),t(na,Ah),h(e,cl,n),h(e,F,n),t(F,Th),t(F,Ye),t(Ye,Ph),t(F,Ch),t(F,Ke),t(Ke,Nh),t(F,Wh),h(e,wl,n),h(e,ha,n),t(ha,xh),h(e,vl,n),h(e,q,n),t(q,Sh),t(q,Ve),t(Ve,Uh),t(q,Dh),t(q,$e),t($e,Mh),t(q,Oh),h(e,gl,n),h(e,da,n),t(da,Bh),h(e,bl,n),h(e,fa,n),t(fa,Gh),h(e,yl,n),h(e,m,n),t(m,Ho),t(Ho,Fh),t(m,qh),t(m,Ro),t(Ro,Hh),t(m,Rh),t(m,zo),t(zo,zh),t(m,jh),t(m,jo),t(jo,Yh),t(m,Kh),t(m,Yo),t(Yo,Vh),t(m,$h),t(m,Ko),t(Ko,Xh),t(m,Jh),t(m,Vo),t(Vo,Zh),t(m,Qh),t(m,$o),t($o,ed),t(m,td),t(m,Xo),t(Xo,ad),t(m,od),t(m,Jo),t(Jo,id),h(e,El,n),h(e,ua,n),t(ua,ld),h(e,_l,n),h(e,ma,n),t(ma,sd),h(e,Ll,n),h(e,pa,n),t(pa,rd),h(e,Il,n),h(e,Xe,n),h(e,kl,n),h(e,ca,n),t(ca,nd),h(e,Al,n),h(e,wa,n),t(wa,hd),h(e,Tl,n),h(e,va,n),t(va,dd),h(e,Pl,n),h(e,ga,n),t(ga,fd),h(e,Cl,n),h(e,ba,n),t(ba,ud),h(e,Nl,n),h(e,ya,n),t(ya,md),h(e,Wl,n),h(e,Ea,n),t(Ea,pd),h(e,xl,n),h(e,_a,n),t(_a,cd),h(e,Sl,n),h(e,ne,n),t(ne,Zo),t(Zo,wd),t(ne,vd),t(ne,Qo),t(Qo,gd),h(e,Ul,n),h(e,La,n),t(La,bd),h(e,Dl,n),h(e,Ia,n),t(Ia,yd),h(e,Ml,n),h(e,I,n),t(I,Je),t(Je,Ed),t(Je,Ze),t(Ze,_d),t(Je,Ld),t(I,Id),t(I,Qe),t(Qe,kd),t(Qe,et),t(et,Ad),t(Qe,Td),t(I,Pd),t(I,ka),t(ka,tt),t(tt,Cd),t(ka,Nd),t(I,Wd),t(I,ei),t(ei,xd),h(e,Ol,n),h(e,Aa,n),t(Aa,Sd),h(e,Bl,n),h(e,Ta,n),t(Ta,Ud),h(e,Gl,n),h(e,k,n),t(k,Dd),t(k,at),t(at,Md),t(k,Od),t(k,ot),t(ot,Bd),t(k,Gd),t(k,it),t(it,Fd),t(k,qd),h(e,Fl,n),h(e,Pa,n),t(Pa,Hd),h(e,ql,n),h(e,A,n),t(A,Rd),t(A,lt),t(lt,zd),t(A,jd),t(A,st),t(st,Yd),t(A,Kd),t(A,rt),t(rt,Vd),t(A,$d),h(e,Hl,n),h(e,Ca,n),t(Ca,Xd),h(e,Rl,n),h(e,H,n),t(H,Jd),t(H,nt),t(nt,Zd),t(H,Qd),t(H,ht),t(ht,ef),t(H,tf),h(e,zl,n),h(e,he,n),t(he,dt),t(dt,ti),t(ti,af),t(dt,of),t(dt,V),t(V,ai),t(ai,oi),t(oi,lf),t(V,sf),t(V,ii),t(ii,li),t(li,rf),t(V,nf),t(V,Na),t(Na,si),t(si,hf),t(Na,df),t(he,ff),t(he,Wa),t(Wa,ri),t(ri,uf),t(Wa,mf),h(e,jl,n),h(e,de,n),t(de,pf),t(de,ft),t(ft,cf),t(de,wf),h(e,Yl,n),h(e,xa,n),t(xa,vf),h(e,Kl,n),h(e,T,n),t(T,gf),t(T,ut),t(ut,bf),t(T,yf),t(T,mt),t(mt,Ef),t(T,_f),t(T,pt),t(pt,Lf),t(T,If),h(e,Vl,n),h(e,ct,n),h(e,$l,n),h(e,Sa,n),t(Sa,kf),h(e,Xl,n),h(e,fe,n),t(fe,Af),t(fe,wt),t(wt,Tf),t(fe,Pf),h(e,Jl,n),h(e,Ua,n),t(Ua,Cf),h(e,Zl,n),h(e,Da,n),t(Da,Nf),h(e,Ql,n),h(e,Ma,n),t(Ma,Wf),h(e,es,n),h(e,R,n),t(R,C),t(C,xf),t(C,vt),t(vt,Sf),t(C,Uf),t(C,gt),t(gt,Df),t(C,Mf),t(C,bt),t(bt,Of),t(C,Bf),t(R,Gf),t(R,ni),t(ni,Ff),t(R,qf),t(R,hi),t(hi,Hf),h(e,ts,n),h(e,Oa,n),t(Oa,Rf),h(e,as,n),h(e,P,n),t(P,ue),t(ue,zf),t(ue,yt),t(yt,jf),t(ue,Yf),t(ue,Et),t(Et,Kf),t(P,Vf),t(P,Ba),t(Ba,$f),t(Ba,_t),t(_t,Xf),t(P,Jf),t(P,Ga),t(Ga,Zf),t(Ga,Lt),t(Lt,Qf),t(P,eu),t(P,Fa),t(Fa,tu),t(Fa,It),t(It,au)},p:Rc,d(e){e&&a(p),e&&a(W),e&&a(x),e&&a(b),e&&a(y),e&&a(mi),e&&a(X),e&&a(pi),e&&a(J),e&&a(ci),e&&a(Tt),e&&a(wi),e&&a(Pt),e&&a(vi),e&&a(S),e&&a(gi),e&&a(Ct),e&&a(bi),e&&a(Nt),e&&a(yi),e&&a(U),e&&a(Ei),e&&a(Z),e&&a(_i),e&&a(Ie),e&&a(Li),e&&a(D),e&&a(Ii),e&&a(Q),e&&a(ki),e&&a(ee),e&&a(Ai),e&&a(te),e&&a(Ti),e&&a(M),e&&a(Pi),e&&a(Wt),e&&a(Ci),e&&a(xt),e&&a(Ni),e&&a(Ne),e&&a(Wi),e&&a(St),e&&a(xi),e&&a(Ut),e&&a(Si),e&&a(O),e&&a(Ui),e&&a(Dt),e&&a(Di),e&&a(Mt),e&&a(Mi),e&&a(Ot),e&&a(Oi),e&&a(ae),e&&a(Bi),e&&a(Bt),e&&a(Gi),e&&a(Gt),e&&a(Fi),e&&a(E),e&&a(qi),e&&a(Ft),e&&a(Hi),e&&a(qt),e&&a(Ri),e&&a(xe),e&&a(zi),e&&a(Ht),e&&a(ji),e&&a(v),e&&a(Yi),e&&a(ie),e&&a(Ki),e&&a(jt),e&&a(Vi),e&&a(Yt),e&&a($i),e&&a(le),e&&a(Xi),e&&a($t),e&&a(Ji),e&&a(Xt),e&&a(Zi),e&&a(Jt),e&&a(Qi),e&&a(Zt),e&&a(el),e&&a(B),e&&a(tl),e&&a(Qt),e&&a(al),e&&a(ea),e&&a(ol),e&&a(ta),e&&a(il),e&&a(L),e&&a(ll),e&&a(aa),e&&a(sl),e&&a(se),e&&a(rl),e&&a(oa),e&&a(nl),e&&a(ia),e&&a(hl),e&&a(g),e&&a(dl),e&&a(la),e&&a(fl),e&&a(sa),e&&a(ul),e&&a(ra),e&&a(ml),e&&a(re),e&&a(pl),e&&a(na),e&&a(cl),e&&a(F),e&&a(wl),e&&a(ha),e&&a(vl),e&&a(q),e&&a(gl),e&&a(da),e&&a(bl),e&&a(fa),e&&a(yl),e&&a(m),e&&a(El),e&&a(ua),e&&a(_l),e&&a(ma),e&&a(Ll),e&&a(pa),e&&a(Il),e&&a(Xe),e&&a(kl),e&&a(ca),e&&a(Al),e&&a(wa),e&&a(Tl),e&&a(va),e&&a(Pl),e&&a(ga),e&&a(Cl),e&&a(ba),e&&a(Nl),e&&a(ya),e&&a(Wl),e&&a(Ea),e&&a(xl),e&&a(_a),e&&a(Sl),e&&a(ne),e&&a(Ul),e&&a(La),e&&a(Dl),e&&a(Ia),e&&a(Ml),e&&a(I),e&&a(Ol),e&&a(Aa),e&&a(Bl),e&&a(Ta),e&&a(Gl),e&&a(k),e&&a(Fl),e&&a(Pa),e&&a(ql),e&&a(A),e&&a(Hl),e&&a(Ca),e&&a(Rl),e&&a(H),e&&a(zl),e&&a(he),e&&a(jl),e&&a(de),e&&a(Yl),e&&a(xa),e&&a(Kl),e&&a(T),e&&a(Vl),e&&a(ct),e&&a($l),e&&a(Sa),e&&a(Xl),e&&a(fe),e&&a(Jl),e&&a(Ua),e&&a(Zl),e&&a(Da),e&&a(Ql),e&&a(Ma),e&&a(es),e&&a(R),e&&a(ts),e&&a(Oa),e&&a(as),e&&a(P)}}}function Vc(At){let p,N;const W=[At[0],Ac];let x={$$slots:{default:[Kc]},$$scope:{ctx:At}};for(let c=0;c<W.length;c+=1)x=mu(x,W[c]);return p=new zc({props:x}),{c(){Mc(p.$$.fragment)},l(c){Oc(p.$$.fragment,c)},m(c,b){Bc(p,c,b),N=!0},p(c,[b]){const y=b&1?Gc(W,[b&1&&Lc(c[0]),b&0&&Lc(Ac)]):{};b&2&&(y.$$scope={dirty:b,ctx:c}),p.$set(y)},i(c){N||(Fc(p.$$.fragment,c),N=!0)},o(c){qc(p.$$.fragment,c),N=!1},d(c){Hc(p,c)}}}const Ac={title:"LAION-400-Million Open Dataset",author:"Christoph Schuhmann",date:"2021-08-20T00:00:00.000Z",tagline:"We present LAION-400M, the world's largest openly available image-text pair dataset with 400 million samples."};function $c(At,p,N){return At.$$set=W=>{N(0,p=mu(mu({},p),Ic(W)))},p=Ic(p),[p]}class Zc extends Sc{constructor(p){super(),Uc(this,p,$c,Vc,Dc,{})}}export{Zc as default,Ac as metadata};
